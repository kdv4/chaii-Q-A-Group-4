{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Murli2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"faIyP9zZEd0G","executionInfo":{"status":"ok","timestamp":1635261179087,"user_tz":-330,"elapsed":8830,"user":{"displayName":"2020 11053","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11099621414422210383"}},"outputId":"588aa0e7-15c7-44fb-ba54-897236feee15"},"source":["! pip install transformers sentencepiece "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 5.3 MB/s \n","\u001b[?25hCollecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 42.9 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 48.6 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 39.0 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 43.7 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting huggingface-hub>=0.0.17\n","  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n","\u001b[K     |████████████████████████████████| 56 kB 5.4 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.19 pyyaml-6.0 sacremoses-0.0.46 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.11.3\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5wo74L4DKXPb","executionInfo":{"status":"ok","timestamp":1635261211610,"user_tz":-330,"elapsed":26136,"user":{"displayName":"2020 11053","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11099621414422210383"}},"outputId":"5e4c94cf-6dc5-4f8e-87c2-27d67bec03f3"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jDu4BWB1EJJ3","executionInfo":{"status":"ok","timestamp":1635261247734,"user_tz":-330,"elapsed":28245,"user":{"displayName":"2020 11053","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11099621414422210383"}},"outputId":"1583f1b9-751d-4b9c-cdb1-236ca094e61e"},"source":["import os\n","import gc\n","gc.enable()\n","import math\n","import json\n","import time\n","import random\n","import multiprocessing\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm, trange\n","from sklearn import model_selection\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import Parameter\n","import torch.optim as optim\n","from torch.utils.data import (\n","    Dataset, DataLoader,\n","    SequentialSampler, RandomSampler\n",")\n","from torch.utils.data.distributed import DistributedSampler\n","\n","try:\n","    from apex import amp\n","    APEX_INSTALLED = True\n","except ImportError:\n","    APEX_INSTALLED = False\n","\n","import transformers\n","from transformers import (\n","    WEIGHTS_NAME,\n","    AdamW,\n","    AutoConfig,\n","    AutoModel,\n","    AutoTokenizer,\n","    get_cosine_schedule_with_warmup,\n","    get_linear_schedule_with_warmup,\n","    logging,\n","    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n",")\n","logging.set_verbosity_warning()\n","logging.set_verbosity_error()\n","\n","def fix_all_seeds(seed):\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","def optimal_num_of_loader_workers():\n","    num_cpus = multiprocessing.cpu_count()\n","    num_gpus = torch.cuda.device_count()\n","    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n","    return optimal_value\n","\n","print(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\n","MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n","MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Apex AMP Installed :: False\n"]}]},{"cell_type":"code","metadata":{"id":"t8HfXz6SHhE7"},"source":["class Config:\n","    # model\n","    model_type = 'bert'\n","    model_name_or_path = \"/content/drive/Shareddrives/NLP/Tarang/xlm-roberta-large-squad2/model\"\n","    config_name = \"/content/drive/Shareddrives/NLP/Tarang/xlm-roberta-large-squad2/model\"\n","    fp16 = True if APEX_INSTALLED else False\n","    fp16_opt_level = \"O1\"\n","    gradient_accumulation_steps = 2\n","\n","    # tokenizer\n","    tokenizer_name = \"/content/drive/Shareddrives/NLP/Tarang/xlm-roberta-large-squad2/tok\"\n","    max_seq_length = 384\n","    doc_stride = 128\n","\n","    # train\n","    epochs = 1\n","    train_batch_size = 1\n","    eval_batch_size = 1\n","\n","    # optimizer\n","    optimizer_type = 'AdamW'\n","    learning_rate = 1.5e-5\n","    weight_decay = 1e-2\n","    epsilon = 1e-8\n","    max_grad_norm = 1.0\n","\n","    # scheduler\n","    decay_name = 'linear-warmup'\n","    warmup_ratio = 0.1\n","\n","    # logging\n","    logging_steps = 10\n","\n","    # evaluate\n","    output_dir = 'output'\n","    seed = 42"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jzv3krJSETEs"},"source":["train = pd.read_csv('/content/drive/Shareddrives/NLP/Dataset/train.csv')\n","test = pd.read_csv('/content/drive/Shareddrives/NLP/Dataset/test.csv')\n","external_mlqa = pd.read_csv('/content/drive/Shareddrives/NLP/Dataset/mlqa_hindi.csv')\n","external_xquad = pd.read_csv('/content/drive/Shareddrives/NLP/Dataset/xquad.csv')\n","external_train = pd.concat([external_mlqa, external_xquad])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wNv3o7qeFmaB"},"source":["def create_folds(data, num_splits):\n","    data[\"kfold\"] = -1\n","    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=69)\n","    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n","        data.loc[v_, 'kfold'] = f\n","    return data\n","\n","train = create_folds(train, num_splits=5)\n","external_train[\"kfold\"] = -1\n","external_train['id'] = list(np.arange(1, len(external_train)+1))\n","train = pd.concat([train, external_train]).reset_index(drop=True)\n","\n","def convert_answers(row):\n","    return {'answer_start': [row[0]], 'text': [row[1]]}\n","\n","train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dVmPKiJZGKGc"},"source":["def prepare_train_features(args, example, tokenizer):\n","    example[\"question\"] = example[\"question\"].lstrip()\n","    tokenized_example = tokenizer(\n","        example[\"question\"],\n","        example[\"context\"],\n","        truncation=\"only_second\",\n","        max_length=args.max_seq_length,\n","        stride=args.doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n","    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n","\n","    features = []\n","    for i, offsets in enumerate(offset_mapping):\n","        feature = {}\n","\n","        input_ids = tokenized_example[\"input_ids\"][i]\n","        attention_mask = tokenized_example[\"attention_mask\"][i]\n","\n","        feature['input_ids'] = input_ids\n","        feature['attention_mask'] = attention_mask\n","        feature['offset_mapping'] = offsets\n","\n","        cls_index = input_ids.index(tokenizer.cls_token_id)\n","        sequence_ids = tokenized_example.sequence_ids(i)\n","\n","        sample_index = sample_mapping[i]\n","        answers = example[\"answers\"]\n","\n","        if len(answers[\"answer_start\"]) == 0:\n","            feature[\"start_position\"] = cls_index\n","            feature[\"end_position\"] = cls_index\n","        else:\n","            start_char = answers[\"answer_start\"][0]\n","            end_char = start_char + len(answers[\"text\"][0])\n","\n","            token_start_index = 0\n","            while sequence_ids[token_start_index] != 1:\n","                token_start_index += 1\n","\n","            token_end_index = len(input_ids) - 1\n","            while sequence_ids[token_end_index] != 1:\n","                token_end_index -= 1\n","\n","            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n","                feature[\"start_position\"] = cls_index\n","                feature[\"end_position\"] = cls_index\n","            else:\n","                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n","                    token_start_index += 1\n","                feature[\"start_position\"] = token_start_index - 1\n","                while offsets[token_end_index][1] >= end_char:\n","                    token_end_index -= 1\n","                feature[\"end_position\"] = token_end_index + 1\n","\n","        features.append(feature)\n","    return features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1X5iXyuyGRuH"},"source":["class DatasetRetriever(Dataset):\n","    def __init__(self, features, mode='train'):\n","        super(DatasetRetriever, self).__init__()\n","        self.features = features\n","        self.mode = mode\n","        \n","    def __len__(self):\n","        return len(self.features)\n","    \n","    def __getitem__(self, item):   \n","        feature = self.features[item]\n","        if self.mode == 'train':\n","            return {\n","                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n","                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n","                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n","                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n","                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n","            }\n","        else:\n","            return {\n","                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n","                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n","                'offset_mapping':feature['offset_mapping'],\n","                'sequence_ids':feature['sequence_ids'],\n","                'id':feature['example_id'],\n","                'context': feature['context'],\n","                'question': feature['question']\n","            }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c83EghN4GXie"},"source":["class Model(nn.Module):\n","    def __init__(self, modelname_or_path, config):\n","        super(Model, self).__init__()\n","        self.config = config\n","        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n","        self.linear_layer = nn.Linear(config.hidden_size, 64)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.qa_outputs = nn.Linear(64, 2)\n","        self._init_weights(self.qa_outputs)\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","\n","    def forward(\n","        self, \n","        input_ids, \n","        attention_mask=None, \n","    ):\n","        outputs = self.xlm_roberta(input_ids,attention_mask=attention_mask)\n","\n","        sequence_output = outputs[0]\n","        pooled_output = outputs[1]\n","        \n","        linear_output = self.linear_layer(sequence_output)\n","        linear_output = self.dropout(linear_output)\n","        qa_logits = self.qa_outputs(linear_output)\n","        \n","        start_logits, end_logits = qa_logits.split(1, dim=-1)\n","        start_logits = start_logits.squeeze(-1)\n","        end_logits = end_logits.squeeze(-1)\n","    \n","        return start_logits, end_logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gKn4pHnvGc4H"},"source":["def loss_fn(preds, labels):\n","    start_preds, end_preds = preds\n","    start_labels, end_labels = labels\n","    \n","    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n","    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n","    total_loss = (start_loss + end_loss) / 2\n","    return total_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qoV8_pn6GgdW"},"source":["def get_optimizer_grouped_parameters(args, model):\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n","    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n","    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n","    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': args.weight_decay},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': args.weight_decay, 'lr': args.learning_rate/2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': args.weight_decay, 'lr': args.learning_rate},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': args.weight_decay, 'lr': args.learning_rate*2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': args.learning_rate/2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': args.learning_rate},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': args.learning_rate*2.6},\n","        {'params': [p for n, p in model.named_parameters() if args.model_type not in n], 'lr':args.learning_rate*20, \"weight_decay\": 0.0},\n","    ]\n","    return optimizer_grouped_parameters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xp_uEUijGrCv"},"source":["class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","        self.max = 0\n","        self.min = 1e5\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","        if val > self.max:\n","            self.max = val\n","        if val < self.min:\n","            self.min = val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RKcK3xENGvQ4"},"source":["def make_model(args):\n","    config = AutoConfig.from_pretrained(args.config_name)\n","    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n","    model = Model(args.model_name_or_path, config=config)\n","    return config, tokenizer, model\n","\n","def make_optimizer(args, model):\n","    optimizer_grouped_parameters = get_optimizer_grouped_parameters(args, model)\n","#     no_decay = [\"bias\", \"LayerNorm.weight\"]\n","#     optimizer_grouped_parameters = [\n","#         {\n","#             \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","#             \"weight_decay\": args.weight_decay,\n","#         },\n","#         {\n","#             \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","#             \"weight_decay\": 0.0,\n","#         },\n","#     ]\n","    if args.optimizer_type == \"AdamW\":\n","        optimizer = AdamW(\n","            optimizer_grouped_parameters,\n","            lr=args.learning_rate,\n","            eps=args.epsilon,\n","            correct_bias=True\n","        )\n","        return optimizer\n","\n","def make_scheduler(\n","    args, optimizer, \n","    num_warmup_steps, \n","    num_training_steps\n","):\n","    if args.decay_name == \"cosine-warmup\":\n","        scheduler = get_cosine_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps\n","        )\n","    else:\n","        scheduler = get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps\n","        )\n","    return scheduler    \n","\n","def make_loader(\n","    args, data, \n","    tokenizer, fold\n","):\n","    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n","    \n","    train_features, valid_features = [[] for _ in range(2)]\n","    for i, row in train_set.iterrows():\n","        train_features += prepare_train_features(args, row, tokenizer)\n","    for i, row in valid_set.iterrows():\n","        valid_features += prepare_train_features(args, row, tokenizer)\n","\n","    train_dataset = DatasetRetriever(train_features)\n","    valid_dataset = DatasetRetriever(valid_features)\n","    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n","    \n","    train_sampler = RandomSampler(train_dataset)\n","    valid_sampler = SequentialSampler(valid_dataset)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=args.train_batch_size,\n","        sampler=train_sampler,\n","        num_workers=optimal_num_of_loader_workers(),\n","        pin_memory=True,\n","        drop_last=False \n","    )\n","\n","    valid_dataloader = DataLoader(\n","        valid_dataset,\n","        batch_size=args.eval_batch_size, \n","        sampler=valid_sampler,\n","        num_workers=optimal_num_of_loader_workers(),\n","        pin_memory=True, \n","        drop_last=False\n","    )\n","\n","    return train_dataloader, valid_dataloader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YYXpJ2UaG4ft"},"source":["class Trainer:\n","    def __init__(\n","        self, model, tokenizer, \n","        optimizer, scheduler\n","    ):\n","        self.model = model\n","        self.tokenizer = tokenizer\n","        self.optimizer = optimizer\n","        self.scheduler = scheduler\n","\n","    def train(\n","        self, args, \n","        train_dataloader, \n","        epoch, result_dict\n","    ):\n","        count = 0\n","        losses = AverageMeter()\n","        \n","        self.model.zero_grad()\n","        self.model.train()\n","        \n","        fix_all_seeds(args.seed)\n","        \n","        for batch_idx, batch_data in enumerate(train_dataloader):\n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                batch_data['input_ids'], batch_data['attention_mask'], \\\n","                    batch_data['start_position'], batch_data['end_position']\n","            \n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n","\n","            outputs_start, outputs_end = self.model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","            )\n","            \n","            loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n","            loss = loss / args.gradient_accumulation_steps\n","\n","            if args.fp16:\n","                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","            else:\n","                loss.backward()\n","\n","            count += input_ids.size(0)\n","            losses.update(loss.item(), input_ids.size(0))\n","\n","            # if args.fp16:\n","            #     torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), args.max_grad_norm)\n","            # else:\n","            #     torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.max_grad_norm)\n","\n","            if batch_idx % args.gradient_accumulation_steps == 0 or batch_idx == len(train_dataloader) - 1:\n","                self.optimizer.step()\n","                self.scheduler.step()\n","                self.optimizer.zero_grad()\n","\n","            if (batch_idx % args.logging_steps == 0) or (batch_idx+1)==len(train_dataloader):\n","                _s = str(len(str(len(train_dataloader.sampler))))\n","                ret = [\n","                    ('Epoch: {:0>2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_dataloader.sampler), 100 * count / len(train_dataloader.sampler)),\n","                    'Train Loss: {: >4.5f}'.format(losses.avg),\n","                ]\n","                print(', '.join(ret))\n","\n","        result_dict['train_loss'].append(losses.avg)\n","        return result_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c7lo_72cG88t"},"source":["class Evaluator:\n","    def __init__(self, model):\n","        self.model = model\n","    \n","    def save(self, result, output_dir):\n","        with open(f'{output_dir}/result_dict.json', 'w') as f:\n","            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n","\n","    def evaluate(self, valid_dataloader, epoch, result_dict):\n","        losses = AverageMeter()\n","        for batch_idx, batch_data in enumerate(valid_dataloader):\n","            self.model = self.model.eval()\n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                batch_data['input_ids'], batch_data['attention_mask'], \\\n","                    batch_data['start_position'], batch_data['end_position']\n","            \n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n","            \n","            with torch.no_grad():            \n","                outputs_start, outputs_end = self.model(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                )\n","                \n","                loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n","                losses.update(loss.item(), input_ids.size(0))\n","                \n","        print('----Validation Results Summary----')\n","        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n","        result_dict['val_loss'].append(losses.avg)        \n","        return result_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wv14GLumHBTO"},"source":["def init_training(args, data, fold):\n","    fix_all_seeds(args.seed)\n","    \n","    if not os.path.exists(args.output_dir):\n","        os.makedirs(args.output_dir)\n","    \n","    # model\n","    model_config, tokenizer, model = make_model(args)\n","    if torch.cuda.device_count() >= 1:\n","        print('Model pushed to {} GPU(s), type {}.'.format(\n","            torch.cuda.device_count(), \n","            torch.cuda.get_device_name(0))\n","        )\n","        model = model.cuda() \n","    else:\n","        raise ValueError('CPU training is not supported')\n","    \n","    # data loaders\n","    train_dataloader, valid_dataloader = make_loader(args, data, tokenizer, fold)\n","\n","    # optimizer\n","    optimizer = make_optimizer(args, model)\n","\n","    # scheduler\n","    num_training_steps = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps) * args.epochs\n","    if args.warmup_ratio > 0:\n","        num_warmup_steps = int(args.warmup_ratio * num_training_steps)\n","    else:\n","        num_warmup_steps = 0\n","    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n","    scheduler = make_scheduler(args, optimizer, num_warmup_steps, num_training_steps)\n","\n","    # mixed precision training with NVIDIA Apex\n","    if args.fp16:\n","        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n","    \n","    result_dict = {\n","        'epoch':[], \n","        'train_loss': [], \n","        'val_loss' : [], \n","        'best_val_loss': np.inf\n","    }\n","\n","    return (\n","        model, model_config, tokenizer, optimizer, scheduler, \n","        train_dataloader, valid_dataloader, result_dict\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XUjSnwFpHKAi"},"source":["def run(data, fold):\n","    args = Config()\n","    model, model_config, tokenizer, optimizer, scheduler, train_dataloader, \\\n","        valid_dataloader, result_dict = init_training(args, data, fold)\n","    \n","    trainer = Trainer(model, tokenizer, optimizer, scheduler)\n","    evaluator = Evaluator(model)\n","\n","    train_time_list = []\n","    valid_time_list = []\n","\n","    for epoch in range(args.epochs):\n","        result_dict['epoch'].append(epoch)\n","\n","        # Train\n","        torch.cuda.synchronize()\n","        tic1 = time.time()\n","        result_dict = trainer.train(\n","            args, train_dataloader, \n","            epoch, result_dict\n","        )\n","        torch.cuda.synchronize()\n","        tic2 = time.time() \n","        train_time_list.append(tic2 - tic1)\n","        \n","        # Evaluate\n","        torch.cuda.synchronize()\n","        tic3 = time.time()\n","        result_dict = evaluator.evaluate(\n","            valid_dataloader, epoch, result_dict\n","        )\n","        torch.cuda.synchronize()\n","        tic4 = time.time() \n","        valid_time_list.append(tic4 - tic3)\n","            \n","        output_dir = os.path.join(args.output_dir, f\"checkpoint-fold-{fold}\")\n","        if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n","            print(\"{} Epoch, Best epoch was updated! Valid Loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n","            result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]        \n","            \n","            os.makedirs(output_dir, exist_ok=True)\n","            torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n","            model_config.save_pretrained(output_dir)\n","            tokenizer.save_pretrained(output_dir)\n","            print(f\"Saving model checkpoint to {output_dir}.\")\n","            \n","        print()\n","\n","    evaluator.save(result_dict, output_dir)\n","    \n","    print(f\"Total Training Time: {np.sum(train_time_list)}secs, Average Training Time per Epoch: {np.mean(train_time_list)}secs.\")\n","    print(f\"Total Validation Time: {np.sum(valid_time_list)}secs, Average Validation Time per Epoch: {np.mean(valid_time_list)}secs.\")\n","    \n","    torch.cuda.empty_cache()\n","    del trainer, evaluator\n","    del model, model_config, tokenizer\n","    del optimizer, scheduler\n","    del train_dataloader, valid_dataloader, result_dict\n","    gc.collect()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bduls9AyHO8p","outputId":"bf52a270-d446-4618-8b63-d54835ffea5e"},"source":["for fold in range(1,2):\n","    print();print()\n","    print('-'*50)\n","    print(f'FOLD: {fold}')\n","    print('-'*50)\n","    run(train, fold)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","--------------------------------------------------\n","FOLD: 1\n","--------------------------------------------------\n","Model pushed to 1 GPU(s), type Tesla K80.\n","Num examples Train= 20014, Num examples Valid=3055\n","Total Training Steps: 10007, Total Warmup Steps: 1000\n","Epoch: 00 [    1/20014 (  0%)], Train Loss: 2.96888\n","Epoch: 00 [   11/20014 (  0%)], Train Loss: 2.96237\n","Epoch: 00 [   21/20014 (  0%)], Train Loss: 2.95901\n","Epoch: 00 [   31/20014 (  0%)], Train Loss: 2.95718\n","Epoch: 00 [   41/20014 (  0%)], Train Loss: 2.95630\n","Epoch: 00 [   51/20014 (  0%)], Train Loss: 2.95182\n","Epoch: 00 [   61/20014 (  0%)], Train Loss: 2.94676\n","Epoch: 00 [   71/20014 (  0%)], Train Loss: 2.93796\n","Epoch: 00 [   81/20014 (  0%)], Train Loss: 2.93339\n","Epoch: 00 [   91/20014 (  0%)], Train Loss: 2.92355\n","Epoch: 00 [  101/20014 (  1%)], Train Loss: 2.91451\n","Epoch: 00 [  111/20014 (  1%)], Train Loss: 2.90819\n","Epoch: 00 [  121/20014 (  1%)], Train Loss: 2.89614\n","Epoch: 00 [  131/20014 (  1%)], Train Loss: 2.88563\n","Epoch: 00 [  141/20014 (  1%)], Train Loss: 2.87338\n","Epoch: 00 [  151/20014 (  1%)], Train Loss: 2.85660\n","Epoch: 00 [  161/20014 (  1%)], Train Loss: 2.84182\n","Epoch: 00 [  171/20014 (  1%)], Train Loss: 2.82425\n","Epoch: 00 [  181/20014 (  1%)], Train Loss: 2.80312\n","Epoch: 00 [  191/20014 (  1%)], Train Loss: 2.77890\n","Epoch: 00 [  201/20014 (  1%)], Train Loss: 2.76201\n","Epoch: 00 [  211/20014 (  1%)], Train Loss: 2.73777\n","Epoch: 00 [  221/20014 (  1%)], Train Loss: 2.70567\n","Epoch: 00 [  231/20014 (  1%)], Train Loss: 2.66698\n","Epoch: 00 [  241/20014 (  1%)], Train Loss: 2.62796\n","Epoch: 00 [  251/20014 (  1%)], Train Loss: 2.58261\n","Epoch: 00 [  261/20014 (  1%)], Train Loss: 2.53950\n","Epoch: 00 [  271/20014 (  1%)], Train Loss: 2.48566\n","Epoch: 00 [  281/20014 (  1%)], Train Loss: 2.45241\n","Epoch: 00 [  291/20014 (  1%)], Train Loss: 2.41663\n","Epoch: 00 [  301/20014 (  2%)], Train Loss: 2.36160\n","Epoch: 00 [  311/20014 (  2%)], Train Loss: 2.30777\n","Epoch: 00 [  321/20014 (  2%)], Train Loss: 2.27026\n","Epoch: 00 [  331/20014 (  2%)], Train Loss: 2.22884\n","Epoch: 00 [  341/20014 (  2%)], Train Loss: 2.18714\n","Epoch: 00 [  351/20014 (  2%)], Train Loss: 2.13835\n","Epoch: 00 [  361/20014 (  2%)], Train Loss: 2.10260\n","Epoch: 00 [  371/20014 (  2%)], Train Loss: 2.05126\n","Epoch: 00 [  381/20014 (  2%)], Train Loss: 2.00998\n","Epoch: 00 [  391/20014 (  2%)], Train Loss: 1.97060\n","Epoch: 00 [  401/20014 (  2%)], Train Loss: 1.94590\n","Epoch: 00 [  411/20014 (  2%)], Train Loss: 1.90608\n","Epoch: 00 [  421/20014 (  2%)], Train Loss: 1.86908\n","Epoch: 00 [  431/20014 (  2%)], Train Loss: 1.84903\n","Epoch: 00 [  441/20014 (  2%)], Train Loss: 1.81024\n","Epoch: 00 [  451/20014 (  2%)], Train Loss: 1.78202\n","Epoch: 00 [  461/20014 (  2%)], Train Loss: 1.75356\n","Epoch: 00 [  471/20014 (  2%)], Train Loss: 1.72399\n","Epoch: 00 [  481/20014 (  2%)], Train Loss: 1.69695\n","Epoch: 00 [  491/20014 (  2%)], Train Loss: 1.66920\n","Epoch: 00 [  501/20014 (  3%)], Train Loss: 1.64595\n","Epoch: 00 [  511/20014 (  3%)], Train Loss: 1.61738\n","Epoch: 00 [  521/20014 (  3%)], Train Loss: 1.59761\n","Epoch: 00 [  531/20014 (  3%)], Train Loss: 1.57251\n","Epoch: 00 [  541/20014 (  3%)], Train Loss: 1.55429\n","Epoch: 00 [  551/20014 (  3%)], Train Loss: 1.53108\n","Epoch: 00 [  561/20014 (  3%)], Train Loss: 1.51865\n","Epoch: 00 [  571/20014 (  3%)], Train Loss: 1.49927\n","Epoch: 00 [  581/20014 (  3%)], Train Loss: 1.47898\n","Epoch: 00 [  591/20014 (  3%)], Train Loss: 1.46304\n","Epoch: 00 [  601/20014 (  3%)], Train Loss: 1.44170\n","Epoch: 00 [  611/20014 (  3%)], Train Loss: 1.42496\n","Epoch: 00 [  621/20014 (  3%)], Train Loss: 1.41634\n","Epoch: 00 [  631/20014 (  3%)], Train Loss: 1.40410\n","Epoch: 00 [  641/20014 (  3%)], Train Loss: 1.39244\n","Epoch: 00 [  651/20014 (  3%)], Train Loss: 1.37314\n","Epoch: 00 [  661/20014 (  3%)], Train Loss: 1.36010\n","Epoch: 00 [  671/20014 (  3%)], Train Loss: 1.35187\n","Epoch: 00 [  681/20014 (  3%)], Train Loss: 1.33927\n","Epoch: 00 [  691/20014 (  3%)], Train Loss: 1.32338\n","Epoch: 00 [  701/20014 (  4%)], Train Loss: 1.31562\n","Epoch: 00 [  711/20014 (  4%)], Train Loss: 1.30060\n","Epoch: 00 [  721/20014 (  4%)], Train Loss: 1.29071\n","Epoch: 00 [  731/20014 (  4%)], Train Loss: 1.27888\n","Epoch: 00 [  741/20014 (  4%)], Train Loss: 1.26531\n","Epoch: 00 [  751/20014 (  4%)], Train Loss: 1.25242\n","Epoch: 00 [  761/20014 (  4%)], Train Loss: 1.23673\n","Epoch: 00 [  771/20014 (  4%)], Train Loss: 1.22838\n","Epoch: 00 [  781/20014 (  4%)], Train Loss: 1.21793\n","Epoch: 00 [  791/20014 (  4%)], Train Loss: 1.21122\n","Epoch: 00 [  801/20014 (  4%)], Train Loss: 1.20449\n","Epoch: 00 [  811/20014 (  4%)], Train Loss: 1.19387\n","Epoch: 00 [  821/20014 (  4%)], Train Loss: 1.19169\n","Epoch: 00 [  831/20014 (  4%)], Train Loss: 1.18453\n","Epoch: 00 [  841/20014 (  4%)], Train Loss: 1.17526\n","Epoch: 00 [  851/20014 (  4%)], Train Loss: 1.16500\n","Epoch: 00 [  861/20014 (  4%)], Train Loss: 1.15461\n","Epoch: 00 [  871/20014 (  4%)], Train Loss: 1.14651\n","Epoch: 00 [  881/20014 (  4%)], Train Loss: 1.13599\n","Epoch: 00 [  891/20014 (  4%)], Train Loss: 1.12745\n","Epoch: 00 [  901/20014 (  5%)], Train Loss: 1.12417\n","Epoch: 00 [  911/20014 (  5%)], Train Loss: 1.11684\n","Epoch: 00 [  921/20014 (  5%)], Train Loss: 1.10603\n","Epoch: 00 [  931/20014 (  5%)], Train Loss: 1.10974\n","Epoch: 00 [  941/20014 (  5%)], Train Loss: 1.10090\n","Epoch: 00 [  951/20014 (  5%)], Train Loss: 1.09202\n","Epoch: 00 [  961/20014 (  5%)], Train Loss: 1.08605\n","Epoch: 00 [  971/20014 (  5%)], Train Loss: 1.07846\n","Epoch: 00 [  981/20014 (  5%)], Train Loss: 1.07153\n","Epoch: 00 [  991/20014 (  5%)], Train Loss: 1.06226\n","Epoch: 00 [ 1001/20014 (  5%)], Train Loss: 1.05511\n","Epoch: 00 [ 1011/20014 (  5%)], Train Loss: 1.05062\n","Epoch: 00 [ 1021/20014 (  5%)], Train Loss: 1.04766\n","Epoch: 00 [ 1031/20014 (  5%)], Train Loss: 1.04711\n","Epoch: 00 [ 1041/20014 (  5%)], Train Loss: 1.03933\n","Epoch: 00 [ 1051/20014 (  5%)], Train Loss: 1.03572\n","Epoch: 00 [ 1061/20014 (  5%)], Train Loss: 1.02907\n","Epoch: 00 [ 1071/20014 (  5%)], Train Loss: 1.02296\n","Epoch: 00 [ 1081/20014 (  5%)], Train Loss: 1.01663\n","Epoch: 00 [ 1091/20014 (  5%)], Train Loss: 1.00928\n","Epoch: 00 [ 1101/20014 (  6%)], Train Loss: 1.00312\n","Epoch: 00 [ 1111/20014 (  6%)], Train Loss: 0.99709\n","Epoch: 00 [ 1121/20014 (  6%)], Train Loss: 0.99536\n","Epoch: 00 [ 1131/20014 (  6%)], Train Loss: 0.98977\n","Epoch: 00 [ 1141/20014 (  6%)], Train Loss: 0.98612\n","Epoch: 00 [ 1151/20014 (  6%)], Train Loss: 0.98550\n","Epoch: 00 [ 1161/20014 (  6%)], Train Loss: 0.97908\n","Epoch: 00 [ 1171/20014 (  6%)], Train Loss: 0.97224\n","Epoch: 00 [ 1181/20014 (  6%)], Train Loss: 0.96511\n","Epoch: 00 [ 1191/20014 (  6%)], Train Loss: 0.96159\n","Epoch: 00 [ 1201/20014 (  6%)], Train Loss: 0.95548\n","Epoch: 00 [ 1211/20014 (  6%)], Train Loss: 0.95480\n","Epoch: 00 [ 1221/20014 (  6%)], Train Loss: 0.95080\n","Epoch: 00 [ 1231/20014 (  6%)], Train Loss: 0.94420\n","Epoch: 00 [ 1241/20014 (  6%)], Train Loss: 0.94220\n","Epoch: 00 [ 1251/20014 (  6%)], Train Loss: 0.93665\n","Epoch: 00 [ 1261/20014 (  6%)], Train Loss: 0.93548\n","Epoch: 00 [ 1271/20014 (  6%)], Train Loss: 0.93358\n","Epoch: 00 [ 1281/20014 (  6%)], Train Loss: 0.92844\n","Epoch: 00 [ 1291/20014 (  6%)], Train Loss: 0.92502\n","Epoch: 00 [ 1301/20014 (  7%)], Train Loss: 0.92477\n","Epoch: 00 [ 1311/20014 (  7%)], Train Loss: 0.91991\n","Epoch: 00 [ 1321/20014 (  7%)], Train Loss: 0.91894\n","Epoch: 00 [ 1331/20014 (  7%)], Train Loss: 0.91511\n","Epoch: 00 [ 1341/20014 (  7%)], Train Loss: 0.91361\n","Epoch: 00 [ 1351/20014 (  7%)], Train Loss: 0.90886\n","Epoch: 00 [ 1361/20014 (  7%)], Train Loss: 0.90715\n","Epoch: 00 [ 1371/20014 (  7%)], Train Loss: 0.90264\n","Epoch: 00 [ 1381/20014 (  7%)], Train Loss: 0.89790\n","Epoch: 00 [ 1391/20014 (  7%)], Train Loss: 0.89288\n","Epoch: 00 [ 1401/20014 (  7%)], Train Loss: 0.88778\n","Epoch: 00 [ 1411/20014 (  7%)], Train Loss: 0.89067\n","Epoch: 00 [ 1421/20014 (  7%)], Train Loss: 0.88719\n","Epoch: 00 [ 1431/20014 (  7%)], Train Loss: 0.88443\n","Epoch: 00 [ 1441/20014 (  7%)], Train Loss: 0.88228\n","Epoch: 00 [ 1451/20014 (  7%)], Train Loss: 0.87888\n","Epoch: 00 [ 1461/20014 (  7%)], Train Loss: 0.87467\n","Epoch: 00 [ 1471/20014 (  7%)], Train Loss: 0.86986\n","Epoch: 00 [ 1481/20014 (  7%)], Train Loss: 0.86863\n","Epoch: 00 [ 1491/20014 (  7%)], Train Loss: 0.86520\n","Epoch: 00 [ 1501/20014 (  7%)], Train Loss: 0.86218\n","Epoch: 00 [ 1511/20014 (  8%)], Train Loss: 0.86001\n","Epoch: 00 [ 1521/20014 (  8%)], Train Loss: 0.85625\n","Epoch: 00 [ 1531/20014 (  8%)], Train Loss: 0.85553\n","Epoch: 00 [ 1541/20014 (  8%)], Train Loss: 0.85122\n","Epoch: 00 [ 1551/20014 (  8%)], Train Loss: 0.84915\n","Epoch: 00 [ 1561/20014 (  8%)], Train Loss: 0.84833\n","Epoch: 00 [ 1571/20014 (  8%)], Train Loss: 0.84625\n","Epoch: 00 [ 1581/20014 (  8%)], Train Loss: 0.84570\n","Epoch: 00 [ 1591/20014 (  8%)], Train Loss: 0.84161\n","Epoch: 00 [ 1601/20014 (  8%)], Train Loss: 0.84114\n","Epoch: 00 [ 1611/20014 (  8%)], Train Loss: 0.83931\n","Epoch: 00 [ 1621/20014 (  8%)], Train Loss: 0.84072\n","Epoch: 00 [ 1631/20014 (  8%)], Train Loss: 0.83726\n","Epoch: 00 [ 1641/20014 (  8%)], Train Loss: 0.83543\n","Epoch: 00 [ 1651/20014 (  8%)], Train Loss: 0.83702\n","Epoch: 00 [ 1661/20014 (  8%)], Train Loss: 0.83750\n","Epoch: 00 [ 1671/20014 (  8%)], Train Loss: 0.83618\n","Epoch: 00 [ 1681/20014 (  8%)], Train Loss: 0.83384\n","Epoch: 00 [ 1691/20014 (  8%)], Train Loss: 0.83184\n","Epoch: 00 [ 1701/20014 (  8%)], Train Loss: 0.83085\n","Epoch: 00 [ 1711/20014 (  9%)], Train Loss: 0.82886\n","Epoch: 00 [ 1721/20014 (  9%)], Train Loss: 0.82821\n","Epoch: 00 [ 1731/20014 (  9%)], Train Loss: 0.82641\n","Epoch: 00 [ 1741/20014 (  9%)], Train Loss: 0.82614\n","Epoch: 00 [ 1751/20014 (  9%)], Train Loss: 0.82295\n","Epoch: 00 [ 1761/20014 (  9%)], Train Loss: 0.82111\n","Epoch: 00 [ 1771/20014 (  9%)], Train Loss: 0.82085\n","Epoch: 00 [ 1781/20014 (  9%)], Train Loss: 0.81823\n","Epoch: 00 [ 1791/20014 (  9%)], Train Loss: 0.81711\n","Epoch: 00 [ 1801/20014 (  9%)], Train Loss: 0.81612\n","Epoch: 00 [ 1811/20014 (  9%)], Train Loss: 0.81416\n","Epoch: 00 [ 1821/20014 (  9%)], Train Loss: 0.81485\n","Epoch: 00 [ 1831/20014 (  9%)], Train Loss: 0.81179\n","Epoch: 00 [ 1841/20014 (  9%)], Train Loss: 0.81080\n","Epoch: 00 [ 1851/20014 (  9%)], Train Loss: 0.80793\n","Epoch: 00 [ 1861/20014 (  9%)], Train Loss: 0.80635\n","Epoch: 00 [ 1871/20014 (  9%)], Train Loss: 0.80464\n","Epoch: 00 [ 1881/20014 (  9%)], Train Loss: 0.80074\n","Epoch: 00 [ 1891/20014 (  9%)], Train Loss: 0.79915\n","Epoch: 00 [ 1901/20014 (  9%)], Train Loss: 0.79557\n","Epoch: 00 [ 1911/20014 ( 10%)], Train Loss: 0.79261\n","Epoch: 00 [ 1921/20014 ( 10%)], Train Loss: 0.79106\n","Epoch: 00 [ 1931/20014 ( 10%)], Train Loss: 0.78952\n","Epoch: 00 [ 1941/20014 ( 10%)], Train Loss: 0.79003\n","Epoch: 00 [ 1951/20014 ( 10%)], Train Loss: 0.79114\n","Epoch: 00 [ 1961/20014 ( 10%)], Train Loss: 0.79257\n","Epoch: 00 [ 1971/20014 ( 10%)], Train Loss: 0.79035\n","Epoch: 00 [ 1981/20014 ( 10%)], Train Loss: 0.78968\n","Epoch: 00 [ 1991/20014 ( 10%)], Train Loss: 0.78838\n","Epoch: 00 [ 2001/20014 ( 10%)], Train Loss: 0.78768\n","Epoch: 00 [ 2011/20014 ( 10%)], Train Loss: 0.78573\n","Epoch: 00 [ 2021/20014 ( 10%)], Train Loss: 0.78413\n","Epoch: 00 [ 2031/20014 ( 10%)], Train Loss: 0.78207\n","Epoch: 00 [ 2041/20014 ( 10%)], Train Loss: 0.78458\n","Epoch: 00 [ 2051/20014 ( 10%)], Train Loss: 0.78284\n","Epoch: 00 [ 2061/20014 ( 10%)], Train Loss: 0.78049\n","Epoch: 00 [ 2071/20014 ( 10%)], Train Loss: 0.77733\n","Epoch: 00 [ 2081/20014 ( 10%)], Train Loss: 0.77422\n","Epoch: 00 [ 2091/20014 ( 10%)], Train Loss: 0.77340\n","Epoch: 00 [ 2101/20014 ( 10%)], Train Loss: 0.77291\n","Epoch: 00 [ 2111/20014 ( 11%)], Train Loss: 0.77429\n","Epoch: 00 [ 2121/20014 ( 11%)], Train Loss: 0.77229\n","Epoch: 00 [ 2131/20014 ( 11%)], Train Loss: 0.77231\n","Epoch: 00 [ 2141/20014 ( 11%)], Train Loss: 0.77365\n","Epoch: 00 [ 2151/20014 ( 11%)], Train Loss: 0.77321\n","Epoch: 00 [ 2161/20014 ( 11%)], Train Loss: 0.77540\n","Epoch: 00 [ 2171/20014 ( 11%)], Train Loss: 0.77318\n","Epoch: 00 [ 2181/20014 ( 11%)], Train Loss: 0.77374\n","Epoch: 00 [ 2191/20014 ( 11%)], Train Loss: 0.77227\n","Epoch: 00 [ 2201/20014 ( 11%)], Train Loss: 0.77190\n","Epoch: 00 [ 2211/20014 ( 11%)], Train Loss: 0.77070\n","Epoch: 00 [ 2221/20014 ( 11%)], Train Loss: 0.76964\n","Epoch: 00 [ 2231/20014 ( 11%)], Train Loss: 0.76624\n","Epoch: 00 [ 2241/20014 ( 11%)], Train Loss: 0.76594\n","Epoch: 00 [ 2251/20014 ( 11%)], Train Loss: 0.76394\n","Epoch: 00 [ 2261/20014 ( 11%)], Train Loss: 0.76180\n","Epoch: 00 [ 2271/20014 ( 11%)], Train Loss: 0.76337\n","Epoch: 00 [ 2281/20014 ( 11%)], Train Loss: 0.76178\n","Epoch: 00 [ 2291/20014 ( 11%)], Train Loss: 0.76084\n","Epoch: 00 [ 2301/20014 ( 11%)], Train Loss: 0.76191\n","Epoch: 00 [ 2311/20014 ( 12%)], Train Loss: 0.76039\n","Epoch: 00 [ 2321/20014 ( 12%)], Train Loss: 0.75801\n","Epoch: 00 [ 2331/20014 ( 12%)], Train Loss: 0.75735\n","Epoch: 00 [ 2341/20014 ( 12%)], Train Loss: 0.75554\n","Epoch: 00 [ 2351/20014 ( 12%)], Train Loss: 0.75389\n","Epoch: 00 [ 2361/20014 ( 12%)], Train Loss: 0.75170\n","Epoch: 00 [ 2371/20014 ( 12%)], Train Loss: 0.75023\n","Epoch: 00 [ 2381/20014 ( 12%)], Train Loss: 0.74762\n","Epoch: 00 [ 2391/20014 ( 12%)], Train Loss: 0.74602\n","Epoch: 00 [ 2401/20014 ( 12%)], Train Loss: 0.74362\n","Epoch: 00 [ 2411/20014 ( 12%)], Train Loss: 0.74202\n","Epoch: 00 [ 2421/20014 ( 12%)], Train Loss: 0.74171\n","Epoch: 00 [ 2431/20014 ( 12%)], Train Loss: 0.74049\n","Epoch: 00 [ 2441/20014 ( 12%)], Train Loss: 0.74189\n","Epoch: 00 [ 2451/20014 ( 12%)], Train Loss: 0.74118\n","Epoch: 00 [ 2461/20014 ( 12%)], Train Loss: 0.74157\n","Epoch: 00 [ 2471/20014 ( 12%)], Train Loss: 0.74199\n","Epoch: 00 [ 2481/20014 ( 12%)], Train Loss: 0.74052\n","Epoch: 00 [ 2491/20014 ( 12%)], Train Loss: 0.73951\n","Epoch: 00 [ 2501/20014 ( 12%)], Train Loss: 0.74189\n","Epoch: 00 [ 2511/20014 ( 13%)], Train Loss: 0.74052\n","Epoch: 00 [ 2521/20014 ( 13%)], Train Loss: 0.74010\n","Epoch: 00 [ 2531/20014 ( 13%)], Train Loss: 0.74074\n","Epoch: 00 [ 2541/20014 ( 13%)], Train Loss: 0.74051\n","Epoch: 00 [ 2551/20014 ( 13%)], Train Loss: 0.73903\n","Epoch: 00 [ 2561/20014 ( 13%)], Train Loss: 0.73664\n","Epoch: 00 [ 2571/20014 ( 13%)], Train Loss: 0.73663\n","Epoch: 00 [ 2581/20014 ( 13%)], Train Loss: 0.73558\n","Epoch: 00 [ 2591/20014 ( 13%)], Train Loss: 0.73323\n","Epoch: 00 [ 2601/20014 ( 13%)], Train Loss: 0.73118\n","Epoch: 00 [ 2611/20014 ( 13%)], Train Loss: 0.72921\n","Epoch: 00 [ 2621/20014 ( 13%)], Train Loss: 0.72739\n","Epoch: 00 [ 2631/20014 ( 13%)], Train Loss: 0.72631\n","Epoch: 00 [ 2641/20014 ( 13%)], Train Loss: 0.72418\n","Epoch: 00 [ 2651/20014 ( 13%)], Train Loss: 0.72344\n","Epoch: 00 [ 2661/20014 ( 13%)], Train Loss: 0.72355\n","Epoch: 00 [ 2671/20014 ( 13%)], Train Loss: 0.72347\n","Epoch: 00 [ 2681/20014 ( 13%)], Train Loss: 0.72240\n","Epoch: 00 [ 2691/20014 ( 13%)], Train Loss: 0.72247\n","Epoch: 00 [ 2701/20014 ( 13%)], Train Loss: 0.72163\n","Epoch: 00 [ 2711/20014 ( 14%)], Train Loss: 0.71952\n","Epoch: 00 [ 2721/20014 ( 14%)], Train Loss: 0.71853\n","Epoch: 00 [ 2731/20014 ( 14%)], Train Loss: 0.71800\n","Epoch: 00 [ 2741/20014 ( 14%)], Train Loss: 0.71638\n","Epoch: 00 [ 2751/20014 ( 14%)], Train Loss: 0.71680\n","Epoch: 00 [ 2761/20014 ( 14%)], Train Loss: 0.71640\n","Epoch: 00 [ 2771/20014 ( 14%)], Train Loss: 0.71516\n","Epoch: 00 [ 2781/20014 ( 14%)], Train Loss: 0.71397\n","Epoch: 00 [ 2791/20014 ( 14%)], Train Loss: 0.71444\n","Epoch: 00 [ 2801/20014 ( 14%)], Train Loss: 0.71223\n","Epoch: 00 [ 2811/20014 ( 14%)], Train Loss: 0.71396\n","Epoch: 00 [ 2821/20014 ( 14%)], Train Loss: 0.71227\n","Epoch: 00 [ 2831/20014 ( 14%)], Train Loss: 0.71116\n","Epoch: 00 [ 2841/20014 ( 14%)], Train Loss: 0.71111\n","Epoch: 00 [ 2851/20014 ( 14%)], Train Loss: 0.70919\n","Epoch: 00 [ 2861/20014 ( 14%)], Train Loss: 0.70805\n","Epoch: 00 [ 2871/20014 ( 14%)], Train Loss: 0.70736\n","Epoch: 00 [ 2881/20014 ( 14%)], Train Loss: 0.70855\n","Epoch: 00 [ 2891/20014 ( 14%)], Train Loss: 0.70669\n","Epoch: 00 [ 2901/20014 ( 14%)], Train Loss: 0.70587\n","Epoch: 00 [ 2911/20014 ( 15%)], Train Loss: 0.70492\n","Epoch: 00 [ 2921/20014 ( 15%)], Train Loss: 0.70317\n","Epoch: 00 [ 2931/20014 ( 15%)], Train Loss: 0.70266\n","Epoch: 00 [ 2941/20014 ( 15%)], Train Loss: 0.70233\n","Epoch: 00 [ 2951/20014 ( 15%)], Train Loss: 0.70081\n","Epoch: 00 [ 2961/20014 ( 15%)], Train Loss: 0.70023\n","Epoch: 00 [ 2971/20014 ( 15%)], Train Loss: 0.70149\n","Epoch: 00 [ 2981/20014 ( 15%)], Train Loss: 0.69958\n","Epoch: 00 [ 2991/20014 ( 15%)], Train Loss: 0.69976\n","Epoch: 00 [ 3001/20014 ( 15%)], Train Loss: 0.70030\n","Epoch: 00 [ 3011/20014 ( 15%)], Train Loss: 0.70016\n","Epoch: 00 [ 3021/20014 ( 15%)], Train Loss: 0.70023\n","Epoch: 00 [ 3031/20014 ( 15%)], Train Loss: 0.69933\n","Epoch: 00 [ 3041/20014 ( 15%)], Train Loss: 0.69830\n","Epoch: 00 [ 3051/20014 ( 15%)], Train Loss: 0.69901\n","Epoch: 00 [ 3061/20014 ( 15%)], Train Loss: 0.69762\n","Epoch: 00 [ 3071/20014 ( 15%)], Train Loss: 0.69817\n","Epoch: 00 [ 3081/20014 ( 15%)], Train Loss: 0.69788\n","Epoch: 00 [ 3091/20014 ( 15%)], Train Loss: 0.69841\n","Epoch: 00 [ 3101/20014 ( 15%)], Train Loss: 0.69723\n","Epoch: 00 [ 3111/20014 ( 16%)], Train Loss: 0.69624\n","Epoch: 00 [ 3121/20014 ( 16%)], Train Loss: 0.69513\n","Epoch: 00 [ 3131/20014 ( 16%)], Train Loss: 0.69356\n","Epoch: 00 [ 3141/20014 ( 16%)], Train Loss: 0.69423\n","Epoch: 00 [ 3151/20014 ( 16%)], Train Loss: 0.69302\n","Epoch: 00 [ 3161/20014 ( 16%)], Train Loss: 0.69328\n","Epoch: 00 [ 3171/20014 ( 16%)], Train Loss: 0.69330\n","Epoch: 00 [ 3181/20014 ( 16%)], Train Loss: 0.69273\n","Epoch: 00 [ 3191/20014 ( 16%)], Train Loss: 0.69263\n","Epoch: 00 [ 3201/20014 ( 16%)], Train Loss: 0.69122\n","Epoch: 00 [ 3211/20014 ( 16%)], Train Loss: 0.69006\n","Epoch: 00 [ 3221/20014 ( 16%)], Train Loss: 0.68925\n","Epoch: 00 [ 3231/20014 ( 16%)], Train Loss: 0.69167\n","Epoch: 00 [ 3241/20014 ( 16%)], Train Loss: 0.69099\n","Epoch: 00 [ 3251/20014 ( 16%)], Train Loss: 0.69022\n","Epoch: 00 [ 3261/20014 ( 16%)], Train Loss: 0.68956\n","Epoch: 00 [ 3271/20014 ( 16%)], Train Loss: 0.68918\n","Epoch: 00 [ 3281/20014 ( 16%)], Train Loss: 0.68952\n","Epoch: 00 [ 3291/20014 ( 16%)], Train Loss: 0.68944\n","Epoch: 00 [ 3301/20014 ( 16%)], Train Loss: 0.68967\n","Epoch: 00 [ 3311/20014 ( 17%)], Train Loss: 0.68800\n","Epoch: 00 [ 3321/20014 ( 17%)], Train Loss: 0.68881\n","Epoch: 00 [ 3331/20014 ( 17%)], Train Loss: 0.68775\n","Epoch: 00 [ 3341/20014 ( 17%)], Train Loss: 0.68762\n","Epoch: 00 [ 3351/20014 ( 17%)], Train Loss: 0.68700\n","Epoch: 00 [ 3361/20014 ( 17%)], Train Loss: 0.68741\n","Epoch: 00 [ 3371/20014 ( 17%)], Train Loss: 0.68642\n","Epoch: 00 [ 3381/20014 ( 17%)], Train Loss: 0.68615\n","Epoch: 00 [ 3391/20014 ( 17%)], Train Loss: 0.68509\n","Epoch: 00 [ 3401/20014 ( 17%)], Train Loss: 0.68492\n","Epoch: 00 [ 3411/20014 ( 17%)], Train Loss: 0.68373\n","Epoch: 00 [ 3421/20014 ( 17%)], Train Loss: 0.68383\n","Epoch: 00 [ 3431/20014 ( 17%)], Train Loss: 0.68428\n","Epoch: 00 [ 3441/20014 ( 17%)], Train Loss: 0.68266\n","Epoch: 00 [ 3451/20014 ( 17%)], Train Loss: 0.68121\n","Epoch: 00 [ 3461/20014 ( 17%)], Train Loss: 0.67988\n","Epoch: 00 [ 3471/20014 ( 17%)], Train Loss: 0.68123\n","Epoch: 00 [ 3481/20014 ( 17%)], Train Loss: 0.67985\n","Epoch: 00 [ 3491/20014 ( 17%)], Train Loss: 0.67942\n","Epoch: 00 [ 3501/20014 ( 17%)], Train Loss: 0.67905\n","Epoch: 00 [ 3511/20014 ( 18%)], Train Loss: 0.67917\n","Epoch: 00 [ 3521/20014 ( 18%)], Train Loss: 0.67947\n","Epoch: 00 [ 3531/20014 ( 18%)], Train Loss: 0.67918\n","Epoch: 00 [ 3541/20014 ( 18%)], Train Loss: 0.67803\n","Epoch: 00 [ 3551/20014 ( 18%)], Train Loss: 0.67729\n","Epoch: 00 [ 3561/20014 ( 18%)], Train Loss: 0.67576\n","Epoch: 00 [ 3571/20014 ( 18%)], Train Loss: 0.67531\n","Epoch: 00 [ 3581/20014 ( 18%)], Train Loss: 0.67361\n","Epoch: 00 [ 3591/20014 ( 18%)], Train Loss: 0.67319\n","Epoch: 00 [ 3601/20014 ( 18%)], Train Loss: 0.67195\n","Epoch: 00 [ 3611/20014 ( 18%)], Train Loss: 0.67291\n","Epoch: 00 [ 3621/20014 ( 18%)], Train Loss: 0.67254\n","Epoch: 00 [ 3631/20014 ( 18%)], Train Loss: 0.67189\n","Epoch: 00 [ 3641/20014 ( 18%)], Train Loss: 0.67174\n","Epoch: 00 [ 3651/20014 ( 18%)], Train Loss: 0.67098\n","Epoch: 00 [ 3661/20014 ( 18%)], Train Loss: 0.67063\n","Epoch: 00 [ 3671/20014 ( 18%)], Train Loss: 0.66982\n","Epoch: 00 [ 3681/20014 ( 18%)], Train Loss: 0.66960\n","Epoch: 00 [ 3691/20014 ( 18%)], Train Loss: 0.67007\n","Epoch: 00 [ 3701/20014 ( 18%)], Train Loss: 0.67002\n","Epoch: 00 [ 3711/20014 ( 19%)], Train Loss: 0.66884\n","Epoch: 00 [ 3721/20014 ( 19%)], Train Loss: 0.66909\n","Epoch: 00 [ 3731/20014 ( 19%)], Train Loss: 0.66881\n","Epoch: 00 [ 3741/20014 ( 19%)], Train Loss: 0.66830\n","Epoch: 00 [ 3751/20014 ( 19%)], Train Loss: 0.66709\n","Epoch: 00 [ 3761/20014 ( 19%)], Train Loss: 0.66613\n","Epoch: 00 [ 3771/20014 ( 19%)], Train Loss: 0.66713\n","Epoch: 00 [ 3781/20014 ( 19%)], Train Loss: 0.66672\n","Epoch: 00 [ 3791/20014 ( 19%)], Train Loss: 0.66559\n","Epoch: 00 [ 3801/20014 ( 19%)], Train Loss: 0.66616\n","Epoch: 00 [ 3811/20014 ( 19%)], Train Loss: 0.66732\n","Epoch: 00 [ 3821/20014 ( 19%)], Train Loss: 0.66725\n","Epoch: 00 [ 3831/20014 ( 19%)], Train Loss: 0.66618\n","Epoch: 00 [ 3841/20014 ( 19%)], Train Loss: 0.66540\n","Epoch: 00 [ 3851/20014 ( 19%)], Train Loss: 0.66619\n","Epoch: 00 [ 3861/20014 ( 19%)], Train Loss: 0.66547\n","Epoch: 00 [ 3871/20014 ( 19%)], Train Loss: 0.66452\n","Epoch: 00 [ 3881/20014 ( 19%)], Train Loss: 0.66338\n","Epoch: 00 [ 3891/20014 ( 19%)], Train Loss: 0.66293\n","Epoch: 00 [ 3901/20014 ( 19%)], Train Loss: 0.66165\n","Epoch: 00 [ 3911/20014 ( 20%)], Train Loss: 0.66078\n","Epoch: 00 [ 3921/20014 ( 20%)], Train Loss: 0.66000\n","Epoch: 00 [ 3931/20014 ( 20%)], Train Loss: 0.66058\n","Epoch: 00 [ 3941/20014 ( 20%)], Train Loss: 0.65990\n","Epoch: 00 [ 3951/20014 ( 20%)], Train Loss: 0.66019\n","Epoch: 00 [ 3961/20014 ( 20%)], Train Loss: 0.65921\n","Epoch: 00 [ 3971/20014 ( 20%)], Train Loss: 0.65890\n","Epoch: 00 [ 3981/20014 ( 20%)], Train Loss: 0.65816\n","Epoch: 00 [ 3991/20014 ( 20%)], Train Loss: 0.65902\n","Epoch: 00 [ 4001/20014 ( 20%)], Train Loss: 0.65924\n","Epoch: 00 [ 4011/20014 ( 20%)], Train Loss: 0.66020\n","Epoch: 00 [ 4021/20014 ( 20%)], Train Loss: 0.66021\n","Epoch: 00 [ 4031/20014 ( 20%)], Train Loss: 0.65998\n","Epoch: 00 [ 4041/20014 ( 20%)], Train Loss: 0.65960\n","Epoch: 00 [ 4051/20014 ( 20%)], Train Loss: 0.65917\n","Epoch: 00 [ 4061/20014 ( 20%)], Train Loss: 0.65810\n","Epoch: 00 [ 4071/20014 ( 20%)], Train Loss: 0.65866\n","Epoch: 00 [ 4081/20014 ( 20%)], Train Loss: 0.65744\n","Epoch: 00 [ 4091/20014 ( 20%)], Train Loss: 0.65774\n","Epoch: 00 [ 4101/20014 ( 20%)], Train Loss: 0.65651\n","Epoch: 00 [ 4111/20014 ( 21%)], Train Loss: 0.65595\n","Epoch: 00 [ 4121/20014 ( 21%)], Train Loss: 0.65496\n","Epoch: 00 [ 4131/20014 ( 21%)], Train Loss: 0.65436\n","Epoch: 00 [ 4141/20014 ( 21%)], Train Loss: 0.65386\n","Epoch: 00 [ 4151/20014 ( 21%)], Train Loss: 0.65555\n","Epoch: 00 [ 4161/20014 ( 21%)], Train Loss: 0.65577\n","Epoch: 00 [ 4171/20014 ( 21%)], Train Loss: 0.65562\n","Epoch: 00 [ 4181/20014 ( 21%)], Train Loss: 0.65485\n","Epoch: 00 [ 4191/20014 ( 21%)], Train Loss: 0.65462\n","Epoch: 00 [ 4201/20014 ( 21%)], Train Loss: 0.65484\n","Epoch: 00 [ 4211/20014 ( 21%)], Train Loss: 0.65462\n","Epoch: 00 [ 4221/20014 ( 21%)], Train Loss: 0.65409\n","Epoch: 00 [ 4231/20014 ( 21%)], Train Loss: 0.65293\n","Epoch: 00 [ 4241/20014 ( 21%)], Train Loss: 0.65271\n","Epoch: 00 [ 4251/20014 ( 21%)], Train Loss: 0.65253\n","Epoch: 00 [ 4261/20014 ( 21%)], Train Loss: 0.65140\n","Epoch: 00 [ 4271/20014 ( 21%)], Train Loss: 0.65005\n","Epoch: 00 [ 4281/20014 ( 21%)], Train Loss: 0.64916\n","Epoch: 00 [ 4291/20014 ( 21%)], Train Loss: 0.64813\n","Epoch: 00 [ 4301/20014 ( 21%)], Train Loss: 0.64786\n","Epoch: 00 [ 4311/20014 ( 22%)], Train Loss: 0.64698\n","Epoch: 00 [ 4321/20014 ( 22%)], Train Loss: 0.64759\n","Epoch: 00 [ 4331/20014 ( 22%)], Train Loss: 0.64716\n","Epoch: 00 [ 4341/20014 ( 22%)], Train Loss: 0.64629\n","Epoch: 00 [ 4351/20014 ( 22%)], Train Loss: 0.64579\n","Epoch: 00 [ 4361/20014 ( 22%)], Train Loss: 0.64545\n","Epoch: 00 [ 4371/20014 ( 22%)], Train Loss: 0.64465\n","Epoch: 00 [ 4381/20014 ( 22%)], Train Loss: 0.64452\n","Epoch: 00 [ 4391/20014 ( 22%)], Train Loss: 0.64433\n","Epoch: 00 [ 4401/20014 ( 22%)], Train Loss: 0.64371\n","Epoch: 00 [ 4411/20014 ( 22%)], Train Loss: 0.64349\n","Epoch: 00 [ 4421/20014 ( 22%)], Train Loss: 0.64248\n","Epoch: 00 [ 4431/20014 ( 22%)], Train Loss: 0.64232\n","Epoch: 00 [ 4441/20014 ( 22%)], Train Loss: 0.64180\n","Epoch: 00 [ 4451/20014 ( 22%)], Train Loss: 0.64084\n","Epoch: 00 [ 4461/20014 ( 22%)], Train Loss: 0.64043\n","Epoch: 00 [ 4471/20014 ( 22%)], Train Loss: 0.64039\n","Epoch: 00 [ 4481/20014 ( 22%)], Train Loss: 0.63946\n","Epoch: 00 [ 4491/20014 ( 22%)], Train Loss: 0.63884\n","Epoch: 00 [ 4501/20014 ( 22%)], Train Loss: 0.63806\n","Epoch: 00 [ 4511/20014 ( 23%)], Train Loss: 0.63763\n","Epoch: 00 [ 4521/20014 ( 23%)], Train Loss: 0.63691\n","Epoch: 00 [ 4531/20014 ( 23%)], Train Loss: 0.63635\n","Epoch: 00 [ 4541/20014 ( 23%)], Train Loss: 0.63615\n","Epoch: 00 [ 4551/20014 ( 23%)], Train Loss: 0.63683\n","Epoch: 00 [ 4561/20014 ( 23%)], Train Loss: 0.63579\n","Epoch: 00 [ 4571/20014 ( 23%)], Train Loss: 0.63555\n","Epoch: 00 [ 4581/20014 ( 23%)], Train Loss: 0.63543\n","Epoch: 00 [ 4591/20014 ( 23%)], Train Loss: 0.63452\n","Epoch: 00 [ 4601/20014 ( 23%)], Train Loss: 0.63460\n","Epoch: 00 [ 4611/20014 ( 23%)], Train Loss: 0.63428\n","Epoch: 00 [ 4621/20014 ( 23%)], Train Loss: 0.63368\n","Epoch: 00 [ 4631/20014 ( 23%)], Train Loss: 0.63313\n","Epoch: 00 [ 4641/20014 ( 23%)], Train Loss: 0.63240\n","Epoch: 00 [ 4651/20014 ( 23%)], Train Loss: 0.63240\n","Epoch: 00 [ 4661/20014 ( 23%)], Train Loss: 0.63177\n","Epoch: 00 [ 4671/20014 ( 23%)], Train Loss: 0.63072\n","Epoch: 00 [ 4681/20014 ( 23%)], Train Loss: 0.63158\n","Epoch: 00 [ 4691/20014 ( 23%)], Train Loss: 0.63166\n","Epoch: 00 [ 4701/20014 ( 23%)], Train Loss: 0.63116\n","Epoch: 00 [ 4711/20014 ( 24%)], Train Loss: 0.63063\n","Epoch: 00 [ 4721/20014 ( 24%)], Train Loss: 0.63153\n","Epoch: 00 [ 4731/20014 ( 24%)], Train Loss: 0.63138\n","Epoch: 00 [ 4741/20014 ( 24%)], Train Loss: 0.63176\n","Epoch: 00 [ 4751/20014 ( 24%)], Train Loss: 0.63124\n","Epoch: 00 [ 4761/20014 ( 24%)], Train Loss: 0.63191\n","Epoch: 00 [ 4771/20014 ( 24%)], Train Loss: 0.63262\n","Epoch: 00 [ 4781/20014 ( 24%)], Train Loss: 0.63253\n","Epoch: 00 [ 4791/20014 ( 24%)], Train Loss: 0.63280\n","Epoch: 00 [ 4801/20014 ( 24%)], Train Loss: 0.63290\n","Epoch: 00 [ 4811/20014 ( 24%)], Train Loss: 0.63312\n","Epoch: 00 [ 4821/20014 ( 24%)], Train Loss: 0.63368\n","Epoch: 00 [ 4831/20014 ( 24%)], Train Loss: 0.63332\n","Epoch: 00 [ 4841/20014 ( 24%)], Train Loss: 0.63244\n","Epoch: 00 [ 4851/20014 ( 24%)], Train Loss: 0.63190\n","Epoch: 00 [ 4861/20014 ( 24%)], Train Loss: 0.63101\n","Epoch: 00 [ 4871/20014 ( 24%)], Train Loss: 0.63051\n","Epoch: 00 [ 4881/20014 ( 24%)], Train Loss: 0.62972\n","Epoch: 00 [ 4891/20014 ( 24%)], Train Loss: 0.62965\n","Epoch: 00 [ 4901/20014 ( 24%)], Train Loss: 0.62859\n","Epoch: 00 [ 4911/20014 ( 25%)], Train Loss: 0.62851\n","Epoch: 00 [ 4921/20014 ( 25%)], Train Loss: 0.62921\n","Epoch: 00 [ 4931/20014 ( 25%)], Train Loss: 0.63008\n","Epoch: 00 [ 4941/20014 ( 25%)], Train Loss: 0.63044\n","Epoch: 00 [ 4951/20014 ( 25%)], Train Loss: 0.63045\n","Epoch: 00 [ 4961/20014 ( 25%)], Train Loss: 0.63029\n","Epoch: 00 [ 4971/20014 ( 25%)], Train Loss: 0.62980\n","Epoch: 00 [ 4981/20014 ( 25%)], Train Loss: 0.62859\n","Epoch: 00 [ 4991/20014 ( 25%)], Train Loss: 0.62817\n","Epoch: 00 [ 5001/20014 ( 25%)], Train Loss: 0.62932\n","Epoch: 00 [ 5011/20014 ( 25%)], Train Loss: 0.62948\n","Epoch: 00 [ 5021/20014 ( 25%)], Train Loss: 0.62937\n","Epoch: 00 [ 5031/20014 ( 25%)], Train Loss: 0.62840\n","Epoch: 00 [ 5041/20014 ( 25%)], Train Loss: 0.62797\n","Epoch: 00 [ 5051/20014 ( 25%)], Train Loss: 0.62764\n","Epoch: 00 [ 5061/20014 ( 25%)], Train Loss: 0.62721\n","Epoch: 00 [ 5071/20014 ( 25%)], Train Loss: 0.62754\n","Epoch: 00 [ 5081/20014 ( 25%)], Train Loss: 0.62721\n","Epoch: 00 [ 5091/20014 ( 25%)], Train Loss: 0.62630\n","Epoch: 00 [ 5101/20014 ( 25%)], Train Loss: 0.62604\n","Epoch: 00 [ 5111/20014 ( 26%)], Train Loss: 0.62576\n","Epoch: 00 [ 5121/20014 ( 26%)], Train Loss: 0.62525\n","Epoch: 00 [ 5131/20014 ( 26%)], Train Loss: 0.62475\n","Epoch: 00 [ 5141/20014 ( 26%)], Train Loss: 0.62606\n","Epoch: 00 [ 5151/20014 ( 26%)], Train Loss: 0.62620\n","Epoch: 00 [ 5161/20014 ( 26%)], Train Loss: 0.62584\n","Epoch: 00 [ 5171/20014 ( 26%)], Train Loss: 0.62574\n","Epoch: 00 [ 5181/20014 ( 26%)], Train Loss: 0.62613\n","Epoch: 00 [ 5191/20014 ( 26%)], Train Loss: 0.62695\n","Epoch: 00 [ 5201/20014 ( 26%)], Train Loss: 0.62655\n","Epoch: 00 [ 5211/20014 ( 26%)], Train Loss: 0.62647\n","Epoch: 00 [ 5221/20014 ( 26%)], Train Loss: 0.62624\n","Epoch: 00 [ 5231/20014 ( 26%)], Train Loss: 0.62661\n","Epoch: 00 [ 5241/20014 ( 26%)], Train Loss: 0.62696\n","Epoch: 00 [ 5251/20014 ( 26%)], Train Loss: 0.62661\n","Epoch: 00 [ 5261/20014 ( 26%)], Train Loss: 0.62609\n","Epoch: 00 [ 5271/20014 ( 26%)], Train Loss: 0.62539\n","Epoch: 00 [ 5281/20014 ( 26%)], Train Loss: 0.62463\n","Epoch: 00 [ 5291/20014 ( 26%)], Train Loss: 0.62488\n","Epoch: 00 [ 5301/20014 ( 26%)], Train Loss: 0.62487\n","Epoch: 00 [ 5311/20014 ( 27%)], Train Loss: 0.62408\n","Epoch: 00 [ 5321/20014 ( 27%)], Train Loss: 0.62365\n","Epoch: 00 [ 5331/20014 ( 27%)], Train Loss: 0.62312\n","Epoch: 00 [ 5341/20014 ( 27%)], Train Loss: 0.62215\n","Epoch: 00 [ 5351/20014 ( 27%)], Train Loss: 0.62248\n","Epoch: 00 [ 5361/20014 ( 27%)], Train Loss: 0.62239\n","Epoch: 00 [ 5371/20014 ( 27%)], Train Loss: 0.62182\n","Epoch: 00 [ 5381/20014 ( 27%)], Train Loss: 0.62222\n","Epoch: 00 [ 5391/20014 ( 27%)], Train Loss: 0.62251\n","Epoch: 00 [ 5401/20014 ( 27%)], Train Loss: 0.62172\n","Epoch: 00 [ 5411/20014 ( 27%)], Train Loss: 0.62170\n","Epoch: 00 [ 5421/20014 ( 27%)], Train Loss: 0.62078\n","Epoch: 00 [ 5431/20014 ( 27%)], Train Loss: 0.61982\n","Epoch: 00 [ 5441/20014 ( 27%)], Train Loss: 0.61910\n","Epoch: 00 [ 5451/20014 ( 27%)], Train Loss: 0.61827\n","Epoch: 00 [ 5461/20014 ( 27%)], Train Loss: 0.61856\n","Epoch: 00 [ 5471/20014 ( 27%)], Train Loss: 0.61820\n","Epoch: 00 [ 5481/20014 ( 27%)], Train Loss: 0.61772\n","Epoch: 00 [ 5491/20014 ( 27%)], Train Loss: 0.61847\n","Epoch: 00 [ 5501/20014 ( 27%)], Train Loss: 0.61875\n","Epoch: 00 [ 5511/20014 ( 28%)], Train Loss: 0.61807\n","Epoch: 00 [ 5521/20014 ( 28%)], Train Loss: 0.61855\n","Epoch: 00 [ 5531/20014 ( 28%)], Train Loss: 0.61835\n","Epoch: 00 [ 5541/20014 ( 28%)], Train Loss: 0.61788\n","Epoch: 00 [ 5551/20014 ( 28%)], Train Loss: 0.61740\n","Epoch: 00 [ 5561/20014 ( 28%)], Train Loss: 0.61677\n","Epoch: 00 [ 5571/20014 ( 28%)], Train Loss: 0.61674\n","Epoch: 00 [ 5581/20014 ( 28%)], Train Loss: 0.61589\n","Epoch: 00 [ 5591/20014 ( 28%)], Train Loss: 0.61599\n","Epoch: 00 [ 5601/20014 ( 28%)], Train Loss: 0.61520\n","Epoch: 00 [ 5611/20014 ( 28%)], Train Loss: 0.61516\n","Epoch: 00 [ 5621/20014 ( 28%)], Train Loss: 0.61447\n","Epoch: 00 [ 5631/20014 ( 28%)], Train Loss: 0.61372\n","Epoch: 00 [ 5641/20014 ( 28%)], Train Loss: 0.61415\n","Epoch: 00 [ 5651/20014 ( 28%)], Train Loss: 0.61358\n","Epoch: 00 [ 5661/20014 ( 28%)], Train Loss: 0.61308\n","Epoch: 00 [ 5671/20014 ( 28%)], Train Loss: 0.61319\n","Epoch: 00 [ 5681/20014 ( 28%)], Train Loss: 0.61282\n","Epoch: 00 [ 5691/20014 ( 28%)], Train Loss: 0.61381\n","Epoch: 00 [ 5701/20014 ( 28%)], Train Loss: 0.61446\n","Epoch: 00 [ 5711/20014 ( 29%)], Train Loss: 0.61462\n","Epoch: 00 [ 5721/20014 ( 29%)], Train Loss: 0.61424\n","Epoch: 00 [ 5731/20014 ( 29%)], Train Loss: 0.61393\n","Epoch: 00 [ 5741/20014 ( 29%)], Train Loss: 0.61324\n","Epoch: 00 [ 5751/20014 ( 29%)], Train Loss: 0.61300\n","Epoch: 00 [ 5761/20014 ( 29%)], Train Loss: 0.61360\n","Epoch: 00 [ 5771/20014 ( 29%)], Train Loss: 0.61304\n","Epoch: 00 [ 5781/20014 ( 29%)], Train Loss: 0.61327\n","Epoch: 00 [ 5791/20014 ( 29%)], Train Loss: 0.61305\n","Epoch: 00 [ 5801/20014 ( 29%)], Train Loss: 0.61345\n","Epoch: 00 [ 5811/20014 ( 29%)], Train Loss: 0.61352\n","Epoch: 00 [ 5821/20014 ( 29%)], Train Loss: 0.61282\n","Epoch: 00 [ 5831/20014 ( 29%)], Train Loss: 0.61228\n","Epoch: 00 [ 5841/20014 ( 29%)], Train Loss: 0.61194\n","Epoch: 00 [ 5851/20014 ( 29%)], Train Loss: 0.61162\n","Epoch: 00 [ 5861/20014 ( 29%)], Train Loss: 0.61160\n","Epoch: 00 [ 5871/20014 ( 29%)], Train Loss: 0.61155\n","Epoch: 00 [ 5881/20014 ( 29%)], Train Loss: 0.61158\n","Epoch: 00 [ 5891/20014 ( 29%)], Train Loss: 0.61089\n","Epoch: 00 [ 5901/20014 ( 29%)], Train Loss: 0.61048\n","Epoch: 00 [ 5911/20014 ( 30%)], Train Loss: 0.61101\n","Epoch: 00 [ 5921/20014 ( 30%)], Train Loss: 0.61067\n","Epoch: 00 [ 5931/20014 ( 30%)], Train Loss: 0.60983\n","Epoch: 00 [ 5941/20014 ( 30%)], Train Loss: 0.60993\n","Epoch: 00 [ 5951/20014 ( 30%)], Train Loss: 0.60966\n","Epoch: 00 [ 5961/20014 ( 30%)], Train Loss: 0.60965\n","Epoch: 00 [ 5971/20014 ( 30%)], Train Loss: 0.60966\n","Epoch: 00 [ 5981/20014 ( 30%)], Train Loss: 0.60905\n","Epoch: 00 [ 5991/20014 ( 30%)], Train Loss: 0.60885\n","Epoch: 00 [ 6001/20014 ( 30%)], Train Loss: 0.60903\n","Epoch: 00 [ 6011/20014 ( 30%)], Train Loss: 0.60847\n","Epoch: 00 [ 6021/20014 ( 30%)], Train Loss: 0.60821\n","Epoch: 00 [ 6031/20014 ( 30%)], Train Loss: 0.60786\n","Epoch: 00 [ 6041/20014 ( 30%)], Train Loss: 0.60756\n","Epoch: 00 [ 6051/20014 ( 30%)], Train Loss: 0.60752\n","Epoch: 00 [ 6061/20014 ( 30%)], Train Loss: 0.60689\n","Epoch: 00 [ 6071/20014 ( 30%)], Train Loss: 0.60625\n","Epoch: 00 [ 6081/20014 ( 30%)], Train Loss: 0.60563\n","Epoch: 00 [ 6091/20014 ( 30%)], Train Loss: 0.60596\n","Epoch: 00 [ 6101/20014 ( 30%)], Train Loss: 0.60526\n","Epoch: 00 [ 6111/20014 ( 31%)], Train Loss: 0.60476\n","Epoch: 00 [ 6121/20014 ( 31%)], Train Loss: 0.60468\n","Epoch: 00 [ 6131/20014 ( 31%)], Train Loss: 0.60494\n","Epoch: 00 [ 6141/20014 ( 31%)], Train Loss: 0.60538\n","Epoch: 00 [ 6151/20014 ( 31%)], Train Loss: 0.60465\n","Epoch: 00 [ 6161/20014 ( 31%)], Train Loss: 0.60446\n","Epoch: 00 [ 6171/20014 ( 31%)], Train Loss: 0.60391\n","Epoch: 00 [ 6181/20014 ( 31%)], Train Loss: 0.60441\n","Epoch: 00 [ 6191/20014 ( 31%)], Train Loss: 0.60429\n","Epoch: 00 [ 6201/20014 ( 31%)], Train Loss: 0.60414\n","Epoch: 00 [ 6211/20014 ( 31%)], Train Loss: 0.60396\n","Epoch: 00 [ 6221/20014 ( 31%)], Train Loss: 0.60451\n","Epoch: 00 [ 6231/20014 ( 31%)], Train Loss: 0.60433\n","Epoch: 00 [ 6241/20014 ( 31%)], Train Loss: 0.60381\n","Epoch: 00 [ 6251/20014 ( 31%)], Train Loss: 0.60434\n","Epoch: 00 [ 6261/20014 ( 31%)], Train Loss: 0.60416\n","Epoch: 00 [ 6271/20014 ( 31%)], Train Loss: 0.60429\n","Epoch: 00 [ 6281/20014 ( 31%)], Train Loss: 0.60402\n","Epoch: 00 [ 6291/20014 ( 31%)], Train Loss: 0.60396\n","Epoch: 00 [ 6301/20014 ( 31%)], Train Loss: 0.60406\n","Epoch: 00 [ 6311/20014 ( 32%)], Train Loss: 0.60400\n","Epoch: 00 [ 6321/20014 ( 32%)], Train Loss: 0.60404\n","Epoch: 00 [ 6331/20014 ( 32%)], Train Loss: 0.60352\n","Epoch: 00 [ 6341/20014 ( 32%)], Train Loss: 0.60299\n","Epoch: 00 [ 6351/20014 ( 32%)], Train Loss: 0.60237\n","Epoch: 00 [ 6361/20014 ( 32%)], Train Loss: 0.60288\n","Epoch: 00 [ 6371/20014 ( 32%)], Train Loss: 0.60339\n","Epoch: 00 [ 6381/20014 ( 32%)], Train Loss: 0.60315\n","Epoch: 00 [ 6391/20014 ( 32%)], Train Loss: 0.60267\n","Epoch: 00 [ 6401/20014 ( 32%)], Train Loss: 0.60270\n","Epoch: 00 [ 6411/20014 ( 32%)], Train Loss: 0.60235\n","Epoch: 00 [ 6421/20014 ( 32%)], Train Loss: 0.60212\n","Epoch: 00 [ 6431/20014 ( 32%)], Train Loss: 0.60246\n","Epoch: 00 [ 6441/20014 ( 32%)], Train Loss: 0.60219\n","Epoch: 00 [ 6451/20014 ( 32%)], Train Loss: 0.60254\n","Epoch: 00 [ 6461/20014 ( 32%)], Train Loss: 0.60184\n","Epoch: 00 [ 6471/20014 ( 32%)], Train Loss: 0.60146\n","Epoch: 00 [ 6481/20014 ( 32%)], Train Loss: 0.60129\n","Epoch: 00 [ 6491/20014 ( 32%)], Train Loss: 0.60118\n","Epoch: 00 [ 6501/20014 ( 32%)], Train Loss: 0.60101\n","Epoch: 00 [ 6511/20014 ( 33%)], Train Loss: 0.60114\n","Epoch: 00 [ 6521/20014 ( 33%)], Train Loss: 0.60144\n","Epoch: 00 [ 6531/20014 ( 33%)], Train Loss: 0.60091\n","Epoch: 00 [ 6541/20014 ( 33%)], Train Loss: 0.60088\n","Epoch: 00 [ 6551/20014 ( 33%)], Train Loss: 0.60022\n","Epoch: 00 [ 6561/20014 ( 33%)], Train Loss: 0.59954\n","Epoch: 00 [ 6571/20014 ( 33%)], Train Loss: 0.59908\n","Epoch: 00 [ 6581/20014 ( 33%)], Train Loss: 0.59914\n","Epoch: 00 [ 6591/20014 ( 33%)], Train Loss: 0.59887\n","Epoch: 00 [ 6601/20014 ( 33%)], Train Loss: 0.59848\n","Epoch: 00 [ 6611/20014 ( 33%)], Train Loss: 0.59921\n","Epoch: 00 [ 6621/20014 ( 33%)], Train Loss: 0.59843\n","Epoch: 00 [ 6631/20014 ( 33%)], Train Loss: 0.59826\n","Epoch: 00 [ 6641/20014 ( 33%)], Train Loss: 0.59760\n","Epoch: 00 [ 6651/20014 ( 33%)], Train Loss: 0.59715\n","Epoch: 00 [ 6661/20014 ( 33%)], Train Loss: 0.59693\n","Epoch: 00 [ 6671/20014 ( 33%)], Train Loss: 0.59632\n","Epoch: 00 [ 6681/20014 ( 33%)], Train Loss: 0.59599\n","Epoch: 00 [ 6691/20014 ( 33%)], Train Loss: 0.59618\n","Epoch: 00 [ 6701/20014 ( 33%)], Train Loss: 0.59563\n","Epoch: 00 [ 6711/20014 ( 34%)], Train Loss: 0.59485\n","Epoch: 00 [ 6721/20014 ( 34%)], Train Loss: 0.59495\n","Epoch: 00 [ 6731/20014 ( 34%)], Train Loss: 0.59417\n","Epoch: 00 [ 6741/20014 ( 34%)], Train Loss: 0.59410\n","Epoch: 00 [ 6751/20014 ( 34%)], Train Loss: 0.59401\n","Epoch: 00 [ 6761/20014 ( 34%)], Train Loss: 0.59357\n","Epoch: 00 [ 6771/20014 ( 34%)], Train Loss: 0.59347\n","Epoch: 00 [ 6781/20014 ( 34%)], Train Loss: 0.59333\n","Epoch: 00 [ 6791/20014 ( 34%)], Train Loss: 0.59278\n","Epoch: 00 [ 6801/20014 ( 34%)], Train Loss: 0.59219\n","Epoch: 00 [ 6811/20014 ( 34%)], Train Loss: 0.59243\n","Epoch: 00 [ 6821/20014 ( 34%)], Train Loss: 0.59202\n","Epoch: 00 [ 6831/20014 ( 34%)], Train Loss: 0.59163\n","Epoch: 00 [ 6841/20014 ( 34%)], Train Loss: 0.59140\n","Epoch: 00 [ 6851/20014 ( 34%)], Train Loss: 0.59175\n","Epoch: 00 [ 6861/20014 ( 34%)], Train Loss: 0.59142\n","Epoch: 00 [ 6871/20014 ( 34%)], Train Loss: 0.59142\n","Epoch: 00 [ 6881/20014 ( 34%)], Train Loss: 0.59156\n","Epoch: 00 [ 6891/20014 ( 34%)], Train Loss: 0.59173\n","Epoch: 00 [ 6901/20014 ( 34%)], Train Loss: 0.59174\n","Epoch: 00 [ 6911/20014 ( 35%)], Train Loss: 0.59159\n","Epoch: 00 [ 6921/20014 ( 35%)], Train Loss: 0.59129\n","Epoch: 00 [ 6931/20014 ( 35%)], Train Loss: 0.59101\n","Epoch: 00 [ 6941/20014 ( 35%)], Train Loss: 0.59031\n","Epoch: 00 [ 6951/20014 ( 35%)], Train Loss: 0.58965\n","Epoch: 00 [ 6961/20014 ( 35%)], Train Loss: 0.58947\n","Epoch: 00 [ 6971/20014 ( 35%)], Train Loss: 0.58878\n","Epoch: 00 [ 6981/20014 ( 35%)], Train Loss: 0.58855\n","Epoch: 00 [ 6991/20014 ( 35%)], Train Loss: 0.58832\n","Epoch: 00 [ 7001/20014 ( 35%)], Train Loss: 0.58821\n","Epoch: 00 [ 7011/20014 ( 35%)], Train Loss: 0.58759\n","Epoch: 00 [ 7021/20014 ( 35%)], Train Loss: 0.58707\n","Epoch: 00 [ 7031/20014 ( 35%)], Train Loss: 0.58652\n","Epoch: 00 [ 7041/20014 ( 35%)], Train Loss: 0.58633\n","Epoch: 00 [ 7051/20014 ( 35%)], Train Loss: 0.58671\n","Epoch: 00 [ 7061/20014 ( 35%)], Train Loss: 0.58604\n","Epoch: 00 [ 7071/20014 ( 35%)], Train Loss: 0.58645\n","Epoch: 00 [ 7081/20014 ( 35%)], Train Loss: 0.58629\n","Epoch: 00 [ 7091/20014 ( 35%)], Train Loss: 0.58617\n","Epoch: 00 [ 7101/20014 ( 35%)], Train Loss: 0.58551\n","Epoch: 00 [ 7111/20014 ( 36%)], Train Loss: 0.58502\n","Epoch: 00 [ 7121/20014 ( 36%)], Train Loss: 0.58469\n","Epoch: 00 [ 7131/20014 ( 36%)], Train Loss: 0.58484\n","Epoch: 00 [ 7141/20014 ( 36%)], Train Loss: 0.58451\n","Epoch: 00 [ 7151/20014 ( 36%)], Train Loss: 0.58441\n","Epoch: 00 [ 7161/20014 ( 36%)], Train Loss: 0.58408\n","Epoch: 00 [ 7171/20014 ( 36%)], Train Loss: 0.58343\n","Epoch: 00 [ 7181/20014 ( 36%)], Train Loss: 0.58389\n","Epoch: 00 [ 7191/20014 ( 36%)], Train Loss: 0.58354\n","Epoch: 00 [ 7201/20014 ( 36%)], Train Loss: 0.58349\n","Epoch: 00 [ 7211/20014 ( 36%)], Train Loss: 0.58292\n","Epoch: 00 [ 7221/20014 ( 36%)], Train Loss: 0.58270\n","Epoch: 00 [ 7231/20014 ( 36%)], Train Loss: 0.58247\n","Epoch: 00 [ 7241/20014 ( 36%)], Train Loss: 0.58254\n","Epoch: 00 [ 7251/20014 ( 36%)], Train Loss: 0.58233\n","Epoch: 00 [ 7261/20014 ( 36%)], Train Loss: 0.58240\n","Epoch: 00 [ 7271/20014 ( 36%)], Train Loss: 0.58250\n","Epoch: 00 [ 7281/20014 ( 36%)], Train Loss: 0.58234\n","Epoch: 00 [ 7291/20014 ( 36%)], Train Loss: 0.58218\n","Epoch: 00 [ 7301/20014 ( 36%)], Train Loss: 0.58219\n","Epoch: 00 [ 7311/20014 ( 37%)], Train Loss: 0.58165\n","Epoch: 00 [ 7321/20014 ( 37%)], Train Loss: 0.58146\n","Epoch: 00 [ 7331/20014 ( 37%)], Train Loss: 0.58122\n","Epoch: 00 [ 7341/20014 ( 37%)], Train Loss: 0.58103\n","Epoch: 00 [ 7351/20014 ( 37%)], Train Loss: 0.58110\n","Epoch: 00 [ 7361/20014 ( 37%)], Train Loss: 0.58129\n","Epoch: 00 [ 7371/20014 ( 37%)], Train Loss: 0.58075\n","Epoch: 00 [ 7381/20014 ( 37%)], Train Loss: 0.58055\n","Epoch: 00 [ 7391/20014 ( 37%)], Train Loss: 0.58036\n","Epoch: 00 [ 7401/20014 ( 37%)], Train Loss: 0.58027\n","Epoch: 00 [ 7411/20014 ( 37%)], Train Loss: 0.57983\n","Epoch: 00 [ 7421/20014 ( 37%)], Train Loss: 0.57977\n","Epoch: 00 [ 7431/20014 ( 37%)], Train Loss: 0.58015\n","Epoch: 00 [ 7441/20014 ( 37%)], Train Loss: 0.57983\n","Epoch: 00 [ 7451/20014 ( 37%)], Train Loss: 0.58004\n","Epoch: 00 [ 7461/20014 ( 37%)], Train Loss: 0.57955\n","Epoch: 00 [ 7471/20014 ( 37%)], Train Loss: 0.57915\n","Epoch: 00 [ 7481/20014 ( 37%)], Train Loss: 0.57943\n","Epoch: 00 [ 7491/20014 ( 37%)], Train Loss: 0.57918\n","Epoch: 00 [ 7501/20014 ( 37%)], Train Loss: 0.57901\n","Epoch: 00 [ 7511/20014 ( 38%)], Train Loss: 0.57872\n","Epoch: 00 [ 7521/20014 ( 38%)], Train Loss: 0.57835\n","Epoch: 00 [ 7531/20014 ( 38%)], Train Loss: 0.57803\n","Epoch: 00 [ 7541/20014 ( 38%)], Train Loss: 0.57826\n","Epoch: 00 [ 7551/20014 ( 38%)], Train Loss: 0.57792\n","Epoch: 00 [ 7561/20014 ( 38%)], Train Loss: 0.57729\n","Epoch: 00 [ 7571/20014 ( 38%)], Train Loss: 0.57715\n","Epoch: 00 [ 7581/20014 ( 38%)], Train Loss: 0.57655\n","Epoch: 00 [ 7591/20014 ( 38%)], Train Loss: 0.57601\n","Epoch: 00 [ 7601/20014 ( 38%)], Train Loss: 0.57584\n","Epoch: 00 [ 7611/20014 ( 38%)], Train Loss: 0.57541\n","Epoch: 00 [ 7621/20014 ( 38%)], Train Loss: 0.57529\n","Epoch: 00 [ 7631/20014 ( 38%)], Train Loss: 0.57551\n","Epoch: 00 [ 7641/20014 ( 38%)], Train Loss: 0.57564\n","Epoch: 00 [ 7651/20014 ( 38%)], Train Loss: 0.57550\n","Epoch: 00 [ 7661/20014 ( 38%)], Train Loss: 0.57570\n","Epoch: 00 [ 7671/20014 ( 38%)], Train Loss: 0.57561\n","Epoch: 00 [ 7681/20014 ( 38%)], Train Loss: 0.57585\n","Epoch: 00 [ 7691/20014 ( 38%)], Train Loss: 0.57560\n","Epoch: 00 [ 7701/20014 ( 38%)], Train Loss: 0.57523\n","Epoch: 00 [ 7711/20014 ( 39%)], Train Loss: 0.57496\n","Epoch: 00 [ 7721/20014 ( 39%)], Train Loss: 0.57481\n","Epoch: 00 [ 7731/20014 ( 39%)], Train Loss: 0.57439\n","Epoch: 00 [ 7741/20014 ( 39%)], Train Loss: 0.57399\n","Epoch: 00 [ 7751/20014 ( 39%)], Train Loss: 0.57391\n","Epoch: 00 [ 7761/20014 ( 39%)], Train Loss: 0.57386\n","Epoch: 00 [ 7771/20014 ( 39%)], Train Loss: 0.57325\n","Epoch: 00 [ 7781/20014 ( 39%)], Train Loss: 0.57291\n","Epoch: 00 [ 7791/20014 ( 39%)], Train Loss: 0.57280\n","Epoch: 00 [ 7801/20014 ( 39%)], Train Loss: 0.57270\n","Epoch: 00 [ 7811/20014 ( 39%)], Train Loss: 0.57250\n","Epoch: 00 [ 7821/20014 ( 39%)], Train Loss: 0.57269\n","Epoch: 00 [ 7831/20014 ( 39%)], Train Loss: 0.57230\n","Epoch: 00 [ 7841/20014 ( 39%)], Train Loss: 0.57212\n","Epoch: 00 [ 7851/20014 ( 39%)], Train Loss: 0.57167\n","Epoch: 00 [ 7861/20014 ( 39%)], Train Loss: 0.57150\n","Epoch: 00 [ 7871/20014 ( 39%)], Train Loss: 0.57189\n","Epoch: 00 [ 7881/20014 ( 39%)], Train Loss: 0.57171\n","Epoch: 00 [ 7891/20014 ( 39%)], Train Loss: 0.57138\n","Epoch: 00 [ 7901/20014 ( 39%)], Train Loss: 0.57128\n","Epoch: 00 [ 7911/20014 ( 40%)], Train Loss: 0.57174\n","Epoch: 00 [ 7921/20014 ( 40%)], Train Loss: 0.57119\n","Epoch: 00 [ 7931/20014 ( 40%)], Train Loss: 0.57135\n","Epoch: 00 [ 7941/20014 ( 40%)], Train Loss: 0.57119\n","Epoch: 00 [ 7951/20014 ( 40%)], Train Loss: 0.57072\n","Epoch: 00 [ 7961/20014 ( 40%)], Train Loss: 0.57045\n","Epoch: 00 [ 7971/20014 ( 40%)], Train Loss: 0.56997\n","Epoch: 00 [ 7981/20014 ( 40%)], Train Loss: 0.56983\n","Epoch: 00 [ 7991/20014 ( 40%)], Train Loss: 0.56983\n","Epoch: 00 [ 8001/20014 ( 40%)], Train Loss: 0.56986\n","Epoch: 00 [ 8011/20014 ( 40%)], Train Loss: 0.56967\n","Epoch: 00 [ 8021/20014 ( 40%)], Train Loss: 0.56980\n","Epoch: 00 [ 8031/20014 ( 40%)], Train Loss: 0.57003\n","Epoch: 00 [ 8041/20014 ( 40%)], Train Loss: 0.56971\n","Epoch: 00 [ 8051/20014 ( 40%)], Train Loss: 0.56932\n","Epoch: 00 [ 8061/20014 ( 40%)], Train Loss: 0.56904\n","Epoch: 00 [ 8071/20014 ( 40%)], Train Loss: 0.56897\n","Epoch: 00 [ 8081/20014 ( 40%)], Train Loss: 0.56916\n","Epoch: 00 [ 8091/20014 ( 40%)], Train Loss: 0.56893\n","Epoch: 00 [ 8101/20014 ( 40%)], Train Loss: 0.56911\n","Epoch: 00 [ 8111/20014 ( 41%)], Train Loss: 0.56881\n","Epoch: 00 [ 8121/20014 ( 41%)], Train Loss: 0.56863\n","Epoch: 00 [ 8131/20014 ( 41%)], Train Loss: 0.56831\n","Epoch: 00 [ 8141/20014 ( 41%)], Train Loss: 0.56774\n","Epoch: 00 [ 8151/20014 ( 41%)], Train Loss: 0.56716\n","Epoch: 00 [ 8161/20014 ( 41%)], Train Loss: 0.56733\n","Epoch: 00 [ 8171/20014 ( 41%)], Train Loss: 0.56696\n","Epoch: 00 [ 8181/20014 ( 41%)], Train Loss: 0.56673\n","Epoch: 00 [ 8191/20014 ( 41%)], Train Loss: 0.56699\n","Epoch: 00 [ 8201/20014 ( 41%)], Train Loss: 0.56642\n","Epoch: 00 [ 8211/20014 ( 41%)], Train Loss: 0.56629\n","Epoch: 00 [ 8221/20014 ( 41%)], Train Loss: 0.56609\n","Epoch: 00 [ 8231/20014 ( 41%)], Train Loss: 0.56619\n","Epoch: 00 [ 8241/20014 ( 41%)], Train Loss: 0.56615\n","Epoch: 00 [ 8251/20014 ( 41%)], Train Loss: 0.56566\n","Epoch: 00 [ 8261/20014 ( 41%)], Train Loss: 0.56518\n","Epoch: 00 [ 8271/20014 ( 41%)], Train Loss: 0.56496\n","Epoch: 00 [ 8281/20014 ( 41%)], Train Loss: 0.56475\n","Epoch: 00 [ 8291/20014 ( 41%)], Train Loss: 0.56469\n","Epoch: 00 [ 8301/20014 ( 41%)], Train Loss: 0.56436\n","Epoch: 00 [ 8311/20014 ( 42%)], Train Loss: 0.56386\n","Epoch: 00 [ 8321/20014 ( 42%)], Train Loss: 0.56396\n","Epoch: 00 [ 8331/20014 ( 42%)], Train Loss: 0.56370\n","Epoch: 00 [ 8341/20014 ( 42%)], Train Loss: 0.56363\n","Epoch: 00 [ 8351/20014 ( 42%)], Train Loss: 0.56354\n","Epoch: 00 [ 8361/20014 ( 42%)], Train Loss: 0.56322\n","Epoch: 00 [ 8371/20014 ( 42%)], Train Loss: 0.56345\n","Epoch: 00 [ 8381/20014 ( 42%)], Train Loss: 0.56346\n","Epoch: 00 [ 8391/20014 ( 42%)], Train Loss: 0.56311\n","Epoch: 00 [ 8401/20014 ( 42%)], Train Loss: 0.56290\n","Epoch: 00 [ 8411/20014 ( 42%)], Train Loss: 0.56295\n","Epoch: 00 [ 8421/20014 ( 42%)], Train Loss: 0.56259\n","Epoch: 00 [ 8431/20014 ( 42%)], Train Loss: 0.56239\n","Epoch: 00 [ 8441/20014 ( 42%)], Train Loss: 0.56194\n","Epoch: 00 [ 8451/20014 ( 42%)], Train Loss: 0.56130\n","Epoch: 00 [ 8461/20014 ( 42%)], Train Loss: 0.56094\n","Epoch: 00 [ 8471/20014 ( 42%)], Train Loss: 0.56113\n","Epoch: 00 [ 8481/20014 ( 42%)], Train Loss: 0.56100\n","Epoch: 00 [ 8491/20014 ( 42%)], Train Loss: 0.56076\n","Epoch: 00 [ 8501/20014 ( 42%)], Train Loss: 0.56051\n","Epoch: 00 [ 8511/20014 ( 43%)], Train Loss: 0.56054\n","Epoch: 00 [ 8521/20014 ( 43%)], Train Loss: 0.56017\n","Epoch: 00 [ 8531/20014 ( 43%)], Train Loss: 0.56010\n","Epoch: 00 [ 8541/20014 ( 43%)], Train Loss: 0.56046\n","Epoch: 00 [ 8551/20014 ( 43%)], Train Loss: 0.56079\n","Epoch: 00 [ 8561/20014 ( 43%)], Train Loss: 0.56054\n","Epoch: 00 [ 8571/20014 ( 43%)], Train Loss: 0.56043\n","Epoch: 00 [ 8581/20014 ( 43%)], Train Loss: 0.56048\n","Epoch: 00 [ 8591/20014 ( 43%)], Train Loss: 0.56026\n","Epoch: 00 [ 8601/20014 ( 43%)], Train Loss: 0.56005\n","Epoch: 00 [ 8611/20014 ( 43%)], Train Loss: 0.55978\n","Epoch: 00 [ 8621/20014 ( 43%)], Train Loss: 0.56003\n","Epoch: 00 [ 8631/20014 ( 43%)], Train Loss: 0.55972\n","Epoch: 00 [ 8641/20014 ( 43%)], Train Loss: 0.55954\n","Epoch: 00 [ 8651/20014 ( 43%)], Train Loss: 0.55895\n","Epoch: 00 [ 8661/20014 ( 43%)], Train Loss: 0.55884\n","Epoch: 00 [ 8671/20014 ( 43%)], Train Loss: 0.55862\n","Epoch: 00 [ 8681/20014 ( 43%)], Train Loss: 0.55813\n","Epoch: 00 [ 8691/20014 ( 43%)], Train Loss: 0.55803\n","Epoch: 00 [ 8701/20014 ( 43%)], Train Loss: 0.55819\n","Epoch: 00 [ 8711/20014 ( 44%)], Train Loss: 0.55799\n","Epoch: 00 [ 8721/20014 ( 44%)], Train Loss: 0.55789\n","Epoch: 00 [ 8731/20014 ( 44%)], Train Loss: 0.55845\n","Epoch: 00 [ 8741/20014 ( 44%)], Train Loss: 0.55802\n","Epoch: 00 [ 8751/20014 ( 44%)], Train Loss: 0.55785\n","Epoch: 00 [ 8761/20014 ( 44%)], Train Loss: 0.55787\n","Epoch: 00 [ 8771/20014 ( 44%)], Train Loss: 0.55778\n","Epoch: 00 [ 8781/20014 ( 44%)], Train Loss: 0.55768\n","Epoch: 00 [ 8791/20014 ( 44%)], Train Loss: 0.55828\n","Epoch: 00 [ 8801/20014 ( 44%)], Train Loss: 0.55808\n","Epoch: 00 [ 8811/20014 ( 44%)], Train Loss: 0.55780\n","Epoch: 00 [ 8821/20014 ( 44%)], Train Loss: 0.55729\n","Epoch: 00 [ 8831/20014 ( 44%)], Train Loss: 0.55681\n","Epoch: 00 [ 8841/20014 ( 44%)], Train Loss: 0.55653\n","Epoch: 00 [ 8851/20014 ( 44%)], Train Loss: 0.55664\n","Epoch: 00 [ 8861/20014 ( 44%)], Train Loss: 0.55625\n","Epoch: 00 [ 8871/20014 ( 44%)], Train Loss: 0.55590\n","Epoch: 00 [ 8881/20014 ( 44%)], Train Loss: 0.55605\n","Epoch: 00 [ 8891/20014 ( 44%)], Train Loss: 0.55601\n","Epoch: 00 [ 8901/20014 ( 44%)], Train Loss: 0.55588\n","Epoch: 00 [ 8911/20014 ( 45%)], Train Loss: 0.55598\n","Epoch: 00 [ 8921/20014 ( 45%)], Train Loss: 0.55558\n","Epoch: 00 [ 8931/20014 ( 45%)], Train Loss: 0.55522\n","Epoch: 00 [ 8941/20014 ( 45%)], Train Loss: 0.55478\n","Epoch: 00 [ 8951/20014 ( 45%)], Train Loss: 0.55438\n","Epoch: 00 [ 8961/20014 ( 45%)], Train Loss: 0.55421\n","Epoch: 00 [ 8971/20014 ( 45%)], Train Loss: 0.55419\n","Epoch: 00 [ 8981/20014 ( 45%)], Train Loss: 0.55395\n","Epoch: 00 [ 8991/20014 ( 45%)], Train Loss: 0.55382\n","Epoch: 00 [ 9001/20014 ( 45%)], Train Loss: 0.55380\n","Epoch: 00 [ 9011/20014 ( 45%)], Train Loss: 0.55366\n","Epoch: 00 [ 9021/20014 ( 45%)], Train Loss: 0.55354\n","Epoch: 00 [ 9031/20014 ( 45%)], Train Loss: 0.55311\n","Epoch: 00 [ 9041/20014 ( 45%)], Train Loss: 0.55280\n","Epoch: 00 [ 9051/20014 ( 45%)], Train Loss: 0.55281\n","Epoch: 00 [ 9061/20014 ( 45%)], Train Loss: 0.55272\n","Epoch: 00 [ 9071/20014 ( 45%)], Train Loss: 0.55216\n","Epoch: 00 [ 9081/20014 ( 45%)], Train Loss: 0.55206\n","Epoch: 00 [ 9091/20014 ( 45%)], Train Loss: 0.55161\n","Epoch: 00 [ 9101/20014 ( 45%)], Train Loss: 0.55125\n","Epoch: 00 [ 9111/20014 ( 46%)], Train Loss: 0.55130\n","Epoch: 00 [ 9121/20014 ( 46%)], Train Loss: 0.55120\n","Epoch: 00 [ 9131/20014 ( 46%)], Train Loss: 0.55068\n","Epoch: 00 [ 9141/20014 ( 46%)], Train Loss: 0.55102\n","Epoch: 00 [ 9151/20014 ( 46%)], Train Loss: 0.55099\n","Epoch: 00 [ 9161/20014 ( 46%)], Train Loss: 0.55045\n","Epoch: 00 [ 9171/20014 ( 46%)], Train Loss: 0.55033\n","Epoch: 00 [ 9181/20014 ( 46%)], Train Loss: 0.55004\n","Epoch: 00 [ 9191/20014 ( 46%)], Train Loss: 0.54990\n","Epoch: 00 [ 9201/20014 ( 46%)], Train Loss: 0.54987\n","Epoch: 00 [ 9211/20014 ( 46%)], Train Loss: 0.54928\n","Epoch: 00 [ 9221/20014 ( 46%)], Train Loss: 0.54963\n","Epoch: 00 [ 9231/20014 ( 46%)], Train Loss: 0.54950\n","Epoch: 00 [ 9241/20014 ( 46%)], Train Loss: 0.54918\n","Epoch: 00 [ 9251/20014 ( 46%)], Train Loss: 0.54869\n","Epoch: 00 [ 9261/20014 ( 46%)], Train Loss: 0.54837\n","Epoch: 00 [ 9271/20014 ( 46%)], Train Loss: 0.54788\n","Epoch: 00 [ 9281/20014 ( 46%)], Train Loss: 0.54755\n","Epoch: 00 [ 9291/20014 ( 46%)], Train Loss: 0.54753\n","Epoch: 00 [ 9301/20014 ( 46%)], Train Loss: 0.54766\n","Epoch: 00 [ 9311/20014 ( 47%)], Train Loss: 0.54785\n","Epoch: 00 [ 9321/20014 ( 47%)], Train Loss: 0.54777\n","Epoch: 00 [ 9331/20014 ( 47%)], Train Loss: 0.54735\n","Epoch: 00 [ 9341/20014 ( 47%)], Train Loss: 0.54723\n","Epoch: 00 [ 9351/20014 ( 47%)], Train Loss: 0.54708\n","Epoch: 00 [ 9361/20014 ( 47%)], Train Loss: 0.54712\n","Epoch: 00 [ 9371/20014 ( 47%)], Train Loss: 0.54693\n","Epoch: 00 [ 9381/20014 ( 47%)], Train Loss: 0.54687\n","Epoch: 00 [ 9391/20014 ( 47%)], Train Loss: 0.54718\n","Epoch: 00 [ 9401/20014 ( 47%)], Train Loss: 0.54717\n","Epoch: 00 [ 9411/20014 ( 47%)], Train Loss: 0.54695\n","Epoch: 00 [ 9421/20014 ( 47%)], Train Loss: 0.54667\n","Epoch: 00 [ 9431/20014 ( 47%)], Train Loss: 0.54668\n","Epoch: 00 [ 9441/20014 ( 47%)], Train Loss: 0.54644\n","Epoch: 00 [ 9451/20014 ( 47%)], Train Loss: 0.54607\n","Epoch: 00 [ 9461/20014 ( 47%)], Train Loss: 0.54601\n","Epoch: 00 [ 9471/20014 ( 47%)], Train Loss: 0.54611\n","Epoch: 00 [ 9481/20014 ( 47%)], Train Loss: 0.54602\n","Epoch: 00 [ 9491/20014 ( 47%)], Train Loss: 0.54555\n","Epoch: 00 [ 9501/20014 ( 47%)], Train Loss: 0.54558\n","Epoch: 00 [ 9511/20014 ( 48%)], Train Loss: 0.54554\n","Epoch: 00 [ 9521/20014 ( 48%)], Train Loss: 0.54534\n","Epoch: 00 [ 9531/20014 ( 48%)], Train Loss: 0.54533\n","Epoch: 00 [ 9541/20014 ( 48%)], Train Loss: 0.54511\n","Epoch: 00 [ 9551/20014 ( 48%)], Train Loss: 0.54481\n","Epoch: 00 [ 9561/20014 ( 48%)], Train Loss: 0.54496\n","Epoch: 00 [ 9571/20014 ( 48%)], Train Loss: 0.54526\n","Epoch: 00 [ 9581/20014 ( 48%)], Train Loss: 0.54531\n","Epoch: 00 [ 9591/20014 ( 48%)], Train Loss: 0.54500\n","Epoch: 00 [ 9601/20014 ( 48%)], Train Loss: 0.54448\n","Epoch: 00 [ 9611/20014 ( 48%)], Train Loss: 0.54430\n","Epoch: 00 [ 9621/20014 ( 48%)], Train Loss: 0.54423\n","Epoch: 00 [ 9631/20014 ( 48%)], Train Loss: 0.54431\n","Epoch: 00 [ 9641/20014 ( 48%)], Train Loss: 0.54412\n","Epoch: 00 [ 9651/20014 ( 48%)], Train Loss: 0.54394\n","Epoch: 00 [ 9661/20014 ( 48%)], Train Loss: 0.54435\n","Epoch: 00 [ 9671/20014 ( 48%)], Train Loss: 0.54417\n","Epoch: 00 [ 9681/20014 ( 48%)], Train Loss: 0.54437\n","Epoch: 00 [ 9691/20014 ( 48%)], Train Loss: 0.54413\n","Epoch: 00 [ 9701/20014 ( 48%)], Train Loss: 0.54395\n","Epoch: 00 [ 9711/20014 ( 49%)], Train Loss: 0.54389\n","Epoch: 00 [ 9721/20014 ( 49%)], Train Loss: 0.54343\n","Epoch: 00 [ 9731/20014 ( 49%)], Train Loss: 0.54303\n","Epoch: 00 [ 9741/20014 ( 49%)], Train Loss: 0.54320\n","Epoch: 00 [ 9751/20014 ( 49%)], Train Loss: 0.54308\n","Epoch: 00 [ 9761/20014 ( 49%)], Train Loss: 0.54335\n","Epoch: 00 [ 9771/20014 ( 49%)], Train Loss: 0.54302\n","Epoch: 00 [ 9781/20014 ( 49%)], Train Loss: 0.54289\n","Epoch: 00 [ 9791/20014 ( 49%)], Train Loss: 0.54305\n","Epoch: 00 [ 9801/20014 ( 49%)], Train Loss: 0.54290\n","Epoch: 00 [ 9811/20014 ( 49%)], Train Loss: 0.54354\n","Epoch: 00 [ 9821/20014 ( 49%)], Train Loss: 0.54328\n","Epoch: 00 [ 9831/20014 ( 49%)], Train Loss: 0.54307\n","Epoch: 00 [ 9841/20014 ( 49%)], Train Loss: 0.54312\n","Epoch: 00 [ 9851/20014 ( 49%)], Train Loss: 0.54321\n","Epoch: 00 [ 9861/20014 ( 49%)], Train Loss: 0.54348\n","Epoch: 00 [ 9871/20014 ( 49%)], Train Loss: 0.54314\n","Epoch: 00 [ 9881/20014 ( 49%)], Train Loss: 0.54324\n","Epoch: 00 [ 9891/20014 ( 49%)], Train Loss: 0.54359\n","Epoch: 00 [ 9901/20014 ( 49%)], Train Loss: 0.54330\n","Epoch: 00 [ 9911/20014 ( 50%)], Train Loss: 0.54301\n","Epoch: 00 [ 9921/20014 ( 50%)], Train Loss: 0.54292\n","Epoch: 00 [ 9931/20014 ( 50%)], Train Loss: 0.54271\n","Epoch: 00 [ 9941/20014 ( 50%)], Train Loss: 0.54229\n","Epoch: 00 [ 9951/20014 ( 50%)], Train Loss: 0.54188\n","Epoch: 00 [ 9961/20014 ( 50%)], Train Loss: 0.54178\n","Epoch: 00 [ 9971/20014 ( 50%)], Train Loss: 0.54261\n","Epoch: 00 [ 9981/20014 ( 50%)], Train Loss: 0.54268\n","Epoch: 00 [ 9991/20014 ( 50%)], Train Loss: 0.54248\n","Epoch: 00 [10001/20014 ( 50%)], Train Loss: 0.54257\n","Epoch: 00 [10011/20014 ( 50%)], Train Loss: 0.54298\n","Epoch: 00 [10021/20014 ( 50%)], Train Loss: 0.54285\n","Epoch: 00 [10031/20014 ( 50%)], Train Loss: 0.54289\n","Epoch: 00 [10041/20014 ( 50%)], Train Loss: 0.54264\n","Epoch: 00 [10051/20014 ( 50%)], Train Loss: 0.54232\n","Epoch: 00 [10061/20014 ( 50%)], Train Loss: 0.54261\n","Epoch: 00 [10071/20014 ( 50%)], Train Loss: 0.54255\n","Epoch: 00 [10081/20014 ( 50%)], Train Loss: 0.54267\n","Epoch: 00 [10091/20014 ( 50%)], Train Loss: 0.54231\n","Epoch: 00 [10101/20014 ( 50%)], Train Loss: 0.54206\n","Epoch: 00 [10111/20014 ( 51%)], Train Loss: 0.54184\n","Epoch: 00 [10121/20014 ( 51%)], Train Loss: 0.54157\n","Epoch: 00 [10131/20014 ( 51%)], Train Loss: 0.54112\n","Epoch: 00 [10141/20014 ( 51%)], Train Loss: 0.54129\n","Epoch: 00 [10151/20014 ( 51%)], Train Loss: 0.54101\n","Epoch: 00 [10161/20014 ( 51%)], Train Loss: 0.54066\n","Epoch: 00 [10171/20014 ( 51%)], Train Loss: 0.54051\n","Epoch: 00 [10181/20014 ( 51%)], Train Loss: 0.54033\n","Epoch: 00 [10191/20014 ( 51%)], Train Loss: 0.54043\n","Epoch: 00 [10201/20014 ( 51%)], Train Loss: 0.54093\n","Epoch: 00 [10211/20014 ( 51%)], Train Loss: 0.54070\n","Epoch: 00 [10221/20014 ( 51%)], Train Loss: 0.54052\n","Epoch: 00 [10231/20014 ( 51%)], Train Loss: 0.54043\n","Epoch: 00 [10241/20014 ( 51%)], Train Loss: 0.54008\n","Epoch: 00 [10251/20014 ( 51%)], Train Loss: 0.53984\n","Epoch: 00 [10261/20014 ( 51%)], Train Loss: 0.53985\n","Epoch: 00 [10271/20014 ( 51%)], Train Loss: 0.53943\n","Epoch: 00 [10281/20014 ( 51%)], Train Loss: 0.53896\n","Epoch: 00 [10291/20014 ( 51%)], Train Loss: 0.53882\n","Epoch: 00 [10301/20014 ( 51%)], Train Loss: 0.53872\n","Epoch: 00 [10311/20014 ( 52%)], Train Loss: 0.53823\n","Epoch: 00 [10321/20014 ( 52%)], Train Loss: 0.53805\n","Epoch: 00 [10331/20014 ( 52%)], Train Loss: 0.53786\n","Epoch: 00 [10341/20014 ( 52%)], Train Loss: 0.53791\n","Epoch: 00 [10351/20014 ( 52%)], Train Loss: 0.53763\n","Epoch: 00 [10361/20014 ( 52%)], Train Loss: 0.53750\n","Epoch: 00 [10371/20014 ( 52%)], Train Loss: 0.53751\n","Epoch: 00 [10381/20014 ( 52%)], Train Loss: 0.53725\n","Epoch: 00 [10391/20014 ( 52%)], Train Loss: 0.53698\n","Epoch: 00 [10401/20014 ( 52%)], Train Loss: 0.53658\n","Epoch: 00 [10411/20014 ( 52%)], Train Loss: 0.53676\n","Epoch: 00 [10421/20014 ( 52%)], Train Loss: 0.53637\n","Epoch: 00 [10431/20014 ( 52%)], Train Loss: 0.53598\n","Epoch: 00 [10441/20014 ( 52%)], Train Loss: 0.53643\n","Epoch: 00 [10451/20014 ( 52%)], Train Loss: 0.53653\n","Epoch: 00 [10461/20014 ( 52%)], Train Loss: 0.53706\n","Epoch: 00 [10471/20014 ( 52%)], Train Loss: 0.53671\n","Epoch: 00 [10481/20014 ( 52%)], Train Loss: 0.53635\n","Epoch: 00 [10491/20014 ( 52%)], Train Loss: 0.53616\n","Epoch: 00 [10501/20014 ( 52%)], Train Loss: 0.53607\n","Epoch: 00 [10511/20014 ( 53%)], Train Loss: 0.53590\n","Epoch: 00 [10521/20014 ( 53%)], Train Loss: 0.53560\n","Epoch: 00 [10531/20014 ( 53%)], Train Loss: 0.53572\n","Epoch: 00 [10541/20014 ( 53%)], Train Loss: 0.53534\n","Epoch: 00 [10551/20014 ( 53%)], Train Loss: 0.53515\n","Epoch: 00 [10561/20014 ( 53%)], Train Loss: 0.53499\n","Epoch: 00 [10571/20014 ( 53%)], Train Loss: 0.53480\n","Epoch: 00 [10581/20014 ( 53%)], Train Loss: 0.53480\n","Epoch: 00 [10591/20014 ( 53%)], Train Loss: 0.53473\n","Epoch: 00 [10601/20014 ( 53%)], Train Loss: 0.53479\n","Epoch: 00 [10611/20014 ( 53%)], Train Loss: 0.53441\n","Epoch: 00 [10621/20014 ( 53%)], Train Loss: 0.53411\n","Epoch: 00 [10631/20014 ( 53%)], Train Loss: 0.53417\n","Epoch: 00 [10641/20014 ( 53%)], Train Loss: 0.53381\n","Epoch: 00 [10651/20014 ( 53%)], Train Loss: 0.53348\n","Epoch: 00 [10661/20014 ( 53%)], Train Loss: 0.53365\n","Epoch: 00 [10671/20014 ( 53%)], Train Loss: 0.53356\n","Epoch: 00 [10681/20014 ( 53%)], Train Loss: 0.53349\n","Epoch: 00 [10691/20014 ( 53%)], Train Loss: 0.53387\n","Epoch: 00 [10701/20014 ( 53%)], Train Loss: 0.53364\n","Epoch: 00 [10711/20014 ( 54%)], Train Loss: 0.53393\n","Epoch: 00 [10721/20014 ( 54%)], Train Loss: 0.53387\n","Epoch: 00 [10731/20014 ( 54%)], Train Loss: 0.53364\n","Epoch: 00 [10741/20014 ( 54%)], Train Loss: 0.53335\n","Epoch: 00 [10751/20014 ( 54%)], Train Loss: 0.53304\n","Epoch: 00 [10761/20014 ( 54%)], Train Loss: 0.53300\n","Epoch: 00 [10771/20014 ( 54%)], Train Loss: 0.53325\n","Epoch: 00 [10781/20014 ( 54%)], Train Loss: 0.53306\n","Epoch: 00 [10791/20014 ( 54%)], Train Loss: 0.53276\n","Epoch: 00 [10801/20014 ( 54%)], Train Loss: 0.53257\n","Epoch: 00 [10811/20014 ( 54%)], Train Loss: 0.53268\n","Epoch: 00 [10821/20014 ( 54%)], Train Loss: 0.53263\n","Epoch: 00 [10831/20014 ( 54%)], Train Loss: 0.53237\n","Epoch: 00 [10841/20014 ( 54%)], Train Loss: 0.53228\n","Epoch: 00 [10851/20014 ( 54%)], Train Loss: 0.53233\n","Epoch: 00 [10861/20014 ( 54%)], Train Loss: 0.53204\n","Epoch: 00 [10871/20014 ( 54%)], Train Loss: 0.53177\n","Epoch: 00 [10881/20014 ( 54%)], Train Loss: 0.53188\n","Epoch: 00 [10891/20014 ( 54%)], Train Loss: 0.53166\n","Epoch: 00 [10901/20014 ( 54%)], Train Loss: 0.53130\n","Epoch: 00 [10911/20014 ( 55%)], Train Loss: 0.53121\n","Epoch: 00 [10921/20014 ( 55%)], Train Loss: 0.53120\n","Epoch: 00 [10931/20014 ( 55%)], Train Loss: 0.53112\n","Epoch: 00 [10941/20014 ( 55%)], Train Loss: 0.53117\n","Epoch: 00 [10951/20014 ( 55%)], Train Loss: 0.53092\n","Epoch: 00 [10961/20014 ( 55%)], Train Loss: 0.53054\n","Epoch: 00 [10971/20014 ( 55%)], Train Loss: 0.53046\n","Epoch: 00 [10981/20014 ( 55%)], Train Loss: 0.53053\n","Epoch: 00 [10991/20014 ( 55%)], Train Loss: 0.53055\n","Epoch: 00 [11001/20014 ( 55%)], Train Loss: 0.53051\n","Epoch: 00 [11011/20014 ( 55%)], Train Loss: 0.53042\n","Epoch: 00 [11021/20014 ( 55%)], Train Loss: 0.53034\n","Epoch: 00 [11031/20014 ( 55%)], Train Loss: 0.53056\n","Epoch: 00 [11041/20014 ( 55%)], Train Loss: 0.53047\n","Epoch: 00 [11051/20014 ( 55%)], Train Loss: 0.53010\n","Epoch: 00 [11061/20014 ( 55%)], Train Loss: 0.52978\n","Epoch: 00 [11071/20014 ( 55%)], Train Loss: 0.52945\n","Epoch: 00 [11081/20014 ( 55%)], Train Loss: 0.52934\n","Epoch: 00 [11091/20014 ( 55%)], Train Loss: 0.52934\n","Epoch: 00 [11101/20014 ( 55%)], Train Loss: 0.52921\n","Epoch: 00 [11111/20014 ( 56%)], Train Loss: 0.52891\n","Epoch: 00 [11121/20014 ( 56%)], Train Loss: 0.52916\n","Epoch: 00 [11131/20014 ( 56%)], Train Loss: 0.52886\n","Epoch: 00 [11141/20014 ( 56%)], Train Loss: 0.52877\n","Epoch: 00 [11151/20014 ( 56%)], Train Loss: 0.52864\n","Epoch: 00 [11161/20014 ( 56%)], Train Loss: 0.52864\n","Epoch: 00 [11171/20014 ( 56%)], Train Loss: 0.52840\n","Epoch: 00 [11181/20014 ( 56%)], Train Loss: 0.52818\n","Epoch: 00 [11191/20014 ( 56%)], Train Loss: 0.52868\n","Epoch: 00 [11201/20014 ( 56%)], Train Loss: 0.52877\n","Epoch: 00 [11211/20014 ( 56%)], Train Loss: 0.52836\n","Epoch: 00 [11221/20014 ( 56%)], Train Loss: 0.52835\n","Epoch: 00 [11231/20014 ( 56%)], Train Loss: 0.52792\n","Epoch: 00 [11241/20014 ( 56%)], Train Loss: 0.52779\n","Epoch: 00 [11251/20014 ( 56%)], Train Loss: 0.52759\n","Epoch: 00 [11261/20014 ( 56%)], Train Loss: 0.52750\n","Epoch: 00 [11271/20014 ( 56%)], Train Loss: 0.52723\n","Epoch: 00 [11281/20014 ( 56%)], Train Loss: 0.52764\n","Epoch: 00 [11291/20014 ( 56%)], Train Loss: 0.52739\n","Epoch: 00 [11301/20014 ( 56%)], Train Loss: 0.52745\n","Epoch: 00 [11311/20014 ( 57%)], Train Loss: 0.52733\n","Epoch: 00 [11321/20014 ( 57%)], Train Loss: 0.52696\n","Epoch: 00 [11331/20014 ( 57%)], Train Loss: 0.52695\n","Epoch: 00 [11341/20014 ( 57%)], Train Loss: 0.52669\n","Epoch: 00 [11351/20014 ( 57%)], Train Loss: 0.52660\n","Epoch: 00 [11361/20014 ( 57%)], Train Loss: 0.52686\n","Epoch: 00 [11371/20014 ( 57%)], Train Loss: 0.52672\n","Epoch: 00 [11381/20014 ( 57%)], Train Loss: 0.52662\n","Epoch: 00 [11391/20014 ( 57%)], Train Loss: 0.52631\n","Epoch: 00 [11401/20014 ( 57%)], Train Loss: 0.52603\n","Epoch: 00 [11411/20014 ( 57%)], Train Loss: 0.52581\n","Epoch: 00 [11421/20014 ( 57%)], Train Loss: 0.52553\n","Epoch: 00 [11431/20014 ( 57%)], Train Loss: 0.52553\n","Epoch: 00 [11441/20014 ( 57%)], Train Loss: 0.52537\n","Epoch: 00 [11451/20014 ( 57%)], Train Loss: 0.52517\n","Epoch: 00 [11461/20014 ( 57%)], Train Loss: 0.52507\n","Epoch: 00 [11471/20014 ( 57%)], Train Loss: 0.52497\n","Epoch: 00 [11481/20014 ( 57%)], Train Loss: 0.52496\n","Epoch: 00 [11491/20014 ( 57%)], Train Loss: 0.52487\n","Epoch: 00 [11501/20014 ( 57%)], Train Loss: 0.52454\n","Epoch: 00 [11511/20014 ( 58%)], Train Loss: 0.52445\n","Epoch: 00 [11521/20014 ( 58%)], Train Loss: 0.52428\n","Epoch: 00 [11531/20014 ( 58%)], Train Loss: 0.52414\n","Epoch: 00 [11541/20014 ( 58%)], Train Loss: 0.52424\n","Epoch: 00 [11551/20014 ( 58%)], Train Loss: 0.52419\n","Epoch: 00 [11561/20014 ( 58%)], Train Loss: 0.52429\n","Epoch: 00 [11571/20014 ( 58%)], Train Loss: 0.52410\n","Epoch: 00 [11581/20014 ( 58%)], Train Loss: 0.52414\n","Epoch: 00 [11591/20014 ( 58%)], Train Loss: 0.52408\n","Epoch: 00 [11601/20014 ( 58%)], Train Loss: 0.52400\n","Epoch: 00 [11611/20014 ( 58%)], Train Loss: 0.52408\n","Epoch: 00 [11621/20014 ( 58%)], Train Loss: 0.52374\n","Epoch: 00 [11631/20014 ( 58%)], Train Loss: 0.52337\n","Epoch: 00 [11641/20014 ( 58%)], Train Loss: 0.52310\n","Epoch: 00 [11651/20014 ( 58%)], Train Loss: 0.52285\n","Epoch: 00 [11661/20014 ( 58%)], Train Loss: 0.52255\n","Epoch: 00 [11671/20014 ( 58%)], Train Loss: 0.52237\n","Epoch: 00 [11681/20014 ( 58%)], Train Loss: 0.52226\n","Epoch: 00 [11691/20014 ( 58%)], Train Loss: 0.52225\n","Epoch: 00 [11701/20014 ( 58%)], Train Loss: 0.52213\n","Epoch: 00 [11711/20014 ( 59%)], Train Loss: 0.52206\n","Epoch: 00 [11721/20014 ( 59%)], Train Loss: 0.52186\n","Epoch: 00 [11731/20014 ( 59%)], Train Loss: 0.52164\n","Epoch: 00 [11741/20014 ( 59%)], Train Loss: 0.52164\n","Epoch: 00 [11751/20014 ( 59%)], Train Loss: 0.52145\n","Epoch: 00 [11761/20014 ( 59%)], Train Loss: 0.52113\n","Epoch: 00 [11771/20014 ( 59%)], Train Loss: 0.52097\n","Epoch: 00 [11781/20014 ( 59%)], Train Loss: 0.52107\n","Epoch: 00 [11791/20014 ( 59%)], Train Loss: 0.52080\n","Epoch: 00 [11801/20014 ( 59%)], Train Loss: 0.52077\n","Epoch: 00 [11811/20014 ( 59%)], Train Loss: 0.52087\n","Epoch: 00 [11821/20014 ( 59%)], Train Loss: 0.52098\n","Epoch: 00 [11831/20014 ( 59%)], Train Loss: 0.52091\n","Epoch: 00 [11841/20014 ( 59%)], Train Loss: 0.52077\n","Epoch: 00 [11851/20014 ( 59%)], Train Loss: 0.52107\n","Epoch: 00 [11861/20014 ( 59%)], Train Loss: 0.52091\n","Epoch: 00 [11871/20014 ( 59%)], Train Loss: 0.52096\n","Epoch: 00 [11881/20014 ( 59%)], Train Loss: 0.52169\n","Epoch: 00 [11891/20014 ( 59%)], Train Loss: 0.52161\n","Epoch: 00 [11901/20014 ( 59%)], Train Loss: 0.52189\n","Epoch: 00 [11911/20014 ( 60%)], Train Loss: 0.52182\n","Epoch: 00 [11921/20014 ( 60%)], Train Loss: 0.52161\n","Epoch: 00 [11931/20014 ( 60%)], Train Loss: 0.52153\n","Epoch: 00 [11941/20014 ( 60%)], Train Loss: 0.52142\n","Epoch: 00 [11951/20014 ( 60%)], Train Loss: 0.52134\n","Epoch: 00 [11961/20014 ( 60%)], Train Loss: 0.52094\n","Epoch: 00 [11971/20014 ( 60%)], Train Loss: 0.52061\n","Epoch: 00 [11981/20014 ( 60%)], Train Loss: 0.52043\n","Epoch: 00 [11991/20014 ( 60%)], Train Loss: 0.52043\n","Epoch: 00 [12001/20014 ( 60%)], Train Loss: 0.52043\n","Epoch: 00 [12011/20014 ( 60%)], Train Loss: 0.52048\n","Epoch: 00 [12021/20014 ( 60%)], Train Loss: 0.52037\n","Epoch: 00 [12031/20014 ( 60%)], Train Loss: 0.52007\n","Epoch: 00 [12041/20014 ( 60%)], Train Loss: 0.51985\n","Epoch: 00 [12051/20014 ( 60%)], Train Loss: 0.51964\n","Epoch: 00 [12061/20014 ( 60%)], Train Loss: 0.51947\n","Epoch: 00 [12071/20014 ( 60%)], Train Loss: 0.51968\n","Epoch: 00 [12081/20014 ( 60%)], Train Loss: 0.51956\n","Epoch: 00 [12091/20014 ( 60%)], Train Loss: 0.51939\n","Epoch: 00 [12101/20014 ( 60%)], Train Loss: 0.51914\n","Epoch: 00 [12111/20014 ( 61%)], Train Loss: 0.51892\n","Epoch: 00 [12121/20014 ( 61%)], Train Loss: 0.51880\n","Epoch: 00 [12131/20014 ( 61%)], Train Loss: 0.51902\n","Epoch: 00 [12141/20014 ( 61%)], Train Loss: 0.51877\n","Epoch: 00 [12151/20014 ( 61%)], Train Loss: 0.51844\n","Epoch: 00 [12161/20014 ( 61%)], Train Loss: 0.51830\n","Epoch: 00 [12171/20014 ( 61%)], Train Loss: 0.51809\n","Epoch: 00 [12181/20014 ( 61%)], Train Loss: 0.51812\n","Epoch: 00 [12191/20014 ( 61%)], Train Loss: 0.51780\n","Epoch: 00 [12201/20014 ( 61%)], Train Loss: 0.51778\n","Epoch: 00 [12211/20014 ( 61%)], Train Loss: 0.51750\n","Epoch: 00 [12221/20014 ( 61%)], Train Loss: 0.51767\n","Epoch: 00 [12231/20014 ( 61%)], Train Loss: 0.51758\n","Epoch: 00 [12241/20014 ( 61%)], Train Loss: 0.51738\n","Epoch: 00 [12251/20014 ( 61%)], Train Loss: 0.51720\n","Epoch: 00 [12261/20014 ( 61%)], Train Loss: 0.51685\n","Epoch: 00 [12271/20014 ( 61%)], Train Loss: 0.51661\n","Epoch: 00 [12281/20014 ( 61%)], Train Loss: 0.51652\n","Epoch: 00 [12291/20014 ( 61%)], Train Loss: 0.51655\n","Epoch: 00 [12301/20014 ( 61%)], Train Loss: 0.51644\n","Epoch: 00 [12311/20014 ( 62%)], Train Loss: 0.51615\n","Epoch: 00 [12321/20014 ( 62%)], Train Loss: 0.51610\n","Epoch: 00 [12331/20014 ( 62%)], Train Loss: 0.51580\n","Epoch: 00 [12341/20014 ( 62%)], Train Loss: 0.51563\n","Epoch: 00 [12351/20014 ( 62%)], Train Loss: 0.51543\n","Epoch: 00 [12361/20014 ( 62%)], Train Loss: 0.51513\n","Epoch: 00 [12371/20014 ( 62%)], Train Loss: 0.51493\n","Epoch: 00 [12381/20014 ( 62%)], Train Loss: 0.51475\n","Epoch: 00 [12391/20014 ( 62%)], Train Loss: 0.51496\n","Epoch: 00 [12401/20014 ( 62%)], Train Loss: 0.51460\n","Epoch: 00 [12411/20014 ( 62%)], Train Loss: 0.51426\n","Epoch: 00 [12421/20014 ( 62%)], Train Loss: 0.51420\n","Epoch: 00 [12431/20014 ( 62%)], Train Loss: 0.51408\n","Epoch: 00 [12441/20014 ( 62%)], Train Loss: 0.51395\n","Epoch: 00 [12451/20014 ( 62%)], Train Loss: 0.51386\n","Epoch: 00 [12461/20014 ( 62%)], Train Loss: 0.51370\n","Epoch: 00 [12471/20014 ( 62%)], Train Loss: 0.51334\n","Epoch: 00 [12481/20014 ( 62%)], Train Loss: 0.51331\n","Epoch: 00 [12491/20014 ( 62%)], Train Loss: 0.51293\n","Epoch: 00 [12501/20014 ( 62%)], Train Loss: 0.51266\n","Epoch: 00 [12511/20014 ( 63%)], Train Loss: 0.51246\n","Epoch: 00 [12521/20014 ( 63%)], Train Loss: 0.51232\n","Epoch: 00 [12531/20014 ( 63%)], Train Loss: 0.51248\n","Epoch: 00 [12541/20014 ( 63%)], Train Loss: 0.51222\n","Epoch: 00 [12551/20014 ( 63%)], Train Loss: 0.51184\n","Epoch: 00 [12561/20014 ( 63%)], Train Loss: 0.51169\n","Epoch: 00 [12571/20014 ( 63%)], Train Loss: 0.51155\n","Epoch: 00 [12581/20014 ( 63%)], Train Loss: 0.51190\n","Epoch: 00 [12591/20014 ( 63%)], Train Loss: 0.51169\n","Epoch: 00 [12601/20014 ( 63%)], Train Loss: 0.51171\n","Epoch: 00 [12611/20014 ( 63%)], Train Loss: 0.51168\n","Epoch: 00 [12621/20014 ( 63%)], Train Loss: 0.51130\n","Epoch: 00 [12631/20014 ( 63%)], Train Loss: 0.51118\n","Epoch: 00 [12641/20014 ( 63%)], Train Loss: 0.51099\n","Epoch: 00 [12651/20014 ( 63%)], Train Loss: 0.51077\n","Epoch: 00 [12661/20014 ( 63%)], Train Loss: 0.51071\n","Epoch: 00 [12671/20014 ( 63%)], Train Loss: 0.51055\n","Epoch: 00 [12681/20014 ( 63%)], Train Loss: 0.51042\n","Epoch: 00 [12691/20014 ( 63%)], Train Loss: 0.51043\n","Epoch: 00 [12701/20014 ( 63%)], Train Loss: 0.51045\n","Epoch: 00 [12711/20014 ( 64%)], Train Loss: 0.51033\n","Epoch: 00 [12721/20014 ( 64%)], Train Loss: 0.51018\n","Epoch: 00 [12731/20014 ( 64%)], Train Loss: 0.51004\n","Epoch: 00 [12741/20014 ( 64%)], Train Loss: 0.50980\n","Epoch: 00 [12751/20014 ( 64%)], Train Loss: 0.50952\n","Epoch: 00 [12761/20014 ( 64%)], Train Loss: 0.50939\n","Epoch: 00 [12771/20014 ( 64%)], Train Loss: 0.50950\n","Epoch: 00 [12781/20014 ( 64%)], Train Loss: 0.50926\n","Epoch: 00 [12791/20014 ( 64%)], Train Loss: 0.50894\n","Epoch: 00 [12801/20014 ( 64%)], Train Loss: 0.50892\n","Epoch: 00 [12811/20014 ( 64%)], Train Loss: 0.50870\n","Epoch: 00 [12821/20014 ( 64%)], Train Loss: 0.50880\n","Epoch: 00 [12831/20014 ( 64%)], Train Loss: 0.50891\n","Epoch: 00 [12841/20014 ( 64%)], Train Loss: 0.50860\n","Epoch: 00 [12851/20014 ( 64%)], Train Loss: 0.50867\n","Epoch: 00 [12861/20014 ( 64%)], Train Loss: 0.50857\n","Epoch: 00 [12871/20014 ( 64%)], Train Loss: 0.50846\n","Epoch: 00 [12881/20014 ( 64%)], Train Loss: 0.50872\n","Epoch: 00 [12891/20014 ( 64%)], Train Loss: 0.50879\n","Epoch: 00 [12901/20014 ( 64%)], Train Loss: 0.50865\n","Epoch: 00 [12911/20014 ( 65%)], Train Loss: 0.50851\n","Epoch: 00 [12921/20014 ( 65%)], Train Loss: 0.50892\n","Epoch: 00 [12931/20014 ( 65%)], Train Loss: 0.50862\n","Epoch: 00 [12941/20014 ( 65%)], Train Loss: 0.50875\n","Epoch: 00 [12951/20014 ( 65%)], Train Loss: 0.50851\n","Epoch: 00 [12961/20014 ( 65%)], Train Loss: 0.50842\n","Epoch: 00 [12971/20014 ( 65%)], Train Loss: 0.50815\n","Epoch: 00 [12981/20014 ( 65%)], Train Loss: 0.50789\n","Epoch: 00 [12991/20014 ( 65%)], Train Loss: 0.50796\n","Epoch: 00 [13001/20014 ( 65%)], Train Loss: 0.50785\n","Epoch: 00 [13011/20014 ( 65%)], Train Loss: 0.50783\n","Epoch: 00 [13021/20014 ( 65%)], Train Loss: 0.50780\n","Epoch: 00 [13031/20014 ( 65%)], Train Loss: 0.50792\n","Epoch: 00 [13041/20014 ( 65%)], Train Loss: 0.50806\n","Epoch: 00 [13051/20014 ( 65%)], Train Loss: 0.50826\n","Epoch: 00 [13061/20014 ( 65%)], Train Loss: 0.50829\n","Epoch: 00 [13071/20014 ( 65%)], Train Loss: 0.50824\n","Epoch: 00 [13081/20014 ( 65%)], Train Loss: 0.50803\n","Epoch: 00 [13091/20014 ( 65%)], Train Loss: 0.50824\n","Epoch: 00 [13101/20014 ( 65%)], Train Loss: 0.50810\n","Epoch: 00 [13111/20014 ( 66%)], Train Loss: 0.50790\n","Epoch: 00 [13121/20014 ( 66%)], Train Loss: 0.50777\n","Epoch: 00 [13131/20014 ( 66%)], Train Loss: 0.50768\n","Epoch: 00 [13141/20014 ( 66%)], Train Loss: 0.50774\n","Epoch: 00 [13151/20014 ( 66%)], Train Loss: 0.50752\n","Epoch: 00 [13161/20014 ( 66%)], Train Loss: 0.50787\n","Epoch: 00 [13171/20014 ( 66%)], Train Loss: 0.50772\n","Epoch: 00 [13181/20014 ( 66%)], Train Loss: 0.50755\n","Epoch: 00 [13191/20014 ( 66%)], Train Loss: 0.50719\n","Epoch: 00 [13201/20014 ( 66%)], Train Loss: 0.50703\n","Epoch: 00 [13211/20014 ( 66%)], Train Loss: 0.50674\n","Epoch: 00 [13221/20014 ( 66%)], Train Loss: 0.50641\n","Epoch: 00 [13231/20014 ( 66%)], Train Loss: 0.50632\n","Epoch: 00 [13241/20014 ( 66%)], Train Loss: 0.50599\n","Epoch: 00 [13251/20014 ( 66%)], Train Loss: 0.50583\n","Epoch: 00 [13261/20014 ( 66%)], Train Loss: 0.50556\n","Epoch: 00 [13271/20014 ( 66%)], Train Loss: 0.50548\n","Epoch: 00 [13281/20014 ( 66%)], Train Loss: 0.50525\n","Epoch: 00 [13291/20014 ( 66%)], Train Loss: 0.50543\n","Epoch: 00 [13301/20014 ( 66%)], Train Loss: 0.50515\n","Epoch: 00 [13311/20014 ( 67%)], Train Loss: 0.50501\n","Epoch: 00 [13321/20014 ( 67%)], Train Loss: 0.50479\n","Epoch: 00 [13331/20014 ( 67%)], Train Loss: 0.50448\n","Epoch: 00 [13341/20014 ( 67%)], Train Loss: 0.50421\n","Epoch: 00 [13351/20014 ( 67%)], Train Loss: 0.50396\n","Epoch: 00 [13361/20014 ( 67%)], Train Loss: 0.50381\n","Epoch: 00 [13371/20014 ( 67%)], Train Loss: 0.50375\n","Epoch: 00 [13381/20014 ( 67%)], Train Loss: 0.50361\n","Epoch: 00 [13391/20014 ( 67%)], Train Loss: 0.50340\n","Epoch: 00 [13401/20014 ( 67%)], Train Loss: 0.50317\n","Epoch: 00 [13411/20014 ( 67%)], Train Loss: 0.50280\n","Epoch: 00 [13421/20014 ( 67%)], Train Loss: 0.50254\n","Epoch: 00 [13431/20014 ( 67%)], Train Loss: 0.50264\n","Epoch: 00 [13441/20014 ( 67%)], Train Loss: 0.50268\n","Epoch: 00 [13451/20014 ( 67%)], Train Loss: 0.50299\n","Epoch: 00 [13461/20014 ( 67%)], Train Loss: 0.50294\n","Epoch: 00 [13471/20014 ( 67%)], Train Loss: 0.50286\n","Epoch: 00 [13481/20014 ( 67%)], Train Loss: 0.50276\n","Epoch: 00 [13491/20014 ( 67%)], Train Loss: 0.50249\n","Epoch: 00 [13501/20014 ( 67%)], Train Loss: 0.50231\n","Epoch: 00 [13511/20014 ( 68%)], Train Loss: 0.50210\n","Epoch: 00 [13521/20014 ( 68%)], Train Loss: 0.50178\n","Epoch: 00 [13531/20014 ( 68%)], Train Loss: 0.50150\n","Epoch: 00 [13541/20014 ( 68%)], Train Loss: 0.50157\n","Epoch: 00 [13551/20014 ( 68%)], Train Loss: 0.50152\n","Epoch: 00 [13561/20014 ( 68%)], Train Loss: 0.50149\n","Epoch: 00 [13571/20014 ( 68%)], Train Loss: 0.50133\n","Epoch: 00 [13581/20014 ( 68%)], Train Loss: 0.50112\n","Epoch: 00 [13591/20014 ( 68%)], Train Loss: 0.50130\n","Epoch: 00 [13601/20014 ( 68%)], Train Loss: 0.50104\n","Epoch: 00 [13611/20014 ( 68%)], Train Loss: 0.50091\n","Epoch: 00 [13621/20014 ( 68%)], Train Loss: 0.50095\n","Epoch: 00 [13631/20014 ( 68%)], Train Loss: 0.50081\n","Epoch: 00 [13641/20014 ( 68%)], Train Loss: 0.50069\n","Epoch: 00 [13651/20014 ( 68%)], Train Loss: 0.50081\n","Epoch: 00 [13661/20014 ( 68%)], Train Loss: 0.50073\n","Epoch: 00 [13671/20014 ( 68%)], Train Loss: 0.50069\n","Epoch: 00 [13681/20014 ( 68%)], Train Loss: 0.50062\n","Epoch: 00 [13691/20014 ( 68%)], Train Loss: 0.50060\n","Epoch: 00 [13701/20014 ( 68%)], Train Loss: 0.50045\n","Epoch: 00 [13711/20014 ( 69%)], Train Loss: 0.50031\n","Epoch: 00 [13721/20014 ( 69%)], Train Loss: 0.50040\n","Epoch: 00 [13731/20014 ( 69%)], Train Loss: 0.50009\n","Epoch: 00 [13741/20014 ( 69%)], Train Loss: 0.50042\n","Epoch: 00 [13751/20014 ( 69%)], Train Loss: 0.50020\n","Epoch: 00 [13761/20014 ( 69%)], Train Loss: 0.49995\n","Epoch: 00 [13771/20014 ( 69%)], Train Loss: 0.49968\n","Epoch: 00 [13781/20014 ( 69%)], Train Loss: 0.49938\n","Epoch: 00 [13791/20014 ( 69%)], Train Loss: 0.49912\n","Epoch: 00 [13801/20014 ( 69%)], Train Loss: 0.49915\n","Epoch: 00 [13811/20014 ( 69%)], Train Loss: 0.49920\n","Epoch: 00 [13821/20014 ( 69%)], Train Loss: 0.49897\n","Epoch: 00 [13831/20014 ( 69%)], Train Loss: 0.49895\n","Epoch: 00 [13841/20014 ( 69%)], Train Loss: 0.49867\n","Epoch: 00 [13851/20014 ( 69%)], Train Loss: 0.49843\n","Epoch: 00 [13861/20014 ( 69%)], Train Loss: 0.49824\n","Epoch: 00 [13871/20014 ( 69%)], Train Loss: 0.49797\n","Epoch: 00 [13881/20014 ( 69%)], Train Loss: 0.49803\n","Epoch: 00 [13891/20014 ( 69%)], Train Loss: 0.49819\n","Epoch: 00 [13901/20014 ( 69%)], Train Loss: 0.49799\n","Epoch: 00 [13911/20014 ( 70%)], Train Loss: 0.49785\n","Epoch: 00 [13921/20014 ( 70%)], Train Loss: 0.49791\n","Epoch: 00 [13931/20014 ( 70%)], Train Loss: 0.49778\n","Epoch: 00 [13941/20014 ( 70%)], Train Loss: 0.49786\n","Epoch: 00 [13951/20014 ( 70%)], Train Loss: 0.49756\n","Epoch: 00 [13961/20014 ( 70%)], Train Loss: 0.49759\n","Epoch: 00 [13971/20014 ( 70%)], Train Loss: 0.49737\n","Epoch: 00 [13981/20014 ( 70%)], Train Loss: 0.49724\n","Epoch: 00 [13991/20014 ( 70%)], Train Loss: 0.49713\n","Epoch: 00 [14001/20014 ( 70%)], Train Loss: 0.49706\n","Epoch: 00 [14011/20014 ( 70%)], Train Loss: 0.49709\n","Epoch: 00 [14021/20014 ( 70%)], Train Loss: 0.49691\n","Epoch: 00 [14031/20014 ( 70%)], Train Loss: 0.49680\n","Epoch: 00 [14041/20014 ( 70%)], Train Loss: 0.49657\n","Epoch: 00 [14051/20014 ( 70%)], Train Loss: 0.49661\n","Epoch: 00 [14061/20014 ( 70%)], Train Loss: 0.49643\n","Epoch: 00 [14071/20014 ( 70%)], Train Loss: 0.49632\n","Epoch: 00 [14081/20014 ( 70%)], Train Loss: 0.49615\n","Epoch: 00 [14091/20014 ( 70%)], Train Loss: 0.49587\n","Epoch: 00 [14101/20014 ( 70%)], Train Loss: 0.49558\n","Epoch: 00 [14111/20014 ( 71%)], Train Loss: 0.49550\n","Epoch: 00 [14121/20014 ( 71%)], Train Loss: 0.49531\n","Epoch: 00 [14131/20014 ( 71%)], Train Loss: 0.49497\n","Epoch: 00 [14141/20014 ( 71%)], Train Loss: 0.49472\n","Epoch: 00 [14151/20014 ( 71%)], Train Loss: 0.49453\n","Epoch: 00 [14161/20014 ( 71%)], Train Loss: 0.49434\n","Epoch: 00 [14171/20014 ( 71%)], Train Loss: 0.49424\n","Epoch: 00 [14181/20014 ( 71%)], Train Loss: 0.49439\n","Epoch: 00 [14191/20014 ( 71%)], Train Loss: 0.49408\n","Epoch: 00 [14201/20014 ( 71%)], Train Loss: 0.49377\n","Epoch: 00 [14211/20014 ( 71%)], Train Loss: 0.49372\n","Epoch: 00 [14221/20014 ( 71%)], Train Loss: 0.49360\n","Epoch: 00 [14231/20014 ( 71%)], Train Loss: 0.49327\n","Epoch: 00 [14241/20014 ( 71%)], Train Loss: 0.49311\n","Epoch: 00 [14251/20014 ( 71%)], Train Loss: 0.49283\n","Epoch: 00 [14261/20014 ( 71%)], Train Loss: 0.49278\n","Epoch: 00 [14271/20014 ( 71%)], Train Loss: 0.49245\n","Epoch: 00 [14281/20014 ( 71%)], Train Loss: 0.49242\n","Epoch: 00 [14291/20014 ( 71%)], Train Loss: 0.49233\n","Epoch: 00 [14301/20014 ( 71%)], Train Loss: 0.49215\n","Epoch: 00 [14311/20014 ( 72%)], Train Loss: 0.49240\n","Epoch: 00 [14321/20014 ( 72%)], Train Loss: 0.49242\n","Epoch: 00 [14331/20014 ( 72%)], Train Loss: 0.49219\n","Epoch: 00 [14341/20014 ( 72%)], Train Loss: 0.49192\n","Epoch: 00 [14351/20014 ( 72%)], Train Loss: 0.49167\n","Epoch: 00 [14361/20014 ( 72%)], Train Loss: 0.49168\n","Epoch: 00 [14371/20014 ( 72%)], Train Loss: 0.49152\n","Epoch: 00 [14381/20014 ( 72%)], Train Loss: 0.49154\n","Epoch: 00 [14391/20014 ( 72%)], Train Loss: 0.49165\n","Epoch: 00 [14401/20014 ( 72%)], Train Loss: 0.49140\n","Epoch: 00 [14411/20014 ( 72%)], Train Loss: 0.49159\n","Epoch: 00 [14421/20014 ( 72%)], Train Loss: 0.49134\n","Epoch: 00 [14431/20014 ( 72%)], Train Loss: 0.49126\n","Epoch: 00 [14441/20014 ( 72%)], Train Loss: 0.49138\n","Epoch: 00 [14451/20014 ( 72%)], Train Loss: 0.49131\n","Epoch: 00 [14461/20014 ( 72%)], Train Loss: 0.49141\n","Epoch: 00 [14471/20014 ( 72%)], Train Loss: 0.49152\n","Epoch: 00 [14481/20014 ( 72%)], Train Loss: 0.49148\n","Epoch: 00 [14491/20014 ( 72%)], Train Loss: 0.49156\n","Epoch: 00 [14501/20014 ( 72%)], Train Loss: 0.49138\n","Epoch: 00 [14511/20014 ( 73%)], Train Loss: 0.49126\n","Epoch: 00 [14521/20014 ( 73%)], Train Loss: 0.49143\n","Epoch: 00 [14531/20014 ( 73%)], Train Loss: 0.49121\n","Epoch: 00 [14541/20014 ( 73%)], Train Loss: 0.49105\n","Epoch: 00 [14551/20014 ( 73%)], Train Loss: 0.49086\n","Epoch: 00 [14561/20014 ( 73%)], Train Loss: 0.49068\n","Epoch: 00 [14571/20014 ( 73%)], Train Loss: 0.49053\n","Epoch: 00 [14581/20014 ( 73%)], Train Loss: 0.49038\n","Epoch: 00 [14591/20014 ( 73%)], Train Loss: 0.49048\n","Epoch: 00 [14601/20014 ( 73%)], Train Loss: 0.49027\n","Epoch: 00 [14611/20014 ( 73%)], Train Loss: 0.48996\n","Epoch: 00 [14621/20014 ( 73%)], Train Loss: 0.49013\n","Epoch: 00 [14631/20014 ( 73%)], Train Loss: 0.49001\n","Epoch: 00 [14641/20014 ( 73%)], Train Loss: 0.49004\n","Epoch: 00 [14651/20014 ( 73%)], Train Loss: 0.48996\n","Epoch: 00 [14661/20014 ( 73%)], Train Loss: 0.48966\n","Epoch: 00 [14671/20014 ( 73%)], Train Loss: 0.48953\n","Epoch: 00 [14681/20014 ( 73%)], Train Loss: 0.48946\n","Epoch: 00 [14691/20014 ( 73%)], Train Loss: 0.48926\n","Epoch: 00 [14701/20014 ( 73%)], Train Loss: 0.48927\n","Epoch: 00 [14711/20014 ( 74%)], Train Loss: 0.48902\n","Epoch: 00 [14721/20014 ( 74%)], Train Loss: 0.48904\n","Epoch: 00 [14731/20014 ( 74%)], Train Loss: 0.48880\n","Epoch: 00 [14741/20014 ( 74%)], Train Loss: 0.48868\n","Epoch: 00 [14751/20014 ( 74%)], Train Loss: 0.48863\n","Epoch: 00 [14761/20014 ( 74%)], Train Loss: 0.48837\n","Epoch: 00 [14771/20014 ( 74%)], Train Loss: 0.48820\n","Epoch: 00 [14781/20014 ( 74%)], Train Loss: 0.48796\n","Epoch: 00 [14791/20014 ( 74%)], Train Loss: 0.48777\n","Epoch: 00 [14801/20014 ( 74%)], Train Loss: 0.48750\n","Epoch: 00 [14811/20014 ( 74%)], Train Loss: 0.48781\n","Epoch: 00 [14821/20014 ( 74%)], Train Loss: 0.48791\n","Epoch: 00 [14831/20014 ( 74%)], Train Loss: 0.48776\n","Epoch: 00 [14841/20014 ( 74%)], Train Loss: 0.48749\n","Epoch: 00 [14851/20014 ( 74%)], Train Loss: 0.48747\n","Epoch: 00 [14861/20014 ( 74%)], Train Loss: 0.48731\n","Epoch: 00 [14871/20014 ( 74%)], Train Loss: 0.48740\n","Epoch: 00 [14881/20014 ( 74%)], Train Loss: 0.48744\n","Epoch: 00 [14891/20014 ( 74%)], Train Loss: 0.48732\n","Epoch: 00 [14901/20014 ( 74%)], Train Loss: 0.48723\n","Epoch: 00 [14911/20014 ( 75%)], Train Loss: 0.48705\n","Epoch: 00 [14921/20014 ( 75%)], Train Loss: 0.48704\n","Epoch: 00 [14931/20014 ( 75%)], Train Loss: 0.48681\n","Epoch: 00 [14941/20014 ( 75%)], Train Loss: 0.48674\n","Epoch: 00 [14951/20014 ( 75%)], Train Loss: 0.48686\n","Epoch: 00 [14961/20014 ( 75%)], Train Loss: 0.48673\n","Epoch: 00 [14971/20014 ( 75%)], Train Loss: 0.48664\n","Epoch: 00 [14981/20014 ( 75%)], Train Loss: 0.48665\n","Epoch: 00 [14991/20014 ( 75%)], Train Loss: 0.48648\n","Epoch: 00 [15001/20014 ( 75%)], Train Loss: 0.48625\n","Epoch: 00 [15011/20014 ( 75%)], Train Loss: 0.48626\n","Epoch: 00 [15021/20014 ( 75%)], Train Loss: 0.48612\n","Epoch: 00 [15031/20014 ( 75%)], Train Loss: 0.48624\n","Epoch: 00 [15041/20014 ( 75%)], Train Loss: 0.48619\n","Epoch: 00 [15051/20014 ( 75%)], Train Loss: 0.48642\n","Epoch: 00 [15061/20014 ( 75%)], Train Loss: 0.48627\n","Epoch: 00 [15071/20014 ( 75%)], Train Loss: 0.48614\n","Epoch: 00 [15081/20014 ( 75%)], Train Loss: 0.48597\n","Epoch: 00 [15091/20014 ( 75%)], Train Loss: 0.48591\n","Epoch: 00 [15101/20014 ( 75%)], Train Loss: 0.48588\n","Epoch: 00 [15111/20014 ( 76%)], Train Loss: 0.48581\n","Epoch: 00 [15121/20014 ( 76%)], Train Loss: 0.48576\n","Epoch: 00 [15131/20014 ( 76%)], Train Loss: 0.48590\n","Epoch: 00 [15141/20014 ( 76%)], Train Loss: 0.48602\n","Epoch: 00 [15151/20014 ( 76%)], Train Loss: 0.48594\n","Epoch: 00 [15161/20014 ( 76%)], Train Loss: 0.48571\n","Epoch: 00 [15171/20014 ( 76%)], Train Loss: 0.48554\n","Epoch: 00 [15181/20014 ( 76%)], Train Loss: 0.48559\n","Epoch: 00 [15191/20014 ( 76%)], Train Loss: 0.48544\n","Epoch: 00 [15201/20014 ( 76%)], Train Loss: 0.48521\n","Epoch: 00 [15211/20014 ( 76%)], Train Loss: 0.48497\n","Epoch: 00 [15221/20014 ( 76%)], Train Loss: 0.48498\n","Epoch: 00 [15231/20014 ( 76%)], Train Loss: 0.48485\n","Epoch: 00 [15241/20014 ( 76%)], Train Loss: 0.48488\n","Epoch: 00 [15251/20014 ( 76%)], Train Loss: 0.48464\n","Epoch: 00 [15261/20014 ( 76%)], Train Loss: 0.48455\n","Epoch: 00 [15271/20014 ( 76%)], Train Loss: 0.48441\n","Epoch: 00 [15281/20014 ( 76%)], Train Loss: 0.48456\n","Epoch: 00 [15291/20014 ( 76%)], Train Loss: 0.48448\n","Epoch: 00 [15301/20014 ( 76%)], Train Loss: 0.48462\n","Epoch: 00 [15311/20014 ( 77%)], Train Loss: 0.48443\n","Epoch: 00 [15321/20014 ( 77%)], Train Loss: 0.48421\n","Epoch: 00 [15331/20014 ( 77%)], Train Loss: 0.48404\n","Epoch: 00 [15341/20014 ( 77%)], Train Loss: 0.48401\n","Epoch: 00 [15351/20014 ( 77%)], Train Loss: 0.48379\n","Epoch: 00 [15361/20014 ( 77%)], Train Loss: 0.48352\n","Epoch: 00 [15371/20014 ( 77%)], Train Loss: 0.48395\n","Epoch: 00 [15381/20014 ( 77%)], Train Loss: 0.48400\n","Epoch: 00 [15391/20014 ( 77%)], Train Loss: 0.48413\n","Epoch: 00 [15401/20014 ( 77%)], Train Loss: 0.48403\n","Epoch: 00 [15411/20014 ( 77%)], Train Loss: 0.48386\n","Epoch: 00 [15421/20014 ( 77%)], Train Loss: 0.48388\n","Epoch: 00 [15431/20014 ( 77%)], Train Loss: 0.48381\n","Epoch: 00 [15441/20014 ( 77%)], Train Loss: 0.48377\n","Epoch: 00 [15451/20014 ( 77%)], Train Loss: 0.48357\n","Epoch: 00 [15461/20014 ( 77%)], Train Loss: 0.48344\n","Epoch: 00 [15471/20014 ( 77%)], Train Loss: 0.48335\n","Epoch: 00 [15481/20014 ( 77%)], Train Loss: 0.48320\n","Epoch: 00 [15491/20014 ( 77%)], Train Loss: 0.48339\n","Epoch: 00 [15501/20014 ( 77%)], Train Loss: 0.48330\n","Epoch: 00 [15511/20014 ( 78%)], Train Loss: 0.48300\n","Epoch: 00 [15521/20014 ( 78%)], Train Loss: 0.48281\n","Epoch: 00 [15531/20014 ( 78%)], Train Loss: 0.48277\n","Epoch: 00 [15541/20014 ( 78%)], Train Loss: 0.48272\n","Epoch: 00 [15551/20014 ( 78%)], Train Loss: 0.48247\n","Epoch: 00 [15561/20014 ( 78%)], Train Loss: 0.48239\n","Epoch: 00 [15571/20014 ( 78%)], Train Loss: 0.48256\n","Epoch: 00 [15581/20014 ( 78%)], Train Loss: 0.48232\n","Epoch: 00 [15591/20014 ( 78%)], Train Loss: 0.48237\n","Epoch: 00 [15601/20014 ( 78%)], Train Loss: 0.48227\n","Epoch: 00 [15611/20014 ( 78%)], Train Loss: 0.48214\n","Epoch: 00 [15621/20014 ( 78%)], Train Loss: 0.48194\n","Epoch: 00 [15631/20014 ( 78%)], Train Loss: 0.48205\n","Epoch: 00 [15641/20014 ( 78%)], Train Loss: 0.48233\n","Epoch: 00 [15651/20014 ( 78%)], Train Loss: 0.48249\n","Epoch: 00 [15661/20014 ( 78%)], Train Loss: 0.48233\n","Epoch: 00 [15671/20014 ( 78%)], Train Loss: 0.48214\n","Epoch: 00 [15681/20014 ( 78%)], Train Loss: 0.48196\n","Epoch: 00 [15691/20014 ( 78%)], Train Loss: 0.48181\n","Epoch: 00 [15701/20014 ( 78%)], Train Loss: 0.48169\n","Epoch: 00 [15711/20014 ( 79%)], Train Loss: 0.48160\n","Epoch: 00 [15721/20014 ( 79%)], Train Loss: 0.48168\n","Epoch: 00 [15731/20014 ( 79%)], Train Loss: 0.48157\n","Epoch: 00 [15741/20014 ( 79%)], Train Loss: 0.48167\n","Epoch: 00 [15751/20014 ( 79%)], Train Loss: 0.48172\n","Epoch: 00 [15761/20014 ( 79%)], Train Loss: 0.48195\n","Epoch: 00 [15771/20014 ( 79%)], Train Loss: 0.48178\n","Epoch: 00 [15781/20014 ( 79%)], Train Loss: 0.48157\n","Epoch: 00 [15791/20014 ( 79%)], Train Loss: 0.48135\n","Epoch: 00 [15801/20014 ( 79%)], Train Loss: 0.48115\n","Epoch: 00 [15811/20014 ( 79%)], Train Loss: 0.48099\n","Epoch: 00 [15821/20014 ( 79%)], Train Loss: 0.48091\n","Epoch: 00 [15831/20014 ( 79%)], Train Loss: 0.48080\n","Epoch: 00 [15841/20014 ( 79%)], Train Loss: 0.48084\n","Epoch: 00 [15851/20014 ( 79%)], Train Loss: 0.48067\n","Epoch: 00 [15861/20014 ( 79%)], Train Loss: 0.48048\n","Epoch: 00 [15871/20014 ( 79%)], Train Loss: 0.48047\n","Epoch: 00 [15881/20014 ( 79%)], Train Loss: 0.48046\n","Epoch: 00 [15891/20014 ( 79%)], Train Loss: 0.48048\n","Epoch: 00 [15901/20014 ( 79%)], Train Loss: 0.48032\n","Epoch: 00 [15911/20014 ( 79%)], Train Loss: 0.48027\n","Epoch: 00 [15921/20014 ( 80%)], Train Loss: 0.48010\n","Epoch: 00 [15931/20014 ( 80%)], Train Loss: 0.48010\n","Epoch: 00 [15941/20014 ( 80%)], Train Loss: 0.48002\n","Epoch: 00 [15951/20014 ( 80%)], Train Loss: 0.47995\n","Epoch: 00 [15961/20014 ( 80%)], Train Loss: 0.48007\n","Epoch: 00 [15971/20014 ( 80%)], Train Loss: 0.48020\n","Epoch: 00 [15981/20014 ( 80%)], Train Loss: 0.48025\n","Epoch: 00 [15991/20014 ( 80%)], Train Loss: 0.48018\n","Epoch: 00 [16001/20014 ( 80%)], Train Loss: 0.47996\n","Epoch: 00 [16011/20014 ( 80%)], Train Loss: 0.47985\n","Epoch: 00 [16021/20014 ( 80%)], Train Loss: 0.47972\n","Epoch: 00 [16031/20014 ( 80%)], Train Loss: 0.47945\n","Epoch: 00 [16041/20014 ( 80%)], Train Loss: 0.47945\n","Epoch: 00 [16051/20014 ( 80%)], Train Loss: 0.47949\n","Epoch: 00 [16061/20014 ( 80%)], Train Loss: 0.47922\n","Epoch: 00 [16071/20014 ( 80%)], Train Loss: 0.47910\n","Epoch: 00 [16081/20014 ( 80%)], Train Loss: 0.47909\n","Epoch: 00 [16091/20014 ( 80%)], Train Loss: 0.47902\n","Epoch: 00 [16101/20014 ( 80%)], Train Loss: 0.47903\n","Epoch: 00 [16111/20014 ( 80%)], Train Loss: 0.47923\n","Epoch: 00 [16121/20014 ( 81%)], Train Loss: 0.47935\n","Epoch: 00 [16131/20014 ( 81%)], Train Loss: 0.47915\n","Epoch: 00 [16141/20014 ( 81%)], Train Loss: 0.47928\n","Epoch: 00 [16151/20014 ( 81%)], Train Loss: 0.47912\n","Epoch: 00 [16161/20014 ( 81%)], Train Loss: 0.47898\n","Epoch: 00 [16171/20014 ( 81%)], Train Loss: 0.47891\n","Epoch: 00 [16181/20014 ( 81%)], Train Loss: 0.47890\n","Epoch: 00 [16191/20014 ( 81%)], Train Loss: 0.47891\n","Epoch: 00 [16201/20014 ( 81%)], Train Loss: 0.47869\n","Epoch: 00 [16211/20014 ( 81%)], Train Loss: 0.47841\n","Epoch: 00 [16221/20014 ( 81%)], Train Loss: 0.47844\n","Epoch: 00 [16231/20014 ( 81%)], Train Loss: 0.47822\n","Epoch: 00 [16241/20014 ( 81%)], Train Loss: 0.47817\n","Epoch: 00 [16251/20014 ( 81%)], Train Loss: 0.47809\n","Epoch: 00 [16261/20014 ( 81%)], Train Loss: 0.47793\n","Epoch: 00 [16271/20014 ( 81%)], Train Loss: 0.47781\n","Epoch: 00 [16281/20014 ( 81%)], Train Loss: 0.47772\n","Epoch: 00 [16291/20014 ( 81%)], Train Loss: 0.47769\n","Epoch: 00 [16301/20014 ( 81%)], Train Loss: 0.47752\n","Epoch: 00 [16311/20014 ( 81%)], Train Loss: 0.47732\n","Epoch: 00 [16321/20014 ( 82%)], Train Loss: 0.47732\n","Epoch: 00 [16331/20014 ( 82%)], Train Loss: 0.47737\n","Epoch: 00 [16341/20014 ( 82%)], Train Loss: 0.47716\n","Epoch: 00 [16351/20014 ( 82%)], Train Loss: 0.47693\n","Epoch: 00 [16361/20014 ( 82%)], Train Loss: 0.47678\n","Epoch: 00 [16371/20014 ( 82%)], Train Loss: 0.47674\n","Epoch: 00 [16381/20014 ( 82%)], Train Loss: 0.47661\n","Epoch: 00 [16391/20014 ( 82%)], Train Loss: 0.47638\n","Epoch: 00 [16401/20014 ( 82%)], Train Loss: 0.47616\n","Epoch: 00 [16411/20014 ( 82%)], Train Loss: 0.47604\n","Epoch: 00 [16421/20014 ( 82%)], Train Loss: 0.47587\n","Epoch: 00 [16431/20014 ( 82%)], Train Loss: 0.47611\n","Epoch: 00 [16441/20014 ( 82%)], Train Loss: 0.47613\n","Epoch: 00 [16451/20014 ( 82%)], Train Loss: 0.47600\n","Epoch: 00 [16461/20014 ( 82%)], Train Loss: 0.47587\n","Epoch: 00 [16471/20014 ( 82%)], Train Loss: 0.47577\n","Epoch: 00 [16481/20014 ( 82%)], Train Loss: 0.47564\n","Epoch: 00 [16491/20014 ( 82%)], Train Loss: 0.47558\n","Epoch: 00 [16501/20014 ( 82%)], Train Loss: 0.47533\n","Epoch: 00 [16511/20014 ( 82%)], Train Loss: 0.47507\n","Epoch: 00 [16521/20014 ( 83%)], Train Loss: 0.47486\n","Epoch: 00 [16531/20014 ( 83%)], Train Loss: 0.47480\n","Epoch: 00 [16541/20014 ( 83%)], Train Loss: 0.47464\n","Epoch: 00 [16551/20014 ( 83%)], Train Loss: 0.47465\n","Epoch: 00 [16561/20014 ( 83%)], Train Loss: 0.47462\n","Epoch: 00 [16571/20014 ( 83%)], Train Loss: 0.47464\n","Epoch: 00 [16581/20014 ( 83%)], Train Loss: 0.47441\n","Epoch: 00 [16591/20014 ( 83%)], Train Loss: 0.47428\n","Epoch: 00 [16601/20014 ( 83%)], Train Loss: 0.47412\n","Epoch: 00 [16611/20014 ( 83%)], Train Loss: 0.47414\n","Epoch: 00 [16621/20014 ( 83%)], Train Loss: 0.47405\n","Epoch: 00 [16631/20014 ( 83%)], Train Loss: 0.47379\n","Epoch: 00 [16641/20014 ( 83%)], Train Loss: 0.47364\n","Epoch: 00 [16651/20014 ( 83%)], Train Loss: 0.47372\n","Epoch: 00 [16661/20014 ( 83%)], Train Loss: 0.47375\n","Epoch: 00 [16671/20014 ( 83%)], Train Loss: 0.47353\n","Epoch: 00 [16681/20014 ( 83%)], Train Loss: 0.47355\n","Epoch: 00 [16691/20014 ( 83%)], Train Loss: 0.47344\n","Epoch: 00 [16701/20014 ( 83%)], Train Loss: 0.47346\n","Epoch: 00 [16711/20014 ( 83%)], Train Loss: 0.47324\n","Epoch: 00 [16721/20014 ( 84%)], Train Loss: 0.47315\n","Epoch: 00 [16731/20014 ( 84%)], Train Loss: 0.47307\n","Epoch: 00 [16741/20014 ( 84%)], Train Loss: 0.47302\n","Epoch: 00 [16751/20014 ( 84%)], Train Loss: 0.47283\n","Epoch: 00 [16761/20014 ( 84%)], Train Loss: 0.47272\n","Epoch: 00 [16771/20014 ( 84%)], Train Loss: 0.47258\n","Epoch: 00 [16781/20014 ( 84%)], Train Loss: 0.47246\n","Epoch: 00 [16791/20014 ( 84%)], Train Loss: 0.47263\n","Epoch: 00 [16801/20014 ( 84%)], Train Loss: 0.47253\n","Epoch: 00 [16811/20014 ( 84%)], Train Loss: 0.47228\n","Epoch: 00 [16821/20014 ( 84%)], Train Loss: 0.47210\n","Epoch: 00 [16831/20014 ( 84%)], Train Loss: 0.47209\n","Epoch: 00 [16841/20014 ( 84%)], Train Loss: 0.47190\n","Epoch: 00 [16851/20014 ( 84%)], Train Loss: 0.47168\n","Epoch: 00 [16861/20014 ( 84%)], Train Loss: 0.47179\n","Epoch: 00 [16871/20014 ( 84%)], Train Loss: 0.47163\n","Epoch: 00 [16881/20014 ( 84%)], Train Loss: 0.47148\n","Epoch: 00 [16891/20014 ( 84%)], Train Loss: 0.47135\n","Epoch: 00 [16901/20014 ( 84%)], Train Loss: 0.47128\n","Epoch: 00 [16911/20014 ( 84%)], Train Loss: 0.47134\n","Epoch: 00 [16921/20014 ( 85%)], Train Loss: 0.47123\n","Epoch: 00 [16931/20014 ( 85%)], Train Loss: 0.47116\n","Epoch: 00 [16941/20014 ( 85%)], Train Loss: 0.47094\n","Epoch: 00 [16951/20014 ( 85%)], Train Loss: 0.47106\n","Epoch: 00 [16961/20014 ( 85%)], Train Loss: 0.47084\n","Epoch: 00 [16971/20014 ( 85%)], Train Loss: 0.47072\n","Epoch: 00 [16981/20014 ( 85%)], Train Loss: 0.47078\n","Epoch: 00 [16991/20014 ( 85%)], Train Loss: 0.47060\n","Epoch: 00 [17001/20014 ( 85%)], Train Loss: 0.47043\n","Epoch: 00 [17011/20014 ( 85%)], Train Loss: 0.47035\n","Epoch: 00 [17021/20014 ( 85%)], Train Loss: 0.47029\n","Epoch: 00 [17031/20014 ( 85%)], Train Loss: 0.47006\n","Epoch: 00 [17041/20014 ( 85%)], Train Loss: 0.46998\n","Epoch: 00 [17051/20014 ( 85%)], Train Loss: 0.46990\n","Epoch: 00 [17061/20014 ( 85%)], Train Loss: 0.46989\n","Epoch: 00 [17071/20014 ( 85%)], Train Loss: 0.46973\n","Epoch: 00 [17081/20014 ( 85%)], Train Loss: 0.46961\n","Epoch: 00 [17091/20014 ( 85%)], Train Loss: 0.46949\n","Epoch: 00 [17101/20014 ( 85%)], Train Loss: 0.46933\n","Epoch: 00 [17111/20014 ( 85%)], Train Loss: 0.46944\n","Epoch: 00 [17121/20014 ( 86%)], Train Loss: 0.46939\n","Epoch: 00 [17131/20014 ( 86%)], Train Loss: 0.46941\n","Epoch: 00 [17141/20014 ( 86%)], Train Loss: 0.46944\n","Epoch: 00 [17151/20014 ( 86%)], Train Loss: 0.46968\n","Epoch: 00 [17161/20014 ( 86%)], Train Loss: 0.46962\n","Epoch: 00 [17171/20014 ( 86%)], Train Loss: 0.46993\n","Epoch: 00 [17181/20014 ( 86%)], Train Loss: 0.47000\n","Epoch: 00 [17191/20014 ( 86%)], Train Loss: 0.46981\n","Epoch: 00 [17201/20014 ( 86%)], Train Loss: 0.46978\n","Epoch: 00 [17211/20014 ( 86%)], Train Loss: 0.46975\n","Epoch: 00 [17221/20014 ( 86%)], Train Loss: 0.46969\n","Epoch: 00 [17231/20014 ( 86%)], Train Loss: 0.46970\n","Epoch: 00 [17241/20014 ( 86%)], Train Loss: 0.46992\n","Epoch: 00 [17251/20014 ( 86%)], Train Loss: 0.46990\n","Epoch: 00 [17261/20014 ( 86%)], Train Loss: 0.46997\n","Epoch: 00 [17271/20014 ( 86%)], Train Loss: 0.47012\n","Epoch: 00 [17281/20014 ( 86%)], Train Loss: 0.47003\n","Epoch: 00 [17291/20014 ( 86%)], Train Loss: 0.46986\n","Epoch: 00 [17301/20014 ( 86%)], Train Loss: 0.46985\n","Epoch: 00 [17311/20014 ( 86%)], Train Loss: 0.46995\n","Epoch: 00 [17321/20014 ( 87%)], Train Loss: 0.46978\n","Epoch: 00 [17331/20014 ( 87%)], Train Loss: 0.46961\n","Epoch: 00 [17341/20014 ( 87%)], Train Loss: 0.46948\n","Epoch: 00 [17351/20014 ( 87%)], Train Loss: 0.46950\n","Epoch: 00 [17361/20014 ( 87%)], Train Loss: 0.46944\n","Epoch: 00 [17371/20014 ( 87%)], Train Loss: 0.46943\n","Epoch: 00 [17381/20014 ( 87%)], Train Loss: 0.46984\n","Epoch: 00 [17391/20014 ( 87%)], Train Loss: 0.46972\n","Epoch: 00 [17401/20014 ( 87%)], Train Loss: 0.46963\n","Epoch: 00 [17411/20014 ( 87%)], Train Loss: 0.46959\n","Epoch: 00 [17421/20014 ( 87%)], Train Loss: 0.46952\n","Epoch: 00 [17431/20014 ( 87%)], Train Loss: 0.46940\n","Epoch: 00 [17441/20014 ( 87%)], Train Loss: 0.46921\n","Epoch: 00 [17451/20014 ( 87%)], Train Loss: 0.46943\n","Epoch: 00 [17461/20014 ( 87%)], Train Loss: 0.46942\n","Epoch: 00 [17471/20014 ( 87%)], Train Loss: 0.46920\n","Epoch: 00 [17481/20014 ( 87%)], Train Loss: 0.46925\n","Epoch: 00 [17491/20014 ( 87%)], Train Loss: 0.46901\n","Epoch: 00 [17501/20014 ( 87%)], Train Loss: 0.46893\n","Epoch: 00 [17511/20014 ( 87%)], Train Loss: 0.46881\n","Epoch: 00 [17521/20014 ( 88%)], Train Loss: 0.46865\n","Epoch: 00 [17531/20014 ( 88%)], Train Loss: 0.46848\n","Epoch: 00 [17541/20014 ( 88%)], Train Loss: 0.46856\n","Epoch: 00 [17551/20014 ( 88%)], Train Loss: 0.46831\n","Epoch: 00 [17561/20014 ( 88%)], Train Loss: 0.46809\n","Epoch: 00 [17571/20014 ( 88%)], Train Loss: 0.46793\n","Epoch: 00 [17581/20014 ( 88%)], Train Loss: 0.46791\n","Epoch: 00 [17591/20014 ( 88%)], Train Loss: 0.46789\n","Epoch: 00 [17601/20014 ( 88%)], Train Loss: 0.46809\n","Epoch: 00 [17611/20014 ( 88%)], Train Loss: 0.46819\n","Epoch: 00 [17621/20014 ( 88%)], Train Loss: 0.46830\n","Epoch: 00 [17631/20014 ( 88%)], Train Loss: 0.46832\n","Epoch: 00 [17641/20014 ( 88%)], Train Loss: 0.46820\n","Epoch: 00 [17651/20014 ( 88%)], Train Loss: 0.46807\n","Epoch: 00 [17661/20014 ( 88%)], Train Loss: 0.46803\n","Epoch: 00 [17671/20014 ( 88%)], Train Loss: 0.46804\n","Epoch: 00 [17681/20014 ( 88%)], Train Loss: 0.46784\n","Epoch: 00 [17691/20014 ( 88%)], Train Loss: 0.46788\n","Epoch: 00 [17701/20014 ( 88%)], Train Loss: 0.46771\n","Epoch: 00 [17711/20014 ( 88%)], Train Loss: 0.46751\n","Epoch: 00 [17721/20014 ( 89%)], Train Loss: 0.46740\n","Epoch: 00 [17731/20014 ( 89%)], Train Loss: 0.46720\n","Epoch: 00 [17741/20014 ( 89%)], Train Loss: 0.46733\n","Epoch: 00 [17751/20014 ( 89%)], Train Loss: 0.46732\n","Epoch: 00 [17761/20014 ( 89%)], Train Loss: 0.46740\n","Epoch: 00 [17771/20014 ( 89%)], Train Loss: 0.46741\n","Epoch: 00 [17781/20014 ( 89%)], Train Loss: 0.46730\n","Epoch: 00 [17791/20014 ( 89%)], Train Loss: 0.46733\n","Epoch: 00 [17801/20014 ( 89%)], Train Loss: 0.46742\n","Epoch: 00 [17811/20014 ( 89%)], Train Loss: 0.46730\n","Epoch: 00 [17821/20014 ( 89%)], Train Loss: 0.46750\n","Epoch: 00 [17831/20014 ( 89%)], Train Loss: 0.46742\n","Epoch: 00 [17841/20014 ( 89%)], Train Loss: 0.46742\n","Epoch: 00 [17851/20014 ( 89%)], Train Loss: 0.46735\n","Epoch: 00 [17861/20014 ( 89%)], Train Loss: 0.46763\n","Epoch: 00 [17871/20014 ( 89%)], Train Loss: 0.46812\n","Epoch: 00 [17881/20014 ( 89%)], Train Loss: 0.46803\n","Epoch: 00 [17891/20014 ( 89%)], Train Loss: 0.46812\n","Epoch: 00 [17901/20014 ( 89%)], Train Loss: 0.46806\n","Epoch: 00 [17911/20014 ( 89%)], Train Loss: 0.46803\n","Epoch: 00 [17921/20014 ( 90%)], Train Loss: 0.46809\n","Epoch: 00 [17931/20014 ( 90%)], Train Loss: 0.46785\n","Epoch: 00 [17941/20014 ( 90%)], Train Loss: 0.46783\n","Epoch: 00 [17951/20014 ( 90%)], Train Loss: 0.46772\n","Epoch: 00 [17961/20014 ( 90%)], Train Loss: 0.46792\n","Epoch: 00 [17971/20014 ( 90%)], Train Loss: 0.46774\n","Epoch: 00 [17981/20014 ( 90%)], Train Loss: 0.46765\n","Epoch: 00 [17991/20014 ( 90%)], Train Loss: 0.46760\n","Epoch: 00 [18001/20014 ( 90%)], Train Loss: 0.46751\n","Epoch: 00 [18011/20014 ( 90%)], Train Loss: 0.46741\n","Epoch: 00 [18021/20014 ( 90%)], Train Loss: 0.46726\n","Epoch: 00 [18031/20014 ( 90%)], Train Loss: 0.46728\n","Epoch: 00 [18041/20014 ( 90%)], Train Loss: 0.46729\n","Epoch: 00 [18051/20014 ( 90%)], Train Loss: 0.46710\n","Epoch: 00 [18061/20014 ( 90%)], Train Loss: 0.46697\n","Epoch: 00 [18071/20014 ( 90%)], Train Loss: 0.46700\n","Epoch: 00 [18081/20014 ( 90%)], Train Loss: 0.46683\n","Epoch: 00 [18091/20014 ( 90%)], Train Loss: 0.46681\n","Epoch: 00 [18101/20014 ( 90%)], Train Loss: 0.46671\n","Epoch: 00 [18111/20014 ( 90%)], Train Loss: 0.46652\n","Epoch: 00 [18121/20014 ( 91%)], Train Loss: 0.46649\n","Epoch: 00 [18131/20014 ( 91%)], Train Loss: 0.46658\n","Epoch: 00 [18141/20014 ( 91%)], Train Loss: 0.46661\n","Epoch: 00 [18151/20014 ( 91%)], Train Loss: 0.46639\n","Epoch: 00 [18161/20014 ( 91%)], Train Loss: 0.46615\n","Epoch: 00 [18171/20014 ( 91%)], Train Loss: 0.46614\n","Epoch: 00 [18181/20014 ( 91%)], Train Loss: 0.46607\n","Epoch: 00 [18191/20014 ( 91%)], Train Loss: 0.46596\n","Epoch: 00 [18201/20014 ( 91%)], Train Loss: 0.46621\n","Epoch: 00 [18211/20014 ( 91%)], Train Loss: 0.46607\n","Epoch: 00 [18221/20014 ( 91%)], Train Loss: 0.46606\n","Epoch: 00 [18231/20014 ( 91%)], Train Loss: 0.46602\n","Epoch: 00 [18241/20014 ( 91%)], Train Loss: 0.46597\n","Epoch: 00 [18251/20014 ( 91%)], Train Loss: 0.46584\n","Epoch: 00 [18261/20014 ( 91%)], Train Loss: 0.46570\n","Epoch: 00 [18271/20014 ( 91%)], Train Loss: 0.46566\n","Epoch: 00 [18281/20014 ( 91%)], Train Loss: 0.46554\n","Epoch: 00 [18291/20014 ( 91%)], Train Loss: 0.46548\n","Epoch: 00 [18301/20014 ( 91%)], Train Loss: 0.46523\n","Epoch: 00 [18311/20014 ( 91%)], Train Loss: 0.46536\n","Epoch: 00 [18321/20014 ( 92%)], Train Loss: 0.46538\n","Epoch: 00 [18331/20014 ( 92%)], Train Loss: 0.46526\n","Epoch: 00 [18341/20014 ( 92%)], Train Loss: 0.46516\n","Epoch: 00 [18351/20014 ( 92%)], Train Loss: 0.46506\n","Epoch: 00 [18361/20014 ( 92%)], Train Loss: 0.46511\n","Epoch: 00 [18371/20014 ( 92%)], Train Loss: 0.46516\n","Epoch: 00 [18381/20014 ( 92%)], Train Loss: 0.46556\n","Epoch: 00 [18391/20014 ( 92%)], Train Loss: 0.46538\n","Epoch: 00 [18401/20014 ( 92%)], Train Loss: 0.46523\n","Epoch: 00 [18411/20014 ( 92%)], Train Loss: 0.46524\n","Epoch: 00 [18421/20014 ( 92%)], Train Loss: 0.46509\n","Epoch: 00 [18431/20014 ( 92%)], Train Loss: 0.46495\n","Epoch: 00 [18441/20014 ( 92%)], Train Loss: 0.46481\n","Epoch: 00 [18451/20014 ( 92%)], Train Loss: 0.46474\n","Epoch: 00 [18461/20014 ( 92%)], Train Loss: 0.46482\n","Epoch: 00 [18471/20014 ( 92%)], Train Loss: 0.46491\n","Epoch: 00 [18481/20014 ( 92%)], Train Loss: 0.46507\n","Epoch: 00 [18491/20014 ( 92%)], Train Loss: 0.46499\n","Epoch: 00 [18501/20014 ( 92%)], Train Loss: 0.46480\n","Epoch: 00 [18511/20014 ( 92%)], Train Loss: 0.46503\n","Epoch: 00 [18521/20014 ( 93%)], Train Loss: 0.46532\n","Epoch: 00 [18531/20014 ( 93%)], Train Loss: 0.46544\n","Epoch: 00 [18541/20014 ( 93%)], Train Loss: 0.46534\n","Epoch: 00 [18551/20014 ( 93%)], Train Loss: 0.46537\n","Epoch: 00 [18561/20014 ( 93%)], Train Loss: 0.46531\n","Epoch: 00 [18571/20014 ( 93%)], Train Loss: 0.46533\n","Epoch: 00 [18581/20014 ( 93%)], Train Loss: 0.46518\n","Epoch: 00 [18591/20014 ( 93%)], Train Loss: 0.46501\n","Epoch: 00 [18601/20014 ( 93%)], Train Loss: 0.46480\n","Epoch: 00 [18611/20014 ( 93%)], Train Loss: 0.46465\n","Epoch: 00 [18621/20014 ( 93%)], Train Loss: 0.46449\n","Epoch: 00 [18631/20014 ( 93%)], Train Loss: 0.46444\n","Epoch: 00 [18641/20014 ( 93%)], Train Loss: 0.46430\n","Epoch: 00 [18651/20014 ( 93%)], Train Loss: 0.46447\n","Epoch: 00 [18661/20014 ( 93%)], Train Loss: 0.46438\n","Epoch: 00 [18671/20014 ( 93%)], Train Loss: 0.46420\n","Epoch: 00 [18681/20014 ( 93%)], Train Loss: 0.46408\n","Epoch: 00 [18691/20014 ( 93%)], Train Loss: 0.46406\n","Epoch: 00 [18701/20014 ( 93%)], Train Loss: 0.46408\n","Epoch: 00 [18711/20014 ( 93%)], Train Loss: 0.46406\n","Epoch: 00 [18721/20014 ( 94%)], Train Loss: 0.46393\n","Epoch: 00 [18731/20014 ( 94%)], Train Loss: 0.46405\n","Epoch: 00 [18741/20014 ( 94%)], Train Loss: 0.46384\n","Epoch: 00 [18751/20014 ( 94%)], Train Loss: 0.46361\n","Epoch: 00 [18761/20014 ( 94%)], Train Loss: 0.46346\n","Epoch: 00 [18771/20014 ( 94%)], Train Loss: 0.46337\n","Epoch: 00 [18781/20014 ( 94%)], Train Loss: 0.46326\n","Epoch: 00 [18791/20014 ( 94%)], Train Loss: 0.46324\n","Epoch: 00 [18801/20014 ( 94%)], Train Loss: 0.46301\n","Epoch: 00 [18811/20014 ( 94%)], Train Loss: 0.46290\n","Epoch: 00 [18821/20014 ( 94%)], Train Loss: 0.46273\n","Epoch: 00 [18831/20014 ( 94%)], Train Loss: 0.46272\n","Epoch: 00 [18841/20014 ( 94%)], Train Loss: 0.46262\n","Epoch: 00 [18851/20014 ( 94%)], Train Loss: 0.46257\n","Epoch: 00 [18861/20014 ( 94%)], Train Loss: 0.46239\n","Epoch: 00 [18871/20014 ( 94%)], Train Loss: 0.46250\n","Epoch: 00 [18881/20014 ( 94%)], Train Loss: 0.46246\n","Epoch: 00 [18891/20014 ( 94%)], Train Loss: 0.46234\n","Epoch: 00 [18901/20014 ( 94%)], Train Loss: 0.46229\n","Epoch: 00 [18911/20014 ( 94%)], Train Loss: 0.46241\n","Epoch: 00 [18921/20014 ( 95%)], Train Loss: 0.46231\n","Epoch: 00 [18931/20014 ( 95%)], Train Loss: 0.46237\n","Epoch: 00 [18941/20014 ( 95%)], Train Loss: 0.46220\n","Epoch: 00 [18951/20014 ( 95%)], Train Loss: 0.46257\n","Epoch: 00 [18961/20014 ( 95%)], Train Loss: 0.46254\n","Epoch: 00 [18971/20014 ( 95%)], Train Loss: 0.46243\n","Epoch: 00 [18981/20014 ( 95%)], Train Loss: 0.46238\n","Epoch: 00 [18991/20014 ( 95%)], Train Loss: 0.46226\n","Epoch: 00 [19001/20014 ( 95%)], Train Loss: 0.46202\n","Epoch: 00 [19011/20014 ( 95%)], Train Loss: 0.46207\n","Epoch: 00 [19021/20014 ( 95%)], Train Loss: 0.46202\n","Epoch: 00 [19031/20014 ( 95%)], Train Loss: 0.46197\n","Epoch: 00 [19041/20014 ( 95%)], Train Loss: 0.46178\n","Epoch: 00 [19051/20014 ( 95%)], Train Loss: 0.46167\n","Epoch: 00 [19061/20014 ( 95%)], Train Loss: 0.46156\n","Epoch: 00 [19071/20014 ( 95%)], Train Loss: 0.46163\n","Epoch: 00 [19081/20014 ( 95%)], Train Loss: 0.46161\n","Epoch: 00 [19091/20014 ( 95%)], Train Loss: 0.46156\n","Epoch: 00 [19101/20014 ( 95%)], Train Loss: 0.46141\n","Epoch: 00 [19111/20014 ( 95%)], Train Loss: 0.46126\n","Epoch: 00 [19121/20014 ( 96%)], Train Loss: 0.46105\n","Epoch: 00 [19131/20014 ( 96%)], Train Loss: 0.46083\n","Epoch: 00 [19141/20014 ( 96%)], Train Loss: 0.46106\n","Epoch: 00 [19151/20014 ( 96%)], Train Loss: 0.46136\n","Epoch: 00 [19161/20014 ( 96%)], Train Loss: 0.46113\n","Epoch: 00 [19171/20014 ( 96%)], Train Loss: 0.46127\n","Epoch: 00 [19181/20014 ( 96%)], Train Loss: 0.46139\n","Epoch: 00 [19191/20014 ( 96%)], Train Loss: 0.46123\n","Epoch: 00 [19201/20014 ( 96%)], Train Loss: 0.46114\n","Epoch: 00 [19211/20014 ( 96%)], Train Loss: 0.46094\n","Epoch: 00 [19221/20014 ( 96%)], Train Loss: 0.46112\n","Epoch: 00 [19231/20014 ( 96%)], Train Loss: 0.46114\n","Epoch: 00 [19241/20014 ( 96%)], Train Loss: 0.46102\n","Epoch: 00 [19251/20014 ( 96%)], Train Loss: 0.46103\n","Epoch: 00 [19261/20014 ( 96%)], Train Loss: 0.46112\n","Epoch: 00 [19271/20014 ( 96%)], Train Loss: 0.46090\n","Epoch: 00 [19281/20014 ( 96%)], Train Loss: 0.46072\n","Epoch: 00 [19291/20014 ( 96%)], Train Loss: 0.46067\n","Epoch: 00 [19301/20014 ( 96%)], Train Loss: 0.46058\n","Epoch: 00 [19311/20014 ( 96%)], Train Loss: 0.46045\n","Epoch: 00 [19321/20014 ( 97%)], Train Loss: 0.46048\n","Epoch: 00 [19331/20014 ( 97%)], Train Loss: 0.46029\n","Epoch: 00 [19341/20014 ( 97%)], Train Loss: 0.46014\n","Epoch: 00 [19351/20014 ( 97%)], Train Loss: 0.45995\n","Epoch: 00 [19361/20014 ( 97%)], Train Loss: 0.45983\n","Epoch: 00 [19371/20014 ( 97%)], Train Loss: 0.45971\n","Epoch: 00 [19381/20014 ( 97%)], Train Loss: 0.45961\n","Epoch: 00 [19391/20014 ( 97%)], Train Loss: 0.45947\n","Epoch: 00 [19401/20014 ( 97%)], Train Loss: 0.45953\n","Epoch: 00 [19411/20014 ( 97%)], Train Loss: 0.45955\n","Epoch: 00 [19421/20014 ( 97%)], Train Loss: 0.45933\n","Epoch: 00 [19431/20014 ( 97%)], Train Loss: 0.45915\n","Epoch: 00 [19441/20014 ( 97%)], Train Loss: 0.45930\n","Epoch: 00 [19451/20014 ( 97%)], Train Loss: 0.45916\n","Epoch: 00 [19461/20014 ( 97%)], Train Loss: 0.45906\n","Epoch: 00 [19471/20014 ( 97%)], Train Loss: 0.45900\n","Epoch: 00 [19481/20014 ( 97%)], Train Loss: 0.45892\n","Epoch: 00 [19491/20014 ( 97%)], Train Loss: 0.45886\n","Epoch: 00 [19501/20014 ( 97%)], Train Loss: 0.45889\n","Epoch: 00 [19511/20014 ( 97%)], Train Loss: 0.45871\n","Epoch: 00 [19521/20014 ( 98%)], Train Loss: 0.45849\n","Epoch: 00 [19531/20014 ( 98%)], Train Loss: 0.45837\n","Epoch: 00 [19541/20014 ( 98%)], Train Loss: 0.45824\n","Epoch: 00 [19551/20014 ( 98%)], Train Loss: 0.45834\n","Epoch: 00 [19561/20014 ( 98%)], Train Loss: 0.45841\n","Epoch: 00 [19571/20014 ( 98%)], Train Loss: 0.45819\n","Epoch: 00 [19581/20014 ( 98%)], Train Loss: 0.45812\n","Epoch: 00 [19591/20014 ( 98%)], Train Loss: 0.45796\n","Epoch: 00 [19601/20014 ( 98%)], Train Loss: 0.45783\n","Epoch: 00 [19611/20014 ( 98%)], Train Loss: 0.45766\n","Epoch: 00 [19621/20014 ( 98%)], Train Loss: 0.45752\n","Epoch: 00 [19631/20014 ( 98%)], Train Loss: 0.45749\n","Epoch: 00 [19641/20014 ( 98%)], Train Loss: 0.45731\n","Epoch: 00 [19651/20014 ( 98%)], Train Loss: 0.45722\n","Epoch: 00 [19661/20014 ( 98%)], Train Loss: 0.45706\n","Epoch: 00 [19671/20014 ( 98%)], Train Loss: 0.45701\n","Epoch: 00 [19681/20014 ( 98%)], Train Loss: 0.45687\n","Epoch: 00 [19691/20014 ( 98%)], Train Loss: 0.45677\n","Epoch: 00 [19701/20014 ( 98%)], Train Loss: 0.45665\n","Epoch: 00 [19711/20014 ( 98%)], Train Loss: 0.45645\n","Epoch: 00 [19721/20014 ( 99%)], Train Loss: 0.45625\n","Epoch: 00 [19731/20014 ( 99%)], Train Loss: 0.45628\n","Epoch: 00 [19741/20014 ( 99%)], Train Loss: 0.45631\n","Epoch: 00 [19751/20014 ( 99%)], Train Loss: 0.45616\n","Epoch: 00 [19761/20014 ( 99%)], Train Loss: 0.45598\n","Epoch: 00 [19771/20014 ( 99%)], Train Loss: 0.45615\n","Epoch: 00 [19781/20014 ( 99%)], Train Loss: 0.45613\n","Epoch: 00 [19791/20014 ( 99%)], Train Loss: 0.45598\n","Epoch: 00 [19801/20014 ( 99%)], Train Loss: 0.45585\n","Epoch: 00 [19811/20014 ( 99%)], Train Loss: 0.45570\n","Epoch: 00 [19821/20014 ( 99%)], Train Loss: 0.45565\n","Epoch: 00 [19831/20014 ( 99%)], Train Loss: 0.45552\n","Epoch: 00 [19841/20014 ( 99%)], Train Loss: 0.45541\n","Epoch: 00 [19851/20014 ( 99%)], Train Loss: 0.45546\n","Epoch: 00 [19861/20014 ( 99%)], Train Loss: 0.45550\n","Epoch: 00 [19871/20014 ( 99%)], Train Loss: 0.45544\n","Epoch: 00 [19881/20014 ( 99%)], Train Loss: 0.45525\n","Epoch: 00 [19891/20014 ( 99%)], Train Loss: 0.45514\n","Epoch: 00 [19901/20014 ( 99%)], Train Loss: 0.45505\n","Epoch: 00 [19911/20014 ( 99%)], Train Loss: 0.45497\n","Epoch: 00 [19921/20014 (100%)], Train Loss: 0.45481\n","Epoch: 00 [19931/20014 (100%)], Train Loss: 0.45472\n"]}]},{"cell_type":"code","metadata":{"id":"o6-F5rzbHSK5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635276130287,"user_tz":-330,"elapsed":611,"user":{"displayName":"2020 11053","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11099621414422210383"}},"outputId":"b5414656-9ee0-4e63-cc67-adabc1dbfa2d"},"source":["! cp -r /content/output/checkpoint-fold-1 /content/drive/Shareddrives/NLP/Dataset/murli"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cp: cannot stat '/content/output/checkpoint-fold-1': No such file or directory\n"]}]},{"cell_type":"code","metadata":{"id":"LdC5X_O7HFVz"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"90k-6WFoWvud"},"source":[""],"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Murli4.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"peBfa4c_y7tX","executionInfo":{"status":"ok","timestamp":1635271866507,"user_tz":-330,"elapsed":36406,"user":{"displayName":"2020 11032","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15452284625747026929"}},"outputId":"6614da98-416e-44a7-ba02-b8c529520f96"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"faIyP9zZEd0G","executionInfo":{"status":"ok","timestamp":1635271882310,"user_tz":-330,"elapsed":9122,"user":{"displayName":"2020 11032","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15452284625747026929"}},"outputId":"82b0ff57-14c8-4321-e25a-c888fbb49a18"},"source":["! pip install transformers sentencepiece"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 5.1 MB/s \n","\u001b[?25hCollecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 39.8 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 38.9 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 41.2 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 38.5 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n","Collecting huggingface-hub>=0.0.17\n","  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n","\u001b[K     |████████████████████████████████| 56 kB 4.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.19 pyyaml-6.0 sacremoses-0.0.46 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.11.3\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jDu4BWB1EJJ3","executionInfo":{"status":"ok","timestamp":1635271944938,"user_tz":-330,"elapsed":410,"user":{"displayName":"2020 11032","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15452284625747026929"}},"outputId":"90980ecb-662d-4b6e-bcb5-7ae2a8654db8"},"source":["import os\n","import gc\n","gc.enable()\n","import math\n","import json\n","import time\n","import random\n","import multiprocessing\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm, trange\n","from sklearn import model_selection\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import Parameter\n","import torch.optim as optim\n","from torch.utils.data import (\n","    Dataset, DataLoader,\n","    SequentialSampler, RandomSampler\n",")\n","from torch.utils.data.distributed import DistributedSampler\n","\n","try:\n","    from apex import amp\n","    APEX_INSTALLED = True\n","except ImportError:\n","    APEX_INSTALLED = False\n","\n","import transformers\n","from transformers import (\n","    WEIGHTS_NAME,\n","    AdamW,\n","    AutoConfig,\n","    AutoModel,\n","    AutoTokenizer,\n","    get_cosine_schedule_with_warmup,\n","    get_linear_schedule_with_warmup,\n","    logging,\n","    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n",")\n","logging.set_verbosity_warning()\n","logging.set_verbosity_error()\n","\n","def fix_all_seeds(seed):\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","def optimal_num_of_loader_workers():\n","    num_cpus = multiprocessing.cpu_count()\n","    num_gpus = torch.cuda.device_count()\n","    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n","    return optimal_value\n","\n","print(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\n","MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n","MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Apex AMP Installed :: False\n"]}]},{"cell_type":"code","metadata":{"id":"t8HfXz6SHhE7","executionInfo":{"status":"ok","timestamp":1635271945851,"user_tz":-330,"elapsed":466,"user":{"displayName":"2020 11032","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15452284625747026929"}}},"source":["class Config:\n","    # model\n","    model_type = 'bert'\n","    model_name_or_path = \"/content/drive/Shareddrives/NLP/Tarang/xlm-roberta-large-squad2/model\"\n","    config_name = \"/content/drive/Shareddrives/NLP/Tarang/xlm-roberta-large-squad2/model\"\n","    fp16 = True if APEX_INSTALLED else False\n","    fp16_opt_level = \"O1\"\n","    gradient_accumulation_steps = 2\n","\n","    # tokenizer\n","    tokenizer_name = \"/content/drive/Shareddrives/NLP/Tarang/xlm-roberta-large-squad2/tok\"\n","    max_seq_length = 384\n","    doc_stride = 128\n","\n","    # train\n","    epochs = 1\n","    train_batch_size = 1\n","    eval_batch_size = 1\n","\n","    # optimizer\n","    optimizer_type = 'AdamW'\n","    learning_rate = 1.5e-5\n","    weight_decay = 1e-2\n","    epsilon = 1e-8\n","    max_grad_norm = 1.0\n","\n","    # scheduler\n","    decay_name = 'linear-warmup'\n","    warmup_ratio = 0.1\n","\n","    # logging\n","    logging_steps = 10\n","\n","    # evaluate\n","    output_dir = 'output'\n","    seed = 42"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jzv3krJSETEs","executionInfo":{"status":"ok","timestamp":1635271949576,"user_tz":-330,"elapsed":3330,"user":{"displayName":"2020 11032","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15452284625747026929"}}},"source":["train = pd.read_csv('/content/drive/Shareddrives/NLP/Dataset/train.csv')\n","test = pd.read_csv('/content/drive/Shareddrives/NLP/Dataset/test.csv')\n","external_mlqa = pd.read_csv('/content/drive/Shareddrives/NLP/Dataset/mlqa_hindi.csv')\n","external_xquad = pd.read_csv('/content/drive/Shareddrives/NLP/Dataset/xquad.csv')\n","external_train = pd.concat([external_mlqa, external_xquad])"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"wNv3o7qeFmaB","executionInfo":{"status":"ok","timestamp":1635271950293,"user_tz":-330,"elapsed":728,"user":{"displayName":"2020 11032","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15452284625747026929"}}},"source":["def create_folds(data, num_splits):\n","    data[\"kfold\"] = -1\n","    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=69)\n","    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n","        data.loc[v_, 'kfold'] = f\n","    return data\n","\n","train = create_folds(train, num_splits=5)\n","external_train[\"kfold\"] = -1\n","external_train['id'] = list(np.arange(1, len(external_train)+1))\n","train = pd.concat([train, external_train]).reset_index(drop=True)\n","\n","def convert_answers(row):\n","    return {'answer_start': [row[0]], 'text': [row[1]]}\n","\n","train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"dVmPKiJZGKGc","executionInfo":{"status":"ok","timestamp":1635271950294,"user_tz":-330,"elapsed":20,"user":{"displayName":"2020 11032","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15452284625747026929"}}},"source":["def prepare_train_features(args, example, tokenizer):\n","    example[\"question\"] = example[\"question\"].lstrip()\n","    tokenized_example = tokenizer(\n","        example[\"question\"],\n","        example[\"context\"],\n","        truncation=\"only_second\",\n","        max_length=args.max_seq_length,\n","        stride=args.doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n","    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n","\n","    features = []\n","    for i, offsets in enumerate(offset_mapping):\n","        feature = {}\n","\n","        input_ids = tokenized_example[\"input_ids\"][i]\n","        attention_mask = tokenized_example[\"attention_mask\"][i]\n","\n","        feature['input_ids'] = input_ids\n","        feature['attention_mask'] = attention_mask\n","        feature['offset_mapping'] = offsets\n","\n","        cls_index = input_ids.index(tokenizer.cls_token_id)\n","        sequence_ids = tokenized_example.sequence_ids(i)\n","\n","        sample_index = sample_mapping[i]\n","        answers = example[\"answers\"]\n","\n","        if len(answers[\"answer_start\"]) == 0:\n","            feature[\"start_position\"] = cls_index\n","            feature[\"end_position\"] = cls_index\n","        else:\n","            start_char = answers[\"answer_start\"][0]\n","            end_char = start_char + len(answers[\"text\"][0])\n","\n","            token_start_index = 0\n","            while sequence_ids[token_start_index] != 1:\n","                token_start_index += 1\n","\n","            token_end_index = len(input_ids) - 1\n","            while sequence_ids[token_end_index] != 1:\n","                token_end_index -= 1\n","\n","            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n","                feature[\"start_position\"] = cls_index\n","                feature[\"end_position\"] = cls_index\n","            else:\n","                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n","                    token_start_index += 1\n","                feature[\"start_position\"] = token_start_index - 1\n","                while offsets[token_end_index][1] >= end_char:\n","                    token_end_index -= 1\n","                feature[\"end_position\"] = token_end_index + 1\n","\n","        features.append(feature)\n","    return features"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"1X5iXyuyGRuH","executionInfo":{"status":"ok","timestamp":1635271950295,"user_tz":-330,"elapsed":19,"user":{"displayName":"2020 11032","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15452284625747026929"}}},"source":["class DatasetRetriever(Dataset):\n","    def __init__(self, features, mode='train'):\n","        super(DatasetRetriever, self).__init__()\n","        self.features = features\n","        self.mode = mode\n","        \n","    def __len__(self):\n","        return len(self.features)\n","    \n","    def __getitem__(self, item):   \n","        feature = self.features[item]\n","        if self.mode == 'train':\n","            return {\n","                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n","                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n","                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n","                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n","                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n","            }\n","        else:\n","            return {\n","                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n","                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n","                'offset_mapping':feature['offset_mapping'],\n","                'sequence_ids':feature['sequence_ids'],\n","                'id':feature['example_id'],\n","                'context': feature['context'],\n","                'question': feature['question']\n","            }"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"c83EghN4GXie","executionInfo":{"status":"ok","timestamp":1635271950295,"user_tz":-330,"elapsed":17,"user":{"displayName":"2020 11032","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15452284625747026929"}}},"source":["class Model(nn.Module):\n","    def __init__(self, modelname_or_path, config):\n","        super(Model, self).__init__()\n","        self.config = config\n","        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n","        self.linear_layer = nn.Linear(config.hidden_size, 64)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.qa_outputs = nn.Linear(64, 2)\n","        self._init_weights(self.qa_outputs)\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","\n","    def forward(\n","        self, \n","        input_ids, \n","        attention_mask=None, \n","    ):\n","        outputs = self.xlm_roberta(input_ids,attention_mask=attention_mask)\n","\n","        sequence_output = outputs[0]\n","        pooled_output = outputs[1]\n","        \n","        linear_output = self.linear_layer(sequence_output)\n","        linear_output = self.dropout(linear_output)\n","        qa_logits = self.qa_outputs(linear_output)\n","        \n","        start_logits, end_logits = qa_logits.split(1, dim=-1)\n","        start_logits = start_logits.squeeze(-1)\n","        end_logits = end_logits.squeeze(-1)\n","    \n","        return start_logits, end_logits"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"gKn4pHnvGc4H","executionInfo":{"status":"ok","timestamp":1635271950296,"user_tz":-330,"elapsed":16,"user":{"displayName":"2020 11032","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15452284625747026929"}}},"source":["def loss_fn(preds, labels):\n","    start_preds, end_preds = preds\n","    start_labels, end_labels = labels\n","    \n","    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n","    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n","    total_loss = (start_loss + end_loss) / 2\n","    return total_loss"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"qoV8_pn6GgdW","executionInfo":{"status":"ok","timestamp":1635271950297,"user_tz":-330,"elapsed":16,"user":{"displayName":"2020 11032","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15452284625747026929"}}},"source":["def get_optimizer_grouped_parameters(args, model):\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n","    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n","    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n","    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': args.weight_decay},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': args.weight_decay, 'lr': args.learning_rate/2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': args.weight_decay, 'lr': args.learning_rate},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': args.weight_decay, 'lr': args.learning_rate*2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': args.learning_rate/2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': args.learning_rate},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': args.learning_rate*2.6},\n","        {'params': [p for n, p in model.named_parameters() if args.model_type not in n], 'lr':args.learning_rate*20, \"weight_decay\": 0.0},\n","    ]\n","    return optimizer_grouped_parameters"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"xp_uEUijGrCv","executionInfo":{"status":"ok","timestamp":1635271950298,"user_tz":-330,"elapsed":16,"user":{"displayName":"2020 11032","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15452284625747026929"}}},"source":["class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","        self.max = 0\n","        self.min = 1e5\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","        if val > self.max:\n","            self.max = val\n","        if val < self.min:\n","            self.min = val"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"RKcK3xENGvQ4","executionInfo":{"status":"ok","timestamp":1635271950299,"user_tz":-330,"elapsed":16,"user":{"displayName":"2020 11032","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15452284625747026929"}}},"source":["def make_model(args):\n","    config = AutoConfig.from_pretrained(args.config_name)\n","    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n","    model = Model(args.model_name_or_path, config=config)\n","    return config, tokenizer, model\n","\n","def make_optimizer(args, model):\n","    optimizer_grouped_parameters = get_optimizer_grouped_parameters(args, model)\n","#     no_decay = [\"bias\", \"LayerNorm.weight\"]\n","#     optimizer_grouped_parameters = [\n","#         {\n","#             \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","#             \"weight_decay\": args.weight_decay,\n","#         },\n","#         {\n","#             \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","#             \"weight_decay\": 0.0,\n","#         },\n","#     ]\n","    if args.optimizer_type == \"AdamW\":\n","        optimizer = AdamW(\n","            optimizer_grouped_parameters,\n","            lr=args.learning_rate,\n","            eps=args.epsilon,\n","            correct_bias=True\n","        )\n","        return optimizer\n","\n","def make_scheduler(\n","    args, optimizer, \n","    num_warmup_steps, \n","    num_training_steps\n","):\n","    if args.decay_name == \"cosine-warmup\":\n","        scheduler = get_cosine_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps\n","        )\n","    else:\n","        scheduler = get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps\n","        )\n","    return scheduler    \n","\n","def make_loader(\n","    args, data, \n","    tokenizer, fold\n","):\n","    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n","    \n","    train_features, valid_features = [[] for _ in range(2)]\n","    for i, row in train_set.iterrows():\n","        train_features += prepare_train_features(args, row, tokenizer)\n","    for i, row in valid_set.iterrows():\n","        valid_features += prepare_train_features(args, row, tokenizer)\n","\n","    train_dataset = DatasetRetriever(train_features)\n","    valid_dataset = DatasetRetriever(valid_features)\n","    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n","    \n","    train_sampler = RandomSampler(train_dataset)\n","    valid_sampler = SequentialSampler(valid_dataset)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=args.train_batch_size,\n","        sampler=train_sampler,\n","        num_workers=optimal_num_of_loader_workers(),\n","        pin_memory=True,\n","        drop_last=False \n","    )\n","\n","    valid_dataloader = DataLoader(\n","        valid_dataset,\n","        batch_size=args.eval_batch_size, \n","        sampler=valid_sampler,\n","        num_workers=optimal_num_of_loader_workers(),\n","        pin_memory=True, \n","        drop_last=False\n","    )\n","\n","    return train_dataloader, valid_dataloader"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"YYXpJ2UaG4ft","executionInfo":{"status":"ok","timestamp":1635271950299,"user_tz":-330,"elapsed":15,"user":{"displayName":"2020 11032","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15452284625747026929"}}},"source":["class Trainer:\n","    def __init__(\n","        self, model, tokenizer, \n","        optimizer, scheduler\n","    ):\n","        self.model = model\n","        self.tokenizer = tokenizer\n","        self.optimizer = optimizer\n","        self.scheduler = scheduler\n","\n","    def train(\n","        self, args, \n","        train_dataloader, \n","        epoch, result_dict\n","    ):\n","        count = 0\n","        losses = AverageMeter()\n","        \n","        self.model.zero_grad()\n","        self.model.train()\n","        \n","        fix_all_seeds(args.seed)\n","        \n","        for batch_idx, batch_data in enumerate(train_dataloader):\n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                batch_data['input_ids'], batch_data['attention_mask'], \\\n","                    batch_data['start_position'], batch_data['end_position']\n","            \n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n","\n","            outputs_start, outputs_end = self.model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","            )\n","            \n","            loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n","            loss = loss / args.gradient_accumulation_steps\n","\n","            if args.fp16:\n","                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","            else:\n","                loss.backward()\n","\n","            count += input_ids.size(0)\n","            losses.update(loss.item(), input_ids.size(0))\n","\n","            # if args.fp16:\n","            #     torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), args.max_grad_norm)\n","            # else:\n","            #     torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.max_grad_norm)\n","\n","            if batch_idx % args.gradient_accumulation_steps == 0 or batch_idx == len(train_dataloader) - 1:\n","                self.optimizer.step()\n","                self.scheduler.step()\n","                self.optimizer.zero_grad()\n","\n","            if (batch_idx % args.logging_steps == 0) or (batch_idx+1)==len(train_dataloader):\n","                _s = str(len(str(len(train_dataloader.sampler))))\n","                ret = [\n","                    ('Epoch: {:0>2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_dataloader.sampler), 100 * count / len(train_dataloader.sampler)),\n","                    'Train Loss: {: >4.5f}'.format(losses.avg),\n","                ]\n","                print(', '.join(ret))\n","\n","        result_dict['train_loss'].append(losses.avg)\n","        return result_dict"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"c7lo_72cG88t","executionInfo":{"status":"ok","timestamp":1635271950300,"user_tz":-330,"elapsed":15,"user":{"displayName":"2020 11032","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15452284625747026929"}}},"source":["class Evaluator:\n","    def __init__(self, model):\n","        self.model = model\n","    \n","    def save(self, result, output_dir):\n","        with open(f'{output_dir}/result_dict.json', 'w') as f:\n","            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n","\n","    def evaluate(self, valid_dataloader, epoch, result_dict):\n","        losses = AverageMeter()\n","        for batch_idx, batch_data in enumerate(valid_dataloader):\n","            self.model = self.model.eval()\n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                batch_data['input_ids'], batch_data['attention_mask'], \\\n","                    batch_data['start_position'], batch_data['end_position']\n","            \n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n","            \n","            with torch.no_grad():            \n","                outputs_start, outputs_end = self.model(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                )\n","                \n","                loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n","                losses.update(loss.item(), input_ids.size(0))\n","                \n","        print('----Validation Results Summary----')\n","        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n","        result_dict['val_loss'].append(losses.avg)        \n","        return result_dict"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wv14GLumHBTO","executionInfo":{"status":"ok","timestamp":1635271950301,"user_tz":-330,"elapsed":16,"user":{"displayName":"2020 11032","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15452284625747026929"}}},"source":["def init_training(args, data, fold):\n","    fix_all_seeds(args.seed)\n","    \n","    if not os.path.exists(args.output_dir):\n","        os.makedirs(args.output_dir)\n","    \n","    # model\n","    model_config, tokenizer, model = make_model(args)\n","    if torch.cuda.device_count() >= 1:\n","        print('Model pushed to {} GPU(s), type {}.'.format(\n","            torch.cuda.device_count(), \n","            torch.cuda.get_device_name(0))\n","        )\n","        model = model.cuda() \n","    else:\n","        raise ValueError('CPU training is not supported')\n","    \n","    # data loaders\n","    train_dataloader, valid_dataloader = make_loader(args, data, tokenizer, fold)\n","\n","    # optimizer\n","    optimizer = make_optimizer(args, model)\n","\n","    # scheduler\n","    num_training_steps = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps) * args.epochs\n","    if args.warmup_ratio > 0:\n","        num_warmup_steps = int(args.warmup_ratio * num_training_steps)\n","    else:\n","        num_warmup_steps = 0\n","    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n","    scheduler = make_scheduler(args, optimizer, num_warmup_steps, num_training_steps)\n","\n","    # mixed precision training with NVIDIA Apex\n","    if args.fp16:\n","        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n","    \n","    result_dict = {\n","        'epoch':[], \n","        'train_loss': [], \n","        'val_loss' : [], \n","        'best_val_loss': np.inf\n","    }\n","\n","    return (\n","        model, model_config, tokenizer, optimizer, scheduler, \n","        train_dataloader, valid_dataloader, result_dict\n","    )"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"XUjSnwFpHKAi","executionInfo":{"status":"ok","timestamp":1635271950301,"user_tz":-330,"elapsed":15,"user":{"displayName":"2020 11032","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15452284625747026929"}}},"source":["def run(data, fold):\n","    args = Config()\n","    model, model_config, tokenizer, optimizer, scheduler, train_dataloader, \\\n","        valid_dataloader, result_dict = init_training(args, data, fold)\n","    \n","    trainer = Trainer(model, tokenizer, optimizer, scheduler)\n","    evaluator = Evaluator(model)\n","\n","    train_time_list = []\n","    valid_time_list = []\n","\n","    for epoch in range(args.epochs):\n","        result_dict['epoch'].append(epoch)\n","\n","        # Train\n","        torch.cuda.synchronize()\n","        tic1 = time.time()\n","        result_dict = trainer.train(\n","            args, train_dataloader, \n","            epoch, result_dict\n","        )\n","        torch.cuda.synchronize()\n","        tic2 = time.time() \n","        train_time_list.append(tic2 - tic1)\n","        \n","        # Evaluate\n","        torch.cuda.synchronize()\n","        tic3 = time.time()\n","        result_dict = evaluator.evaluate(\n","            valid_dataloader, epoch, result_dict\n","        )\n","        torch.cuda.synchronize()\n","        tic4 = time.time() \n","        valid_time_list.append(tic4 - tic3)\n","            \n","        output_dir = os.path.join(args.output_dir, f\"checkpoint-fold-{fold}\")\n","        if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n","            print(\"{} Epoch, Best epoch was updated! Valid Loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n","            result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]        \n","            \n","            os.makedirs(output_dir, exist_ok=True)\n","            torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n","            model_config.save_pretrained(output_dir)\n","            tokenizer.save_pretrained(output_dir)\n","            print(f\"Saving model checkpoint to {output_dir}.\")\n","            \n","        print()\n","\n","    evaluator.save(result_dict, output_dir)\n","    \n","    print(f\"Total Training Time: {np.sum(train_time_list)}secs, Average Training Time per Epoch: {np.mean(train_time_list)}secs.\")\n","    print(f\"Total Validation Time: {np.sum(valid_time_list)}secs, Average Validation Time per Epoch: {np.mean(valid_time_list)}secs.\")\n","    \n","    torch.cuda.empty_cache()\n","    del trainer, evaluator\n","    del model, model_config, tokenizer\n","    del optimizer, scheduler\n","    del train_dataloader, valid_dataloader, result_dict\n","    gc.collect()\n"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bduls9AyHO8p","outputId":"e5bd4de4-cb23-40ca-b9d6-fc7a24a6ed83"},"source":["for fold in range(3,4):\n","    print();print()\n","    print('-'*50)\n","    print(f'FOLD: {fold}')\n","    print('-'*50)\n","    run(train, fold)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","--------------------------------------------------\n","FOLD: 3\n","--------------------------------------------------\n","Model pushed to 1 GPU(s), type Tesla K80.\n","Num examples Train= 20101, Num examples Valid=2968\n","Total Training Steps: 10051, Total Warmup Steps: 1005\n","Epoch: 00 [    1/20101 (  0%)], Train Loss: 2.99301\n","Epoch: 00 [   11/20101 (  0%)], Train Loss: 2.96260\n","Epoch: 00 [   21/20101 (  0%)], Train Loss: 2.96123\n","Epoch: 00 [   31/20101 (  0%)], Train Loss: 2.95773\n","Epoch: 00 [   41/20101 (  0%)], Train Loss: 2.95431\n","Epoch: 00 [   51/20101 (  0%)], Train Loss: 2.95065\n","Epoch: 00 [   61/20101 (  0%)], Train Loss: 2.94557\n","Epoch: 00 [   71/20101 (  0%)], Train Loss: 2.93825\n","Epoch: 00 [   81/20101 (  0%)], Train Loss: 2.93041\n","Epoch: 00 [   91/20101 (  0%)], Train Loss: 2.92411\n","Epoch: 00 [  101/20101 (  1%)], Train Loss: 2.91608\n","Epoch: 00 [  111/20101 (  1%)], Train Loss: 2.90775\n","Epoch: 00 [  121/20101 (  1%)], Train Loss: 2.89691\n","Epoch: 00 [  131/20101 (  1%)], Train Loss: 2.88623\n","Epoch: 00 [  141/20101 (  1%)], Train Loss: 2.87293\n","Epoch: 00 [  151/20101 (  1%)], Train Loss: 2.85621\n","Epoch: 00 [  161/20101 (  1%)], Train Loss: 2.83938\n","Epoch: 00 [  171/20101 (  1%)], Train Loss: 2.82110\n","Epoch: 00 [  181/20101 (  1%)], Train Loss: 2.80007\n","Epoch: 00 [  191/20101 (  1%)], Train Loss: 2.77037\n","Epoch: 00 [  201/20101 (  1%)], Train Loss: 2.74258\n","Epoch: 00 [  211/20101 (  1%)], Train Loss: 2.70909\n","Epoch: 00 [  221/20101 (  1%)], Train Loss: 2.68482\n","Epoch: 00 [  231/20101 (  1%)], Train Loss: 2.64854\n","Epoch: 00 [  241/20101 (  1%)], Train Loss: 2.60695\n","Epoch: 00 [  251/20101 (  1%)], Train Loss: 2.57123\n","Epoch: 00 [  261/20101 (  1%)], Train Loss: 2.52563\n","Epoch: 00 [  271/20101 (  1%)], Train Loss: 2.47727\n","Epoch: 00 [  281/20101 (  1%)], Train Loss: 2.43735\n","Epoch: 00 [  291/20101 (  1%)], Train Loss: 2.39652\n","Epoch: 00 [  301/20101 (  1%)], Train Loss: 2.34828\n","Epoch: 00 [  311/20101 (  2%)], Train Loss: 2.30326\n","Epoch: 00 [  321/20101 (  2%)], Train Loss: 2.25202\n","Epoch: 00 [  331/20101 (  2%)], Train Loss: 2.21592\n","Epoch: 00 [  341/20101 (  2%)], Train Loss: 2.17135\n","Epoch: 00 [  351/20101 (  2%)], Train Loss: 2.11934\n","Epoch: 00 [  361/20101 (  2%)], Train Loss: 2.08479\n","Epoch: 00 [  371/20101 (  2%)], Train Loss: 2.03189\n","Epoch: 00 [  381/20101 (  2%)], Train Loss: 1.99756\n","Epoch: 00 [  391/20101 (  2%)], Train Loss: 1.96953\n","Epoch: 00 [  401/20101 (  2%)], Train Loss: 1.92482\n","Epoch: 00 [  411/20101 (  2%)], Train Loss: 1.88899\n","Epoch: 00 [  421/20101 (  2%)], Train Loss: 1.85212\n","Epoch: 00 [  431/20101 (  2%)], Train Loss: 1.81546\n","Epoch: 00 [  441/20101 (  2%)], Train Loss: 1.78489\n","Epoch: 00 [  451/20101 (  2%)], Train Loss: 1.76169\n","Epoch: 00 [  461/20101 (  2%)], Train Loss: 1.73001\n","Epoch: 00 [  471/20101 (  2%)], Train Loss: 1.71022\n","Epoch: 00 [  481/20101 (  2%)], Train Loss: 1.67992\n","Epoch: 00 [  491/20101 (  2%)], Train Loss: 1.65665\n","Epoch: 00 [  501/20101 (  2%)], Train Loss: 1.62747\n","Epoch: 00 [  511/20101 (  3%)], Train Loss: 1.59750\n","Epoch: 00 [  521/20101 (  3%)], Train Loss: 1.57608\n","Epoch: 00 [  531/20101 (  3%)], Train Loss: 1.55494\n","Epoch: 00 [  541/20101 (  3%)], Train Loss: 1.53290\n","Epoch: 00 [  551/20101 (  3%)], Train Loss: 1.51444\n","Epoch: 00 [  561/20101 (  3%)], Train Loss: 1.49989\n","Epoch: 00 [  571/20101 (  3%)], Train Loss: 1.48639\n","Epoch: 00 [  581/20101 (  3%)], Train Loss: 1.46730\n","Epoch: 00 [  591/20101 (  3%)], Train Loss: 1.45075\n","Epoch: 00 [  601/20101 (  3%)], Train Loss: 1.44832\n","Epoch: 00 [  611/20101 (  3%)], Train Loss: 1.43149\n","Epoch: 00 [  621/20101 (  3%)], Train Loss: 1.41981\n","Epoch: 00 [  631/20101 (  3%)], Train Loss: 1.40484\n","Epoch: 00 [  641/20101 (  3%)], Train Loss: 1.38781\n","Epoch: 00 [  651/20101 (  3%)], Train Loss: 1.37079\n","Epoch: 00 [  661/20101 (  3%)], Train Loss: 1.36915\n","Epoch: 00 [  671/20101 (  3%)], Train Loss: 1.35720\n","Epoch: 00 [  681/20101 (  3%)], Train Loss: 1.34748\n","Epoch: 00 [  691/20101 (  3%)], Train Loss: 1.34045\n","Epoch: 00 [  701/20101 (  3%)], Train Loss: 1.33532\n","Epoch: 00 [  711/20101 (  4%)], Train Loss: 1.32302\n","Epoch: 00 [  721/20101 (  4%)], Train Loss: 1.30962\n","Epoch: 00 [  731/20101 (  4%)], Train Loss: 1.29621\n","Epoch: 00 [  741/20101 (  4%)], Train Loss: 1.28773\n","Epoch: 00 [  751/20101 (  4%)], Train Loss: 1.27760\n","Epoch: 00 [  761/20101 (  4%)], Train Loss: 1.27016\n","Epoch: 00 [  771/20101 (  4%)], Train Loss: 1.25582\n","Epoch: 00 [  781/20101 (  4%)], Train Loss: 1.24620\n","Epoch: 00 [  791/20101 (  4%)], Train Loss: 1.23724\n","Epoch: 00 [  801/20101 (  4%)], Train Loss: 1.23139\n","Epoch: 00 [  811/20101 (  4%)], Train Loss: 1.22170\n","Epoch: 00 [  821/20101 (  4%)], Train Loss: 1.21360\n","Epoch: 00 [  831/20101 (  4%)], Train Loss: 1.20419\n","Epoch: 00 [  841/20101 (  4%)], Train Loss: 1.19359\n","Epoch: 00 [  851/20101 (  4%)], Train Loss: 1.18718\n","Epoch: 00 [  861/20101 (  4%)], Train Loss: 1.17399\n","Epoch: 00 [  871/20101 (  4%)], Train Loss: 1.16426\n","Epoch: 00 [  881/20101 (  4%)], Train Loss: 1.15235\n","Epoch: 00 [  891/20101 (  4%)], Train Loss: 1.14322\n","Epoch: 00 [  901/20101 (  4%)], Train Loss: 1.13523\n","Epoch: 00 [  911/20101 (  5%)], Train Loss: 1.12892\n","Epoch: 00 [  921/20101 (  5%)], Train Loss: 1.12038\n","Epoch: 00 [  931/20101 (  5%)], Train Loss: 1.11692\n","Epoch: 00 [  941/20101 (  5%)], Train Loss: 1.11117\n","Epoch: 00 [  951/20101 (  5%)], Train Loss: 1.10252\n","Epoch: 00 [  961/20101 (  5%)], Train Loss: 1.09350\n","Epoch: 00 [  971/20101 (  5%)], Train Loss: 1.08858\n","Epoch: 00 [  981/20101 (  5%)], Train Loss: 1.07906\n","Epoch: 00 [  991/20101 (  5%)], Train Loss: 1.07180\n","Epoch: 00 [ 1001/20101 (  5%)], Train Loss: 1.06223\n","Epoch: 00 [ 1011/20101 (  5%)], Train Loss: 1.05406\n","Epoch: 00 [ 1021/20101 (  5%)], Train Loss: 1.04802\n","Epoch: 00 [ 1031/20101 (  5%)], Train Loss: 1.03919\n","Epoch: 00 [ 1041/20101 (  5%)], Train Loss: 1.03471\n","Epoch: 00 [ 1051/20101 (  5%)], Train Loss: 1.02656\n","Epoch: 00 [ 1061/20101 (  5%)], Train Loss: 1.01845\n","Epoch: 00 [ 1071/20101 (  5%)], Train Loss: 1.01087\n","Epoch: 00 [ 1081/20101 (  5%)], Train Loss: 1.00305\n","Epoch: 00 [ 1091/20101 (  5%)], Train Loss: 0.99801\n","Epoch: 00 [ 1101/20101 (  5%)], Train Loss: 0.99153\n","Epoch: 00 [ 1111/20101 (  6%)], Train Loss: 0.98666\n","Epoch: 00 [ 1121/20101 (  6%)], Train Loss: 0.98032\n","Epoch: 00 [ 1131/20101 (  6%)], Train Loss: 0.97741\n","Epoch: 00 [ 1141/20101 (  6%)], Train Loss: 0.97484\n","Epoch: 00 [ 1151/20101 (  6%)], Train Loss: 0.97485\n","Epoch: 00 [ 1161/20101 (  6%)], Train Loss: 0.97237\n","Epoch: 00 [ 1171/20101 (  6%)], Train Loss: 0.96516\n","Epoch: 00 [ 1181/20101 (  6%)], Train Loss: 0.95858\n","Epoch: 00 [ 1191/20101 (  6%)], Train Loss: 0.95466\n","Epoch: 00 [ 1201/20101 (  6%)], Train Loss: 0.94983\n","Epoch: 00 [ 1211/20101 (  6%)], Train Loss: 0.94356\n","Epoch: 00 [ 1221/20101 (  6%)], Train Loss: 0.94032\n","Epoch: 00 [ 1231/20101 (  6%)], Train Loss: 0.93438\n","Epoch: 00 [ 1241/20101 (  6%)], Train Loss: 0.93183\n","Epoch: 00 [ 1251/20101 (  6%)], Train Loss: 0.92569\n","Epoch: 00 [ 1261/20101 (  6%)], Train Loss: 0.92372\n","Epoch: 00 [ 1271/20101 (  6%)], Train Loss: 0.91813\n","Epoch: 00 [ 1281/20101 (  6%)], Train Loss: 0.91735\n","Epoch: 00 [ 1291/20101 (  6%)], Train Loss: 0.91323\n","Epoch: 00 [ 1301/20101 (  6%)], Train Loss: 0.91064\n","Epoch: 00 [ 1311/20101 (  7%)], Train Loss: 0.90482\n","Epoch: 00 [ 1321/20101 (  7%)], Train Loss: 0.90025\n","Epoch: 00 [ 1331/20101 (  7%)], Train Loss: 0.90055\n","Epoch: 00 [ 1341/20101 (  7%)], Train Loss: 0.89830\n","Epoch: 00 [ 1351/20101 (  7%)], Train Loss: 0.89565\n","Epoch: 00 [ 1361/20101 (  7%)], Train Loss: 0.89120\n","Epoch: 00 [ 1371/20101 (  7%)], Train Loss: 0.88676\n","Epoch: 00 [ 1381/20101 (  7%)], Train Loss: 0.88221\n","Epoch: 00 [ 1391/20101 (  7%)], Train Loss: 0.87862\n","Epoch: 00 [ 1401/20101 (  7%)], Train Loss: 0.87322\n","Epoch: 00 [ 1411/20101 (  7%)], Train Loss: 0.86830\n","Epoch: 00 [ 1421/20101 (  7%)], Train Loss: 0.86497\n","Epoch: 00 [ 1431/20101 (  7%)], Train Loss: 0.86153\n","Epoch: 00 [ 1441/20101 (  7%)], Train Loss: 0.85982\n","Epoch: 00 [ 1451/20101 (  7%)], Train Loss: 0.86055\n","Epoch: 00 [ 1461/20101 (  7%)], Train Loss: 0.85601\n","Epoch: 00 [ 1471/20101 (  7%)], Train Loss: 0.85350\n","Epoch: 00 [ 1481/20101 (  7%)], Train Loss: 0.84952\n","Epoch: 00 [ 1491/20101 (  7%)], Train Loss: 0.84444\n","Epoch: 00 [ 1501/20101 (  7%)], Train Loss: 0.84234\n","Epoch: 00 [ 1511/20101 (  8%)], Train Loss: 0.83857\n","Epoch: 00 [ 1521/20101 (  8%)], Train Loss: 0.83836\n","Epoch: 00 [ 1531/20101 (  8%)], Train Loss: 0.83655\n","Epoch: 00 [ 1541/20101 (  8%)], Train Loss: 0.83592\n","Epoch: 00 [ 1551/20101 (  8%)], Train Loss: 0.83490\n","Epoch: 00 [ 1561/20101 (  8%)], Train Loss: 0.83139\n","Epoch: 00 [ 1571/20101 (  8%)], Train Loss: 0.82833\n","Epoch: 00 [ 1581/20101 (  8%)], Train Loss: 0.82802\n","Epoch: 00 [ 1591/20101 (  8%)], Train Loss: 0.82516\n","Epoch: 00 [ 1601/20101 (  8%)], Train Loss: 0.82083\n","Epoch: 00 [ 1611/20101 (  8%)], Train Loss: 0.82232\n","Epoch: 00 [ 1621/20101 (  8%)], Train Loss: 0.82062\n","Epoch: 00 [ 1631/20101 (  8%)], Train Loss: 0.82052\n","Epoch: 00 [ 1641/20101 (  8%)], Train Loss: 0.81810\n","Epoch: 00 [ 1651/20101 (  8%)], Train Loss: 0.81635\n","Epoch: 00 [ 1661/20101 (  8%)], Train Loss: 0.81758\n","Epoch: 00 [ 1671/20101 (  8%)], Train Loss: 0.81722\n","Epoch: 00 [ 1681/20101 (  8%)], Train Loss: 0.81515\n","Epoch: 00 [ 1691/20101 (  8%)], Train Loss: 0.81411\n","Epoch: 00 [ 1701/20101 (  8%)], Train Loss: 0.81246\n","Epoch: 00 [ 1711/20101 (  9%)], Train Loss: 0.80937\n","Epoch: 00 [ 1721/20101 (  9%)], Train Loss: 0.80785\n","Epoch: 00 [ 1731/20101 (  9%)], Train Loss: 0.80662\n","Epoch: 00 [ 1741/20101 (  9%)], Train Loss: 0.80596\n","Epoch: 00 [ 1751/20101 (  9%)], Train Loss: 0.80454\n","Epoch: 00 [ 1761/20101 (  9%)], Train Loss: 0.80357\n","Epoch: 00 [ 1771/20101 (  9%)], Train Loss: 0.80402\n","Epoch: 00 [ 1781/20101 (  9%)], Train Loss: 0.80215\n","Epoch: 00 [ 1791/20101 (  9%)], Train Loss: 0.80025\n","Epoch: 00 [ 1801/20101 (  9%)], Train Loss: 0.79798\n","Epoch: 00 [ 1811/20101 (  9%)], Train Loss: 0.79364\n","Epoch: 00 [ 1821/20101 (  9%)], Train Loss: 0.79210\n","Epoch: 00 [ 1831/20101 (  9%)], Train Loss: 0.79205\n","Epoch: 00 [ 1841/20101 (  9%)], Train Loss: 0.79369\n","Epoch: 00 [ 1851/20101 (  9%)], Train Loss: 0.79328\n","Epoch: 00 [ 1861/20101 (  9%)], Train Loss: 0.79097\n","Epoch: 00 [ 1871/20101 (  9%)], Train Loss: 0.78760\n","Epoch: 00 [ 1881/20101 (  9%)], Train Loss: 0.78477\n","Epoch: 00 [ 1891/20101 (  9%)], Train Loss: 0.78262\n","Epoch: 00 [ 1901/20101 (  9%)], Train Loss: 0.78257\n","Epoch: 00 [ 1911/20101 ( 10%)], Train Loss: 0.78089\n","Epoch: 00 [ 1921/20101 ( 10%)], Train Loss: 0.78528\n","Epoch: 00 [ 1931/20101 ( 10%)], Train Loss: 0.78518\n","Epoch: 00 [ 1941/20101 ( 10%)], Train Loss: 0.78346\n","Epoch: 00 [ 1951/20101 ( 10%)], Train Loss: 0.78118\n","Epoch: 00 [ 1961/20101 ( 10%)], Train Loss: 0.77831\n","Epoch: 00 [ 1971/20101 ( 10%)], Train Loss: 0.77563\n","Epoch: 00 [ 1981/20101 ( 10%)], Train Loss: 0.77479\n","Epoch: 00 [ 1991/20101 ( 10%)], Train Loss: 0.77292\n","Epoch: 00 [ 2001/20101 ( 10%)], Train Loss: 0.77338\n","Epoch: 00 [ 2011/20101 ( 10%)], Train Loss: 0.77134\n","Epoch: 00 [ 2021/20101 ( 10%)], Train Loss: 0.77063\n","Epoch: 00 [ 2031/20101 ( 10%)], Train Loss: 0.76820\n","Epoch: 00 [ 2041/20101 ( 10%)], Train Loss: 0.76995\n","Epoch: 00 [ 2051/20101 ( 10%)], Train Loss: 0.77100\n","Epoch: 00 [ 2061/20101 ( 10%)], Train Loss: 0.76957\n","Epoch: 00 [ 2071/20101 ( 10%)], Train Loss: 0.76938\n","Epoch: 00 [ 2081/20101 ( 10%)], Train Loss: 0.76759\n","Epoch: 00 [ 2091/20101 ( 10%)], Train Loss: 0.76492\n","Epoch: 00 [ 2101/20101 ( 10%)], Train Loss: 0.76317\n","Epoch: 00 [ 2111/20101 ( 11%)], Train Loss: 0.76135\n","Epoch: 00 [ 2121/20101 ( 11%)], Train Loss: 0.75974\n","Epoch: 00 [ 2131/20101 ( 11%)], Train Loss: 0.75812\n","Epoch: 00 [ 2141/20101 ( 11%)], Train Loss: 0.75767\n","Epoch: 00 [ 2151/20101 ( 11%)], Train Loss: 0.75602\n","Epoch: 00 [ 2161/20101 ( 11%)], Train Loss: 0.75357\n","Epoch: 00 [ 2171/20101 ( 11%)], Train Loss: 0.75251\n","Epoch: 00 [ 2181/20101 ( 11%)], Train Loss: 0.74936\n","Epoch: 00 [ 2191/20101 ( 11%)], Train Loss: 0.74981\n","Epoch: 00 [ 2201/20101 ( 11%)], Train Loss: 0.74757\n","Epoch: 00 [ 2211/20101 ( 11%)], Train Loss: 0.74826\n","Epoch: 00 [ 2221/20101 ( 11%)], Train Loss: 0.74948\n","Epoch: 00 [ 2231/20101 ( 11%)], Train Loss: 0.74725\n","Epoch: 00 [ 2241/20101 ( 11%)], Train Loss: 0.74680\n","Epoch: 00 [ 2251/20101 ( 11%)], Train Loss: 0.74452\n","Epoch: 00 [ 2261/20101 ( 11%)], Train Loss: 0.74392\n","Epoch: 00 [ 2271/20101 ( 11%)], Train Loss: 0.74239\n","Epoch: 00 [ 2281/20101 ( 11%)], Train Loss: 0.74159\n","Epoch: 00 [ 2291/20101 ( 11%)], Train Loss: 0.74292\n","Epoch: 00 [ 2301/20101 ( 11%)], Train Loss: 0.74241\n","Epoch: 00 [ 2311/20101 ( 11%)], Train Loss: 0.74123\n","Epoch: 00 [ 2321/20101 ( 12%)], Train Loss: 0.73857\n","Epoch: 00 [ 2331/20101 ( 12%)], Train Loss: 0.73778\n","Epoch: 00 [ 2341/20101 ( 12%)], Train Loss: 0.73605\n","Epoch: 00 [ 2351/20101 ( 12%)], Train Loss: 0.73507\n","Epoch: 00 [ 2361/20101 ( 12%)], Train Loss: 0.73380\n","Epoch: 00 [ 2371/20101 ( 12%)], Train Loss: 0.73208\n","Epoch: 00 [ 2381/20101 ( 12%)], Train Loss: 0.72983\n","Epoch: 00 [ 2391/20101 ( 12%)], Train Loss: 0.72871\n","Epoch: 00 [ 2401/20101 ( 12%)], Train Loss: 0.72962\n","Epoch: 00 [ 2411/20101 ( 12%)], Train Loss: 0.72947\n","Epoch: 00 [ 2421/20101 ( 12%)], Train Loss: 0.73083\n","Epoch: 00 [ 2431/20101 ( 12%)], Train Loss: 0.73134\n","Epoch: 00 [ 2441/20101 ( 12%)], Train Loss: 0.72995\n","Epoch: 00 [ 2451/20101 ( 12%)], Train Loss: 0.72798\n","Epoch: 00 [ 2461/20101 ( 12%)], Train Loss: 0.72690\n","Epoch: 00 [ 2471/20101 ( 12%)], Train Loss: 0.72459\n","Epoch: 00 [ 2481/20101 ( 12%)], Train Loss: 0.72431\n","Epoch: 00 [ 2491/20101 ( 12%)], Train Loss: 0.72386\n","Epoch: 00 [ 2501/20101 ( 12%)], Train Loss: 0.72158\n","Epoch: 00 [ 2511/20101 ( 12%)], Train Loss: 0.72246\n","Epoch: 00 [ 2521/20101 ( 13%)], Train Loss: 0.72060\n","Epoch: 00 [ 2531/20101 ( 13%)], Train Loss: 0.72404\n","Epoch: 00 [ 2541/20101 ( 13%)], Train Loss: 0.72343\n","Epoch: 00 [ 2551/20101 ( 13%)], Train Loss: 0.72201\n","Epoch: 00 [ 2561/20101 ( 13%)], Train Loss: 0.71993\n","Epoch: 00 [ 2571/20101 ( 13%)], Train Loss: 0.71998\n","Epoch: 00 [ 2581/20101 ( 13%)], Train Loss: 0.71784\n","Epoch: 00 [ 2591/20101 ( 13%)], Train Loss: 0.71770\n","Epoch: 00 [ 2601/20101 ( 13%)], Train Loss: 0.71653\n","Epoch: 00 [ 2611/20101 ( 13%)], Train Loss: 0.71594\n","Epoch: 00 [ 2621/20101 ( 13%)], Train Loss: 0.71372\n","Epoch: 00 [ 2631/20101 ( 13%)], Train Loss: 0.71326\n","Epoch: 00 [ 2641/20101 ( 13%)], Train Loss: 0.71456\n","Epoch: 00 [ 2651/20101 ( 13%)], Train Loss: 0.71507\n","Epoch: 00 [ 2661/20101 ( 13%)], Train Loss: 0.71460\n","Epoch: 00 [ 2671/20101 ( 13%)], Train Loss: 0.71393\n","Epoch: 00 [ 2681/20101 ( 13%)], Train Loss: 0.71381\n","Epoch: 00 [ 2691/20101 ( 13%)], Train Loss: 0.71494\n","Epoch: 00 [ 2701/20101 ( 13%)], Train Loss: 0.71418\n","Epoch: 00 [ 2711/20101 ( 13%)], Train Loss: 0.71424\n","Epoch: 00 [ 2721/20101 ( 14%)], Train Loss: 0.71515\n","Epoch: 00 [ 2731/20101 ( 14%)], Train Loss: 0.71473\n","Epoch: 00 [ 2741/20101 ( 14%)], Train Loss: 0.71304\n","Epoch: 00 [ 2751/20101 ( 14%)], Train Loss: 0.71377\n","Epoch: 00 [ 2761/20101 ( 14%)], Train Loss: 0.71297\n","Epoch: 00 [ 2771/20101 ( 14%)], Train Loss: 0.71060\n","Epoch: 00 [ 2781/20101 ( 14%)], Train Loss: 0.70943\n","Epoch: 00 [ 2791/20101 ( 14%)], Train Loss: 0.70755\n","Epoch: 00 [ 2801/20101 ( 14%)], Train Loss: 0.70686\n","Epoch: 00 [ 2811/20101 ( 14%)], Train Loss: 0.70552\n","Epoch: 00 [ 2821/20101 ( 14%)], Train Loss: 0.70572\n","Epoch: 00 [ 2831/20101 ( 14%)], Train Loss: 0.70555\n","Epoch: 00 [ 2841/20101 ( 14%)], Train Loss: 0.70553\n","Epoch: 00 [ 2851/20101 ( 14%)], Train Loss: 0.70445\n","Epoch: 00 [ 2861/20101 ( 14%)], Train Loss: 0.70533\n","Epoch: 00 [ 2871/20101 ( 14%)], Train Loss: 0.70495\n","Epoch: 00 [ 2881/20101 ( 14%)], Train Loss: 0.70474\n","Epoch: 00 [ 2891/20101 ( 14%)], Train Loss: 0.70445\n","Epoch: 00 [ 2901/20101 ( 14%)], Train Loss: 0.70349\n","Epoch: 00 [ 2911/20101 ( 14%)], Train Loss: 0.70174\n","Epoch: 00 [ 2921/20101 ( 15%)], Train Loss: 0.70296\n","Epoch: 00 [ 2931/20101 ( 15%)], Train Loss: 0.70356\n","Epoch: 00 [ 2941/20101 ( 15%)], Train Loss: 0.70208\n","Epoch: 00 [ 2951/20101 ( 15%)], Train Loss: 0.70055\n","Epoch: 00 [ 2961/20101 ( 15%)], Train Loss: 0.69951\n","Epoch: 00 [ 2971/20101 ( 15%)], Train Loss: 0.69785\n","Epoch: 00 [ 2981/20101 ( 15%)], Train Loss: 0.69710\n","Epoch: 00 [ 2991/20101 ( 15%)], Train Loss: 0.69870\n","Epoch: 00 [ 3001/20101 ( 15%)], Train Loss: 0.69803\n","Epoch: 00 [ 3011/20101 ( 15%)], Train Loss: 0.69839\n","Epoch: 00 [ 3021/20101 ( 15%)], Train Loss: 0.69718\n","Epoch: 00 [ 3031/20101 ( 15%)], Train Loss: 0.69713\n","Epoch: 00 [ 3041/20101 ( 15%)], Train Loss: 0.69560\n","Epoch: 00 [ 3051/20101 ( 15%)], Train Loss: 0.69510\n","Epoch: 00 [ 3061/20101 ( 15%)], Train Loss: 0.69463\n","Epoch: 00 [ 3071/20101 ( 15%)], Train Loss: 0.69370\n","Epoch: 00 [ 3081/20101 ( 15%)], Train Loss: 0.69302\n","Epoch: 00 [ 3091/20101 ( 15%)], Train Loss: 0.69229\n","Epoch: 00 [ 3101/20101 ( 15%)], Train Loss: 0.69566\n","Epoch: 00 [ 3111/20101 ( 15%)], Train Loss: 0.69550\n","Epoch: 00 [ 3121/20101 ( 16%)], Train Loss: 0.69683\n","Epoch: 00 [ 3131/20101 ( 16%)], Train Loss: 0.69669\n","Epoch: 00 [ 3141/20101 ( 16%)], Train Loss: 0.69684\n","Epoch: 00 [ 3151/20101 ( 16%)], Train Loss: 0.69531\n","Epoch: 00 [ 3161/20101 ( 16%)], Train Loss: 0.69375\n","Epoch: 00 [ 3171/20101 ( 16%)], Train Loss: 0.69322\n","Epoch: 00 [ 3181/20101 ( 16%)], Train Loss: 0.69216\n","Epoch: 00 [ 3191/20101 ( 16%)], Train Loss: 0.69352\n","Epoch: 00 [ 3201/20101 ( 16%)], Train Loss: 0.69395\n","Epoch: 00 [ 3211/20101 ( 16%)], Train Loss: 0.69384\n","Epoch: 00 [ 3221/20101 ( 16%)], Train Loss: 0.69363\n","Epoch: 00 [ 3231/20101 ( 16%)], Train Loss: 0.69263\n","Epoch: 00 [ 3241/20101 ( 16%)], Train Loss: 0.69191\n","Epoch: 00 [ 3251/20101 ( 16%)], Train Loss: 0.69183\n","Epoch: 00 [ 3261/20101 ( 16%)], Train Loss: 0.69072\n","Epoch: 00 [ 3271/20101 ( 16%)], Train Loss: 0.68960\n","Epoch: 00 [ 3281/20101 ( 16%)], Train Loss: 0.68848\n","Epoch: 00 [ 3291/20101 ( 16%)], Train Loss: 0.68808\n","Epoch: 00 [ 3301/20101 ( 16%)], Train Loss: 0.68932\n","Epoch: 00 [ 3311/20101 ( 16%)], Train Loss: 0.68821\n","Epoch: 00 [ 3321/20101 ( 17%)], Train Loss: 0.68740\n","Epoch: 00 [ 3331/20101 ( 17%)], Train Loss: 0.68679\n","Epoch: 00 [ 3341/20101 ( 17%)], Train Loss: 0.68683\n","Epoch: 00 [ 3351/20101 ( 17%)], Train Loss: 0.68560\n","Epoch: 00 [ 3361/20101 ( 17%)], Train Loss: 0.68430\n","Epoch: 00 [ 3371/20101 ( 17%)], Train Loss: 0.68335\n","Epoch: 00 [ 3381/20101 ( 17%)], Train Loss: 0.68185\n","Epoch: 00 [ 3391/20101 ( 17%)], Train Loss: 0.68225\n","Epoch: 00 [ 3401/20101 ( 17%)], Train Loss: 0.68186\n","Epoch: 00 [ 3411/20101 ( 17%)], Train Loss: 0.68088\n","Epoch: 00 [ 3421/20101 ( 17%)], Train Loss: 0.67972\n","Epoch: 00 [ 3431/20101 ( 17%)], Train Loss: 0.67839\n","Epoch: 00 [ 3441/20101 ( 17%)], Train Loss: 0.67825\n","Epoch: 00 [ 3451/20101 ( 17%)], Train Loss: 0.67720\n","Epoch: 00 [ 3461/20101 ( 17%)], Train Loss: 0.67675\n","Epoch: 00 [ 3471/20101 ( 17%)], Train Loss: 0.67567\n","Epoch: 00 [ 3481/20101 ( 17%)], Train Loss: 0.67521\n","Epoch: 00 [ 3491/20101 ( 17%)], Train Loss: 0.67345\n","Epoch: 00 [ 3501/20101 ( 17%)], Train Loss: 0.67284\n","Epoch: 00 [ 3511/20101 ( 17%)], Train Loss: 0.67265\n","Epoch: 00 [ 3521/20101 ( 18%)], Train Loss: 0.67248\n","Epoch: 00 [ 3531/20101 ( 18%)], Train Loss: 0.67241\n","Epoch: 00 [ 3541/20101 ( 18%)], Train Loss: 0.67324\n","Epoch: 00 [ 3551/20101 ( 18%)], Train Loss: 0.67241\n","Epoch: 00 [ 3561/20101 ( 18%)], Train Loss: 0.67242\n","Epoch: 00 [ 3571/20101 ( 18%)], Train Loss: 0.67128\n","Epoch: 00 [ 3581/20101 ( 18%)], Train Loss: 0.67106\n","Epoch: 00 [ 3591/20101 ( 18%)], Train Loss: 0.67019\n","Epoch: 00 [ 3601/20101 ( 18%)], Train Loss: 0.67032\n","Epoch: 00 [ 3611/20101 ( 18%)], Train Loss: 0.66910\n","Epoch: 00 [ 3621/20101 ( 18%)], Train Loss: 0.66951\n","Epoch: 00 [ 3631/20101 ( 18%)], Train Loss: 0.66875\n","Epoch: 00 [ 3641/20101 ( 18%)], Train Loss: 0.66840\n","Epoch: 00 [ 3651/20101 ( 18%)], Train Loss: 0.66833\n","Epoch: 00 [ 3661/20101 ( 18%)], Train Loss: 0.66896\n","Epoch: 00 [ 3671/20101 ( 18%)], Train Loss: 0.66755\n","Epoch: 00 [ 3681/20101 ( 18%)], Train Loss: 0.66734\n","Epoch: 00 [ 3691/20101 ( 18%)], Train Loss: 0.66581\n","Epoch: 00 [ 3701/20101 ( 18%)], Train Loss: 0.66504\n","Epoch: 00 [ 3711/20101 ( 18%)], Train Loss: 0.66480\n","Epoch: 00 [ 3721/20101 ( 19%)], Train Loss: 0.66499\n","Epoch: 00 [ 3731/20101 ( 19%)], Train Loss: 0.66484\n","Epoch: 00 [ 3741/20101 ( 19%)], Train Loss: 0.66476\n","Epoch: 00 [ 3751/20101 ( 19%)], Train Loss: 0.66348\n","Epoch: 00 [ 3761/20101 ( 19%)], Train Loss: 0.66248\n","Epoch: 00 [ 3771/20101 ( 19%)], Train Loss: 0.66166\n","Epoch: 00 [ 3781/20101 ( 19%)], Train Loss: 0.66330\n","Epoch: 00 [ 3791/20101 ( 19%)], Train Loss: 0.66278\n","Epoch: 00 [ 3801/20101 ( 19%)], Train Loss: 0.66158\n","Epoch: 00 [ 3811/20101 ( 19%)], Train Loss: 0.66109\n","Epoch: 00 [ 3821/20101 ( 19%)], Train Loss: 0.66072\n","Epoch: 00 [ 3831/20101 ( 19%)], Train Loss: 0.65963\n","Epoch: 00 [ 3841/20101 ( 19%)], Train Loss: 0.65847\n","Epoch: 00 [ 3851/20101 ( 19%)], Train Loss: 0.65831\n","Epoch: 00 [ 3861/20101 ( 19%)], Train Loss: 0.65733\n","Epoch: 00 [ 3871/20101 ( 19%)], Train Loss: 0.65651\n","Epoch: 00 [ 3881/20101 ( 19%)], Train Loss: 0.65585\n","Epoch: 00 [ 3891/20101 ( 19%)], Train Loss: 0.65512\n","Epoch: 00 [ 3901/20101 ( 19%)], Train Loss: 0.65489\n","Epoch: 00 [ 3911/20101 ( 19%)], Train Loss: 0.65419\n","Epoch: 00 [ 3921/20101 ( 20%)], Train Loss: 0.65313\n","Epoch: 00 [ 3931/20101 ( 20%)], Train Loss: 0.65192\n","Epoch: 00 [ 3941/20101 ( 20%)], Train Loss: 0.65106\n","Epoch: 00 [ 3951/20101 ( 20%)], Train Loss: 0.65132\n","Epoch: 00 [ 3961/20101 ( 20%)], Train Loss: 0.65033\n","Epoch: 00 [ 3971/20101 ( 20%)], Train Loss: 0.64951\n","Epoch: 00 [ 3981/20101 ( 20%)], Train Loss: 0.64906\n","Epoch: 00 [ 3991/20101 ( 20%)], Train Loss: 0.64831\n","Epoch: 00 [ 4001/20101 ( 20%)], Train Loss: 0.64796\n","Epoch: 00 [ 4011/20101 ( 20%)], Train Loss: 0.64688\n","Epoch: 00 [ 4021/20101 ( 20%)], Train Loss: 0.64670\n","Epoch: 00 [ 4031/20101 ( 20%)], Train Loss: 0.64589\n","Epoch: 00 [ 4041/20101 ( 20%)], Train Loss: 0.64516\n","Epoch: 00 [ 4051/20101 ( 20%)], Train Loss: 0.64420\n","Epoch: 00 [ 4061/20101 ( 20%)], Train Loss: 0.64382\n","Epoch: 00 [ 4071/20101 ( 20%)], Train Loss: 0.64312\n","Epoch: 00 [ 4081/20101 ( 20%)], Train Loss: 0.64274\n","Epoch: 00 [ 4091/20101 ( 20%)], Train Loss: 0.64248\n","Epoch: 00 [ 4101/20101 ( 20%)], Train Loss: 0.64198\n","Epoch: 00 [ 4111/20101 ( 20%)], Train Loss: 0.64096\n","Epoch: 00 [ 4121/20101 ( 21%)], Train Loss: 0.64019\n","Epoch: 00 [ 4131/20101 ( 21%)], Train Loss: 0.63985\n","Epoch: 00 [ 4141/20101 ( 21%)], Train Loss: 0.63865\n","Epoch: 00 [ 4151/20101 ( 21%)], Train Loss: 0.63873\n","Epoch: 00 [ 4161/20101 ( 21%)], Train Loss: 0.63797\n","Epoch: 00 [ 4171/20101 ( 21%)], Train Loss: 0.63680\n","Epoch: 00 [ 4181/20101 ( 21%)], Train Loss: 0.63662\n","Epoch: 00 [ 4191/20101 ( 21%)], Train Loss: 0.63608\n","Epoch: 00 [ 4201/20101 ( 21%)], Train Loss: 0.63498\n","Epoch: 00 [ 4211/20101 ( 21%)], Train Loss: 0.63480\n","Epoch: 00 [ 4221/20101 ( 21%)], Train Loss: 0.63385\n","Epoch: 00 [ 4231/20101 ( 21%)], Train Loss: 0.63369\n","Epoch: 00 [ 4241/20101 ( 21%)], Train Loss: 0.63317\n","Epoch: 00 [ 4251/20101 ( 21%)], Train Loss: 0.63268\n","Epoch: 00 [ 4261/20101 ( 21%)], Train Loss: 0.63209\n","Epoch: 00 [ 4271/20101 ( 21%)], Train Loss: 0.63157\n","Epoch: 00 [ 4281/20101 ( 21%)], Train Loss: 0.63164\n","Epoch: 00 [ 4291/20101 ( 21%)], Train Loss: 0.63085\n","Epoch: 00 [ 4301/20101 ( 21%)], Train Loss: 0.63051\n","Epoch: 00 [ 4311/20101 ( 21%)], Train Loss: 0.62988\n","Epoch: 00 [ 4321/20101 ( 21%)], Train Loss: 0.62968\n","Epoch: 00 [ 4331/20101 ( 22%)], Train Loss: 0.63033\n","Epoch: 00 [ 4341/20101 ( 22%)], Train Loss: 0.62945\n","Epoch: 00 [ 4351/20101 ( 22%)], Train Loss: 0.62844\n","Epoch: 00 [ 4361/20101 ( 22%)], Train Loss: 0.62958\n","Epoch: 00 [ 4371/20101 ( 22%)], Train Loss: 0.62864\n","Epoch: 00 [ 4381/20101 ( 22%)], Train Loss: 0.62816\n","Epoch: 00 [ 4391/20101 ( 22%)], Train Loss: 0.62740\n","Epoch: 00 [ 4401/20101 ( 22%)], Train Loss: 0.62663\n","Epoch: 00 [ 4411/20101 ( 22%)], Train Loss: 0.62710\n","Epoch: 00 [ 4421/20101 ( 22%)], Train Loss: 0.62708\n","Epoch: 00 [ 4431/20101 ( 22%)], Train Loss: 0.62653\n","Epoch: 00 [ 4441/20101 ( 22%)], Train Loss: 0.62747\n","Epoch: 00 [ 4451/20101 ( 22%)], Train Loss: 0.62748\n","Epoch: 00 [ 4461/20101 ( 22%)], Train Loss: 0.62684\n","Epoch: 00 [ 4471/20101 ( 22%)], Train Loss: 0.62609\n","Epoch: 00 [ 4481/20101 ( 22%)], Train Loss: 0.62684\n","Epoch: 00 [ 4491/20101 ( 22%)], Train Loss: 0.62605\n","Epoch: 00 [ 4501/20101 ( 22%)], Train Loss: 0.62531\n","Epoch: 00 [ 4511/20101 ( 22%)], Train Loss: 0.62617\n","Epoch: 00 [ 4521/20101 ( 22%)], Train Loss: 0.62658\n","Epoch: 00 [ 4531/20101 ( 23%)], Train Loss: 0.62708\n","Epoch: 00 [ 4541/20101 ( 23%)], Train Loss: 0.62732\n","Epoch: 00 [ 4551/20101 ( 23%)], Train Loss: 0.62660\n","Epoch: 00 [ 4561/20101 ( 23%)], Train Loss: 0.62607\n","Epoch: 00 [ 4571/20101 ( 23%)], Train Loss: 0.62635\n","Epoch: 00 [ 4581/20101 ( 23%)], Train Loss: 0.62581\n","Epoch: 00 [ 4591/20101 ( 23%)], Train Loss: 0.62664\n","Epoch: 00 [ 4601/20101 ( 23%)], Train Loss: 0.62624\n","Epoch: 00 [ 4611/20101 ( 23%)], Train Loss: 0.62560\n","Epoch: 00 [ 4621/20101 ( 23%)], Train Loss: 0.62549\n","Epoch: 00 [ 4631/20101 ( 23%)], Train Loss: 0.62467\n","Epoch: 00 [ 4641/20101 ( 23%)], Train Loss: 0.62357\n","Epoch: 00 [ 4651/20101 ( 23%)], Train Loss: 0.62294\n","Epoch: 00 [ 4661/20101 ( 23%)], Train Loss: 0.62195\n","Epoch: 00 [ 4671/20101 ( 23%)], Train Loss: 0.62109\n","Epoch: 00 [ 4681/20101 ( 23%)], Train Loss: 0.62088\n","Epoch: 00 [ 4691/20101 ( 23%)], Train Loss: 0.62119\n","Epoch: 00 [ 4701/20101 ( 23%)], Train Loss: 0.62114\n","Epoch: 00 [ 4711/20101 ( 23%)], Train Loss: 0.62150\n","Epoch: 00 [ 4721/20101 ( 23%)], Train Loss: 0.62104\n","Epoch: 00 [ 4731/20101 ( 24%)], Train Loss: 0.62089\n","Epoch: 00 [ 4741/20101 ( 24%)], Train Loss: 0.61985\n","Epoch: 00 [ 4751/20101 ( 24%)], Train Loss: 0.61928\n","Epoch: 00 [ 4761/20101 ( 24%)], Train Loss: 0.61928\n","Epoch: 00 [ 4771/20101 ( 24%)], Train Loss: 0.61999\n","Epoch: 00 [ 4781/20101 ( 24%)], Train Loss: 0.62090\n","Epoch: 00 [ 4791/20101 ( 24%)], Train Loss: 0.62039\n","Epoch: 00 [ 4801/20101 ( 24%)], Train Loss: 0.62047\n","Epoch: 00 [ 4811/20101 ( 24%)], Train Loss: 0.61994\n","Epoch: 00 [ 4821/20101 ( 24%)], Train Loss: 0.61939\n","Epoch: 00 [ 4831/20101 ( 24%)], Train Loss: 0.61832\n","Epoch: 00 [ 4841/20101 ( 24%)], Train Loss: 0.61799\n","Epoch: 00 [ 4851/20101 ( 24%)], Train Loss: 0.61710\n","Epoch: 00 [ 4861/20101 ( 24%)], Train Loss: 0.61648\n","Epoch: 00 [ 4871/20101 ( 24%)], Train Loss: 0.61643\n","Epoch: 00 [ 4881/20101 ( 24%)], Train Loss: 0.61589\n","Epoch: 00 [ 4891/20101 ( 24%)], Train Loss: 0.61518\n","Epoch: 00 [ 4901/20101 ( 24%)], Train Loss: 0.61440\n","Epoch: 00 [ 4911/20101 ( 24%)], Train Loss: 0.61376\n","Epoch: 00 [ 4921/20101 ( 24%)], Train Loss: 0.61354\n","Epoch: 00 [ 4931/20101 ( 25%)], Train Loss: 0.61285\n","Epoch: 00 [ 4941/20101 ( 25%)], Train Loss: 0.61205\n","Epoch: 00 [ 4951/20101 ( 25%)], Train Loss: 0.61152\n","Epoch: 00 [ 4961/20101 ( 25%)], Train Loss: 0.61093\n","Epoch: 00 [ 4971/20101 ( 25%)], Train Loss: 0.61125\n","Epoch: 00 [ 4981/20101 ( 25%)], Train Loss: 0.61106\n","Epoch: 00 [ 4991/20101 ( 25%)], Train Loss: 0.61049\n","Epoch: 00 [ 5001/20101 ( 25%)], Train Loss: 0.61052\n","Epoch: 00 [ 5011/20101 ( 25%)], Train Loss: 0.61085\n","Epoch: 00 [ 5021/20101 ( 25%)], Train Loss: 0.61080\n","Epoch: 00 [ 5031/20101 ( 25%)], Train Loss: 0.61085\n","Epoch: 00 [ 5041/20101 ( 25%)], Train Loss: 0.61109\n","Epoch: 00 [ 5051/20101 ( 25%)], Train Loss: 0.61081\n","Epoch: 00 [ 5061/20101 ( 25%)], Train Loss: 0.61009\n","Epoch: 00 [ 5071/20101 ( 25%)], Train Loss: 0.60926\n","Epoch: 00 [ 5081/20101 ( 25%)], Train Loss: 0.60877\n","Epoch: 00 [ 5091/20101 ( 25%)], Train Loss: 0.60873\n","Epoch: 00 [ 5101/20101 ( 25%)], Train Loss: 0.60951\n","Epoch: 00 [ 5111/20101 ( 25%)], Train Loss: 0.60919\n","Epoch: 00 [ 5121/20101 ( 25%)], Train Loss: 0.60902\n","Epoch: 00 [ 5131/20101 ( 26%)], Train Loss: 0.60844\n","Epoch: 00 [ 5141/20101 ( 26%)], Train Loss: 0.60826\n","Epoch: 00 [ 5151/20101 ( 26%)], Train Loss: 0.60829\n","Epoch: 00 [ 5161/20101 ( 26%)], Train Loss: 0.60744\n","Epoch: 00 [ 5171/20101 ( 26%)], Train Loss: 0.60717\n","Epoch: 00 [ 5181/20101 ( 26%)], Train Loss: 0.60716\n","Epoch: 00 [ 5191/20101 ( 26%)], Train Loss: 0.60614\n","Epoch: 00 [ 5201/20101 ( 26%)], Train Loss: 0.60510\n","Epoch: 00 [ 5211/20101 ( 26%)], Train Loss: 0.60536\n","Epoch: 00 [ 5221/20101 ( 26%)], Train Loss: 0.60459\n","Epoch: 00 [ 5231/20101 ( 26%)], Train Loss: 0.60473\n","Epoch: 00 [ 5241/20101 ( 26%)], Train Loss: 0.60523\n","Epoch: 00 [ 5251/20101 ( 26%)], Train Loss: 0.60440\n","Epoch: 00 [ 5261/20101 ( 26%)], Train Loss: 0.60379\n","Epoch: 00 [ 5271/20101 ( 26%)], Train Loss: 0.60298\n","Epoch: 00 [ 5281/20101 ( 26%)], Train Loss: 0.60228\n","Epoch: 00 [ 5291/20101 ( 26%)], Train Loss: 0.60144\n","Epoch: 00 [ 5301/20101 ( 26%)], Train Loss: 0.60211\n","Epoch: 00 [ 5311/20101 ( 26%)], Train Loss: 0.60249\n","Epoch: 00 [ 5321/20101 ( 26%)], Train Loss: 0.60302\n","Epoch: 00 [ 5331/20101 ( 27%)], Train Loss: 0.60278\n","Epoch: 00 [ 5341/20101 ( 27%)], Train Loss: 0.60295\n","Epoch: 00 [ 5351/20101 ( 27%)], Train Loss: 0.60315\n","Epoch: 00 [ 5361/20101 ( 27%)], Train Loss: 0.60216\n","Epoch: 00 [ 5371/20101 ( 27%)], Train Loss: 0.60217\n","Epoch: 00 [ 5381/20101 ( 27%)], Train Loss: 0.60192\n","Epoch: 00 [ 5391/20101 ( 27%)], Train Loss: 0.60234\n","Epoch: 00 [ 5401/20101 ( 27%)], Train Loss: 0.60200\n","Epoch: 00 [ 5411/20101 ( 27%)], Train Loss: 0.60173\n","Epoch: 00 [ 5421/20101 ( 27%)], Train Loss: 0.60244\n","Epoch: 00 [ 5431/20101 ( 27%)], Train Loss: 0.60233\n","Epoch: 00 [ 5441/20101 ( 27%)], Train Loss: 0.60232\n","Epoch: 00 [ 5451/20101 ( 27%)], Train Loss: 0.60205\n","Epoch: 00 [ 5461/20101 ( 27%)], Train Loss: 0.60181\n","Epoch: 00 [ 5471/20101 ( 27%)], Train Loss: 0.60132\n","Epoch: 00 [ 5481/20101 ( 27%)], Train Loss: 0.60151\n","Epoch: 00 [ 5491/20101 ( 27%)], Train Loss: 0.60076\n","Epoch: 00 [ 5501/20101 ( 27%)], Train Loss: 0.59984\n","Epoch: 00 [ 5511/20101 ( 27%)], Train Loss: 0.59923\n","Epoch: 00 [ 5521/20101 ( 27%)], Train Loss: 0.59965\n","Epoch: 00 [ 5531/20101 ( 28%)], Train Loss: 0.59980\n","Epoch: 00 [ 5541/20101 ( 28%)], Train Loss: 0.59987\n","Epoch: 00 [ 5551/20101 ( 28%)], Train Loss: 0.59948\n","Epoch: 00 [ 5561/20101 ( 28%)], Train Loss: 0.60100\n","Epoch: 00 [ 5571/20101 ( 28%)], Train Loss: 0.60108\n","Epoch: 00 [ 5581/20101 ( 28%)], Train Loss: 0.60137\n","Epoch: 00 [ 5591/20101 ( 28%)], Train Loss: 0.60086\n","Epoch: 00 [ 5601/20101 ( 28%)], Train Loss: 0.60119\n","Epoch: 00 [ 5611/20101 ( 28%)], Train Loss: 0.60092\n","Epoch: 00 [ 5621/20101 ( 28%)], Train Loss: 0.60021\n","Epoch: 00 [ 5631/20101 ( 28%)], Train Loss: 0.59952\n","Epoch: 00 [ 5641/20101 ( 28%)], Train Loss: 0.59919\n","Epoch: 00 [ 5651/20101 ( 28%)], Train Loss: 0.59864\n","Epoch: 00 [ 5661/20101 ( 28%)], Train Loss: 0.59836\n","Epoch: 00 [ 5671/20101 ( 28%)], Train Loss: 0.59845\n","Epoch: 00 [ 5681/20101 ( 28%)], Train Loss: 0.59795\n","Epoch: 00 [ 5691/20101 ( 28%)], Train Loss: 0.59717\n","Epoch: 00 [ 5701/20101 ( 28%)], Train Loss: 0.59631\n","Epoch: 00 [ 5711/20101 ( 28%)], Train Loss: 0.59645\n","Epoch: 00 [ 5721/20101 ( 28%)], Train Loss: 0.59616\n","Epoch: 00 [ 5731/20101 ( 29%)], Train Loss: 0.59619\n","Epoch: 00 [ 5741/20101 ( 29%)], Train Loss: 0.59542\n","Epoch: 00 [ 5751/20101 ( 29%)], Train Loss: 0.59465\n","Epoch: 00 [ 5761/20101 ( 29%)], Train Loss: 0.59507\n","Epoch: 00 [ 5771/20101 ( 29%)], Train Loss: 0.59505\n","Epoch: 00 [ 5781/20101 ( 29%)], Train Loss: 0.59529\n","Epoch: 00 [ 5791/20101 ( 29%)], Train Loss: 0.59483\n","Epoch: 00 [ 5801/20101 ( 29%)], Train Loss: 0.59521\n","Epoch: 00 [ 5811/20101 ( 29%)], Train Loss: 0.59520\n","Epoch: 00 [ 5821/20101 ( 29%)], Train Loss: 0.59495\n","Epoch: 00 [ 5831/20101 ( 29%)], Train Loss: 0.59431\n","Epoch: 00 [ 5841/20101 ( 29%)], Train Loss: 0.59382\n","Epoch: 00 [ 5851/20101 ( 29%)], Train Loss: 0.59422\n","Epoch: 00 [ 5861/20101 ( 29%)], Train Loss: 0.59392\n","Epoch: 00 [ 5871/20101 ( 29%)], Train Loss: 0.59351\n","Epoch: 00 [ 5881/20101 ( 29%)], Train Loss: 0.59290\n","Epoch: 00 [ 5891/20101 ( 29%)], Train Loss: 0.59287\n","Epoch: 00 [ 5901/20101 ( 29%)], Train Loss: 0.59324\n","Epoch: 00 [ 5911/20101 ( 29%)], Train Loss: 0.59337\n","Epoch: 00 [ 5921/20101 ( 29%)], Train Loss: 0.59341\n","Epoch: 00 [ 5931/20101 ( 30%)], Train Loss: 0.59373\n","Epoch: 00 [ 5941/20101 ( 30%)], Train Loss: 0.59291\n","Epoch: 00 [ 5951/20101 ( 30%)], Train Loss: 0.59283\n","Epoch: 00 [ 5961/20101 ( 30%)], Train Loss: 0.59249\n","Epoch: 00 [ 5971/20101 ( 30%)], Train Loss: 0.59217\n","Epoch: 00 [ 5981/20101 ( 30%)], Train Loss: 0.59156\n","Epoch: 00 [ 5991/20101 ( 30%)], Train Loss: 0.59193\n","Epoch: 00 [ 6001/20101 ( 30%)], Train Loss: 0.59115\n","Epoch: 00 [ 6011/20101 ( 30%)], Train Loss: 0.59074\n","Epoch: 00 [ 6021/20101 ( 30%)], Train Loss: 0.59122\n","Epoch: 00 [ 6031/20101 ( 30%)], Train Loss: 0.59121\n","Epoch: 00 [ 6041/20101 ( 30%)], Train Loss: 0.59158\n","Epoch: 00 [ 6051/20101 ( 30%)], Train Loss: 0.59172\n","Epoch: 00 [ 6061/20101 ( 30%)], Train Loss: 0.59125\n","Epoch: 00 [ 6071/20101 ( 30%)], Train Loss: 0.59203\n","Epoch: 00 [ 6081/20101 ( 30%)], Train Loss: 0.59201\n","Epoch: 00 [ 6091/20101 ( 30%)], Train Loss: 0.59131\n","Epoch: 00 [ 6101/20101 ( 30%)], Train Loss: 0.59106\n","Epoch: 00 [ 6111/20101 ( 30%)], Train Loss: 0.59091\n","Epoch: 00 [ 6121/20101 ( 30%)], Train Loss: 0.59117\n","Epoch: 00 [ 6131/20101 ( 31%)], Train Loss: 0.59122\n","Epoch: 00 [ 6141/20101 ( 31%)], Train Loss: 0.59095\n","Epoch: 00 [ 6151/20101 ( 31%)], Train Loss: 0.59043\n","Epoch: 00 [ 6161/20101 ( 31%)], Train Loss: 0.59067\n","Epoch: 00 [ 6171/20101 ( 31%)], Train Loss: 0.59016\n","Epoch: 00 [ 6181/20101 ( 31%)], Train Loss: 0.59087\n","Epoch: 00 [ 6191/20101 ( 31%)], Train Loss: 0.59031\n","Epoch: 00 [ 6201/20101 ( 31%)], Train Loss: 0.58972\n","Epoch: 00 [ 6211/20101 ( 31%)], Train Loss: 0.58936\n","Epoch: 00 [ 6221/20101 ( 31%)], Train Loss: 0.58888\n","Epoch: 00 [ 6231/20101 ( 31%)], Train Loss: 0.58915\n","Epoch: 00 [ 6241/20101 ( 31%)], Train Loss: 0.58867\n","Epoch: 00 [ 6251/20101 ( 31%)], Train Loss: 0.58779\n","Epoch: 00 [ 6261/20101 ( 31%)], Train Loss: 0.58784\n","Epoch: 00 [ 6271/20101 ( 31%)], Train Loss: 0.58753\n","Epoch: 00 [ 6281/20101 ( 31%)], Train Loss: 0.58691\n","Epoch: 00 [ 6291/20101 ( 31%)], Train Loss: 0.58621\n","Epoch: 00 [ 6301/20101 ( 31%)], Train Loss: 0.58621\n","Epoch: 00 [ 6311/20101 ( 31%)], Train Loss: 0.58755\n","Epoch: 00 [ 6321/20101 ( 31%)], Train Loss: 0.58767\n","Epoch: 00 [ 6331/20101 ( 31%)], Train Loss: 0.58730\n","Epoch: 00 [ 6341/20101 ( 32%)], Train Loss: 0.58713\n","Epoch: 00 [ 6351/20101 ( 32%)], Train Loss: 0.58733\n","Epoch: 00 [ 6361/20101 ( 32%)], Train Loss: 0.58704\n","Epoch: 00 [ 6371/20101 ( 32%)], Train Loss: 0.58687\n","Epoch: 00 [ 6381/20101 ( 32%)], Train Loss: 0.58668\n","Epoch: 00 [ 6391/20101 ( 32%)], Train Loss: 0.58651\n","Epoch: 00 [ 6401/20101 ( 32%)], Train Loss: 0.58633\n","Epoch: 00 [ 6411/20101 ( 32%)], Train Loss: 0.58589\n","Epoch: 00 [ 6421/20101 ( 32%)], Train Loss: 0.58579\n","Epoch: 00 [ 6431/20101 ( 32%)], Train Loss: 0.58523\n","Epoch: 00 [ 6441/20101 ( 32%)], Train Loss: 0.58458\n","Epoch: 00 [ 6451/20101 ( 32%)], Train Loss: 0.58455\n","Epoch: 00 [ 6461/20101 ( 32%)], Train Loss: 0.58416\n","Epoch: 00 [ 6471/20101 ( 32%)], Train Loss: 0.58510\n","Epoch: 00 [ 6481/20101 ( 32%)], Train Loss: 0.58511\n","Epoch: 00 [ 6491/20101 ( 32%)], Train Loss: 0.58537\n","Epoch: 00 [ 6501/20101 ( 32%)], Train Loss: 0.58606\n","Epoch: 00 [ 6511/20101 ( 32%)], Train Loss: 0.58620\n","Epoch: 00 [ 6521/20101 ( 32%)], Train Loss: 0.58599\n","Epoch: 00 [ 6531/20101 ( 32%)], Train Loss: 0.58541\n","Epoch: 00 [ 6541/20101 ( 33%)], Train Loss: 0.58515\n","Epoch: 00 [ 6551/20101 ( 33%)], Train Loss: 0.58514\n","Epoch: 00 [ 6561/20101 ( 33%)], Train Loss: 0.58469\n","Epoch: 00 [ 6571/20101 ( 33%)], Train Loss: 0.58402\n","Epoch: 00 [ 6581/20101 ( 33%)], Train Loss: 0.58356\n","Epoch: 00 [ 6591/20101 ( 33%)], Train Loss: 0.58306\n","Epoch: 00 [ 6601/20101 ( 33%)], Train Loss: 0.58369\n","Epoch: 00 [ 6611/20101 ( 33%)], Train Loss: 0.58343\n","Epoch: 00 [ 6621/20101 ( 33%)], Train Loss: 0.58293\n","Epoch: 00 [ 6631/20101 ( 33%)], Train Loss: 0.58256\n","Epoch: 00 [ 6641/20101 ( 33%)], Train Loss: 0.58335\n","Epoch: 00 [ 6651/20101 ( 33%)], Train Loss: 0.58359\n","Epoch: 00 [ 6661/20101 ( 33%)], Train Loss: 0.58297\n","Epoch: 00 [ 6671/20101 ( 33%)], Train Loss: 0.58289\n","Epoch: 00 [ 6681/20101 ( 33%)], Train Loss: 0.58344\n","Epoch: 00 [ 6691/20101 ( 33%)], Train Loss: 0.58299\n","Epoch: 00 [ 6701/20101 ( 33%)], Train Loss: 0.58245\n","Epoch: 00 [ 6711/20101 ( 33%)], Train Loss: 0.58212\n","Epoch: 00 [ 6721/20101 ( 33%)], Train Loss: 0.58196\n","Epoch: 00 [ 6731/20101 ( 33%)], Train Loss: 0.58186\n","Epoch: 00 [ 6741/20101 ( 34%)], Train Loss: 0.58171\n","Epoch: 00 [ 6751/20101 ( 34%)], Train Loss: 0.58136\n","Epoch: 00 [ 6761/20101 ( 34%)], Train Loss: 0.58213\n","Epoch: 00 [ 6771/20101 ( 34%)], Train Loss: 0.58173\n","Epoch: 00 [ 6781/20101 ( 34%)], Train Loss: 0.58108\n","Epoch: 00 [ 6791/20101 ( 34%)], Train Loss: 0.58040\n","Epoch: 00 [ 6801/20101 ( 34%)], Train Loss: 0.58037\n","Epoch: 00 [ 6811/20101 ( 34%)], Train Loss: 0.57997\n","Epoch: 00 [ 6821/20101 ( 34%)], Train Loss: 0.57968\n","Epoch: 00 [ 6831/20101 ( 34%)], Train Loss: 0.57979\n","Epoch: 00 [ 6841/20101 ( 34%)], Train Loss: 0.58025\n","Epoch: 00 [ 6851/20101 ( 34%)], Train Loss: 0.58009\n","Epoch: 00 [ 6861/20101 ( 34%)], Train Loss: 0.58055\n","Epoch: 00 [ 6871/20101 ( 34%)], Train Loss: 0.58076\n","Epoch: 00 [ 6881/20101 ( 34%)], Train Loss: 0.58108\n","Epoch: 00 [ 6891/20101 ( 34%)], Train Loss: 0.58096\n","Epoch: 00 [ 6901/20101 ( 34%)], Train Loss: 0.58057\n","Epoch: 00 [ 6911/20101 ( 34%)], Train Loss: 0.58040\n","Epoch: 00 [ 6921/20101 ( 34%)], Train Loss: 0.57979\n","Epoch: 00 [ 6931/20101 ( 34%)], Train Loss: 0.57980\n","Epoch: 00 [ 6941/20101 ( 35%)], Train Loss: 0.57986\n","Epoch: 00 [ 6951/20101 ( 35%)], Train Loss: 0.58018\n","Epoch: 00 [ 6961/20101 ( 35%)], Train Loss: 0.57962\n","Epoch: 00 [ 6971/20101 ( 35%)], Train Loss: 0.57945\n","Epoch: 00 [ 6981/20101 ( 35%)], Train Loss: 0.57931\n","Epoch: 00 [ 6991/20101 ( 35%)], Train Loss: 0.57857\n","Epoch: 00 [ 7001/20101 ( 35%)], Train Loss: 0.57849\n","Epoch: 00 [ 7011/20101 ( 35%)], Train Loss: 0.57780\n","Epoch: 00 [ 7021/20101 ( 35%)], Train Loss: 0.57783\n","Epoch: 00 [ 7031/20101 ( 35%)], Train Loss: 0.57726\n","Epoch: 00 [ 7041/20101 ( 35%)], Train Loss: 0.57751\n","Epoch: 00 [ 7051/20101 ( 35%)], Train Loss: 0.57765\n","Epoch: 00 [ 7061/20101 ( 35%)], Train Loss: 0.57722\n","Epoch: 00 [ 7071/20101 ( 35%)], Train Loss: 0.57705\n","Epoch: 00 [ 7081/20101 ( 35%)], Train Loss: 0.57695\n","Epoch: 00 [ 7091/20101 ( 35%)], Train Loss: 0.57630\n","Epoch: 00 [ 7101/20101 ( 35%)], Train Loss: 0.57575\n","Epoch: 00 [ 7111/20101 ( 35%)], Train Loss: 0.57584\n","Epoch: 00 [ 7121/20101 ( 35%)], Train Loss: 0.57519\n","Epoch: 00 [ 7131/20101 ( 35%)], Train Loss: 0.57570\n","Epoch: 00 [ 7141/20101 ( 36%)], Train Loss: 0.57515\n","Epoch: 00 [ 7151/20101 ( 36%)], Train Loss: 0.57505\n","Epoch: 00 [ 7161/20101 ( 36%)], Train Loss: 0.57488\n","Epoch: 00 [ 7171/20101 ( 36%)], Train Loss: 0.57434\n","Epoch: 00 [ 7181/20101 ( 36%)], Train Loss: 0.57395\n","Epoch: 00 [ 7191/20101 ( 36%)], Train Loss: 0.57340\n","Epoch: 00 [ 7201/20101 ( 36%)], Train Loss: 0.57315\n","Epoch: 00 [ 7211/20101 ( 36%)], Train Loss: 0.57249\n","Epoch: 00 [ 7221/20101 ( 36%)], Train Loss: 0.57366\n","Epoch: 00 [ 7231/20101 ( 36%)], Train Loss: 0.57309\n","Epoch: 00 [ 7241/20101 ( 36%)], Train Loss: 0.57280\n","Epoch: 00 [ 7251/20101 ( 36%)], Train Loss: 0.57240\n","Epoch: 00 [ 7261/20101 ( 36%)], Train Loss: 0.57226\n","Epoch: 00 [ 7271/20101 ( 36%)], Train Loss: 0.57188\n","Epoch: 00 [ 7281/20101 ( 36%)], Train Loss: 0.57115\n","Epoch: 00 [ 7291/20101 ( 36%)], Train Loss: 0.57113\n","Epoch: 00 [ 7301/20101 ( 36%)], Train Loss: 0.57080\n","Epoch: 00 [ 7311/20101 ( 36%)], Train Loss: 0.57012\n","Epoch: 00 [ 7321/20101 ( 36%)], Train Loss: 0.57091\n","Epoch: 00 [ 7331/20101 ( 36%)], Train Loss: 0.57060\n","Epoch: 00 [ 7341/20101 ( 37%)], Train Loss: 0.57099\n","Epoch: 00 [ 7351/20101 ( 37%)], Train Loss: 0.57049\n","Epoch: 00 [ 7361/20101 ( 37%)], Train Loss: 0.57091\n","Epoch: 00 [ 7371/20101 ( 37%)], Train Loss: 0.57066\n","Epoch: 00 [ 7381/20101 ( 37%)], Train Loss: 0.57092\n","Epoch: 00 [ 7391/20101 ( 37%)], Train Loss: 0.57095\n","Epoch: 00 [ 7401/20101 ( 37%)], Train Loss: 0.57090\n","Epoch: 00 [ 7411/20101 ( 37%)], Train Loss: 0.57058\n","Epoch: 00 [ 7421/20101 ( 37%)], Train Loss: 0.57079\n","Epoch: 00 [ 7431/20101 ( 37%)], Train Loss: 0.57016\n","Epoch: 00 [ 7441/20101 ( 37%)], Train Loss: 0.57015\n","Epoch: 00 [ 7451/20101 ( 37%)], Train Loss: 0.57014\n","Epoch: 00 [ 7461/20101 ( 37%)], Train Loss: 0.56947\n","Epoch: 00 [ 7471/20101 ( 37%)], Train Loss: 0.56900\n","Epoch: 00 [ 7481/20101 ( 37%)], Train Loss: 0.56840\n","Epoch: 00 [ 7491/20101 ( 37%)], Train Loss: 0.56926\n","Epoch: 00 [ 7501/20101 ( 37%)], Train Loss: 0.56867\n","Epoch: 00 [ 7511/20101 ( 37%)], Train Loss: 0.56797\n","Epoch: 00 [ 7521/20101 ( 37%)], Train Loss: 0.56731\n","Epoch: 00 [ 7531/20101 ( 37%)], Train Loss: 0.56684\n","Epoch: 00 [ 7541/20101 ( 38%)], Train Loss: 0.56661\n","Epoch: 00 [ 7551/20101 ( 38%)], Train Loss: 0.56672\n","Epoch: 00 [ 7561/20101 ( 38%)], Train Loss: 0.56698\n","Epoch: 00 [ 7571/20101 ( 38%)], Train Loss: 0.56707\n","Epoch: 00 [ 7581/20101 ( 38%)], Train Loss: 0.56755\n","Epoch: 00 [ 7591/20101 ( 38%)], Train Loss: 0.56729\n","Epoch: 00 [ 7601/20101 ( 38%)], Train Loss: 0.56664\n","Epoch: 00 [ 7611/20101 ( 38%)], Train Loss: 0.56655\n","Epoch: 00 [ 7621/20101 ( 38%)], Train Loss: 0.56609\n","Epoch: 00 [ 7631/20101 ( 38%)], Train Loss: 0.56676\n","Epoch: 00 [ 7641/20101 ( 38%)], Train Loss: 0.56649\n","Epoch: 00 [ 7651/20101 ( 38%)], Train Loss: 0.56659\n","Epoch: 00 [ 7661/20101 ( 38%)], Train Loss: 0.56662\n","Epoch: 00 [ 7671/20101 ( 38%)], Train Loss: 0.56659\n","Epoch: 00 [ 7681/20101 ( 38%)], Train Loss: 0.56661\n","Epoch: 00 [ 7691/20101 ( 38%)], Train Loss: 0.56657\n","Epoch: 00 [ 7701/20101 ( 38%)], Train Loss: 0.56614\n","Epoch: 00 [ 7711/20101 ( 38%)], Train Loss: 0.56687\n","Epoch: 00 [ 7721/20101 ( 38%)], Train Loss: 0.56628\n","Epoch: 00 [ 7731/20101 ( 38%)], Train Loss: 0.56621\n","Epoch: 00 [ 7741/20101 ( 39%)], Train Loss: 0.56627\n","Epoch: 00 [ 7751/20101 ( 39%)], Train Loss: 0.56599\n","Epoch: 00 [ 7761/20101 ( 39%)], Train Loss: 0.56582\n","Epoch: 00 [ 7771/20101 ( 39%)], Train Loss: 0.56567\n","Epoch: 00 [ 7781/20101 ( 39%)], Train Loss: 0.56497\n","Epoch: 00 [ 7791/20101 ( 39%)], Train Loss: 0.56460\n","Epoch: 00 [ 7801/20101 ( 39%)], Train Loss: 0.56440\n","Epoch: 00 [ 7811/20101 ( 39%)], Train Loss: 0.56429\n","Epoch: 00 [ 7821/20101 ( 39%)], Train Loss: 0.56447\n","Epoch: 00 [ 7831/20101 ( 39%)], Train Loss: 0.56390\n","Epoch: 00 [ 7841/20101 ( 39%)], Train Loss: 0.56365\n","Epoch: 00 [ 7851/20101 ( 39%)], Train Loss: 0.56346\n","Epoch: 00 [ 7861/20101 ( 39%)], Train Loss: 0.56336\n","Epoch: 00 [ 7871/20101 ( 39%)], Train Loss: 0.56319\n","Epoch: 00 [ 7881/20101 ( 39%)], Train Loss: 0.56322\n","Epoch: 00 [ 7891/20101 ( 39%)], Train Loss: 0.56252\n","Epoch: 00 [ 7901/20101 ( 39%)], Train Loss: 0.56233\n","Epoch: 00 [ 7911/20101 ( 39%)], Train Loss: 0.56222\n","Epoch: 00 [ 7921/20101 ( 39%)], Train Loss: 0.56219\n","Epoch: 00 [ 7931/20101 ( 39%)], Train Loss: 0.56252\n","Epoch: 00 [ 7941/20101 ( 40%)], Train Loss: 0.56250\n","Epoch: 00 [ 7951/20101 ( 40%)], Train Loss: 0.56241\n","Epoch: 00 [ 7961/20101 ( 40%)], Train Loss: 0.56258\n","Epoch: 00 [ 7971/20101 ( 40%)], Train Loss: 0.56285\n","Epoch: 00 [ 7981/20101 ( 40%)], Train Loss: 0.56298\n","Epoch: 00 [ 7991/20101 ( 40%)], Train Loss: 0.56249\n","Epoch: 00 [ 8001/20101 ( 40%)], Train Loss: 0.56225\n","Epoch: 00 [ 8011/20101 ( 40%)], Train Loss: 0.56172\n","Epoch: 00 [ 8021/20101 ( 40%)], Train Loss: 0.56168\n","Epoch: 00 [ 8031/20101 ( 40%)], Train Loss: 0.56177\n","Epoch: 00 [ 8041/20101 ( 40%)], Train Loss: 0.56187\n","Epoch: 00 [ 8051/20101 ( 40%)], Train Loss: 0.56124\n","Epoch: 00 [ 8061/20101 ( 40%)], Train Loss: 0.56193\n","Epoch: 00 [ 8071/20101 ( 40%)], Train Loss: 0.56164\n","Epoch: 00 [ 8081/20101 ( 40%)], Train Loss: 0.56172\n","Epoch: 00 [ 8091/20101 ( 40%)], Train Loss: 0.56170\n","Epoch: 00 [ 8101/20101 ( 40%)], Train Loss: 0.56145\n","Epoch: 00 [ 8111/20101 ( 40%)], Train Loss: 0.56125\n","Epoch: 00 [ 8121/20101 ( 40%)], Train Loss: 0.56157\n","Epoch: 00 [ 8131/20101 ( 40%)], Train Loss: 0.56120\n","Epoch: 00 [ 8141/20101 ( 41%)], Train Loss: 0.56108\n","Epoch: 00 [ 8151/20101 ( 41%)], Train Loss: 0.56072\n","Epoch: 00 [ 8161/20101 ( 41%)], Train Loss: 0.56053\n","Epoch: 00 [ 8171/20101 ( 41%)], Train Loss: 0.56106\n","Epoch: 00 [ 8181/20101 ( 41%)], Train Loss: 0.56058\n","Epoch: 00 [ 8191/20101 ( 41%)], Train Loss: 0.56011\n","Epoch: 00 [ 8201/20101 ( 41%)], Train Loss: 0.55993\n","Epoch: 00 [ 8211/20101 ( 41%)], Train Loss: 0.56000\n","Epoch: 00 [ 8221/20101 ( 41%)], Train Loss: 0.56013\n","Epoch: 00 [ 8231/20101 ( 41%)], Train Loss: 0.56019\n","Epoch: 00 [ 8241/20101 ( 41%)], Train Loss: 0.56021\n","Epoch: 00 [ 8251/20101 ( 41%)], Train Loss: 0.56012\n","Epoch: 00 [ 8261/20101 ( 41%)], Train Loss: 0.56012\n","Epoch: 00 [ 8271/20101 ( 41%)], Train Loss: 0.56007\n","Epoch: 00 [ 8281/20101 ( 41%)], Train Loss: 0.56003\n","Epoch: 00 [ 8291/20101 ( 41%)], Train Loss: 0.55982\n","Epoch: 00 [ 8301/20101 ( 41%)], Train Loss: 0.55959\n","Epoch: 00 [ 8311/20101 ( 41%)], Train Loss: 0.55925\n","Epoch: 00 [ 8321/20101 ( 41%)], Train Loss: 0.55921\n","Epoch: 00 [ 8331/20101 ( 41%)], Train Loss: 0.55948\n","Epoch: 00 [ 8341/20101 ( 41%)], Train Loss: 0.55912\n","Epoch: 00 [ 8351/20101 ( 42%)], Train Loss: 0.55865\n","Epoch: 00 [ 8361/20101 ( 42%)], Train Loss: 0.55846\n","Epoch: 00 [ 8371/20101 ( 42%)], Train Loss: 0.55874\n","Epoch: 00 [ 8381/20101 ( 42%)], Train Loss: 0.55867\n","Epoch: 00 [ 8391/20101 ( 42%)], Train Loss: 0.55832\n","Epoch: 00 [ 8401/20101 ( 42%)], Train Loss: 0.55817\n","Epoch: 00 [ 8411/20101 ( 42%)], Train Loss: 0.55796\n","Epoch: 00 [ 8421/20101 ( 42%)], Train Loss: 0.55786\n","Epoch: 00 [ 8431/20101 ( 42%)], Train Loss: 0.55745\n","Epoch: 00 [ 8441/20101 ( 42%)], Train Loss: 0.55728\n","Epoch: 00 [ 8451/20101 ( 42%)], Train Loss: 0.55711\n","Epoch: 00 [ 8461/20101 ( 42%)], Train Loss: 0.55674\n","Epoch: 00 [ 8471/20101 ( 42%)], Train Loss: 0.55624\n","Epoch: 00 [ 8481/20101 ( 42%)], Train Loss: 0.55595\n","Epoch: 00 [ 8491/20101 ( 42%)], Train Loss: 0.55550\n","Epoch: 00 [ 8501/20101 ( 42%)], Train Loss: 0.55523\n","Epoch: 00 [ 8511/20101 ( 42%)], Train Loss: 0.55508\n","Epoch: 00 [ 8521/20101 ( 42%)], Train Loss: 0.55550\n","Epoch: 00 [ 8531/20101 ( 42%)], Train Loss: 0.55509\n","Epoch: 00 [ 8541/20101 ( 42%)], Train Loss: 0.55465\n","Epoch: 00 [ 8551/20101 ( 43%)], Train Loss: 0.55406\n","Epoch: 00 [ 8561/20101 ( 43%)], Train Loss: 0.55361\n","Epoch: 00 [ 8571/20101 ( 43%)], Train Loss: 0.55406\n","Epoch: 00 [ 8581/20101 ( 43%)], Train Loss: 0.55446\n","Epoch: 00 [ 8591/20101 ( 43%)], Train Loss: 0.55479\n","Epoch: 00 [ 8601/20101 ( 43%)], Train Loss: 0.55453\n","Epoch: 00 [ 8611/20101 ( 43%)], Train Loss: 0.55465\n","Epoch: 00 [ 8621/20101 ( 43%)], Train Loss: 0.55462\n","Epoch: 00 [ 8631/20101 ( 43%)], Train Loss: 0.55458\n","Epoch: 00 [ 8641/20101 ( 43%)], Train Loss: 0.55433\n","Epoch: 00 [ 8651/20101 ( 43%)], Train Loss: 0.55447\n","Epoch: 00 [ 8661/20101 ( 43%)], Train Loss: 0.55391\n","Epoch: 00 [ 8671/20101 ( 43%)], Train Loss: 0.55362\n","Epoch: 00 [ 8681/20101 ( 43%)], Train Loss: 0.55329\n","Epoch: 00 [ 8691/20101 ( 43%)], Train Loss: 0.55314\n","Epoch: 00 [ 8701/20101 ( 43%)], Train Loss: 0.55366\n","Epoch: 00 [ 8711/20101 ( 43%)], Train Loss: 0.55369\n","Epoch: 00 [ 8721/20101 ( 43%)], Train Loss: 0.55349\n","Epoch: 00 [ 8731/20101 ( 43%)], Train Loss: 0.55316\n","Epoch: 00 [ 8741/20101 ( 43%)], Train Loss: 0.55278\n","Epoch: 00 [ 8751/20101 ( 44%)], Train Loss: 0.55234\n","Epoch: 00 [ 8761/20101 ( 44%)], Train Loss: 0.55181\n","Epoch: 00 [ 8771/20101 ( 44%)], Train Loss: 0.55158\n","Epoch: 00 [ 8781/20101 ( 44%)], Train Loss: 0.55138\n","Epoch: 00 [ 8791/20101 ( 44%)], Train Loss: 0.55125\n","Epoch: 00 [ 8801/20101 ( 44%)], Train Loss: 0.55101\n","Epoch: 00 [ 8811/20101 ( 44%)], Train Loss: 0.55077\n","Epoch: 00 [ 8821/20101 ( 44%)], Train Loss: 0.55036\n","Epoch: 00 [ 8831/20101 ( 44%)], Train Loss: 0.54989\n","Epoch: 00 [ 8841/20101 ( 44%)], Train Loss: 0.55013\n","Epoch: 00 [ 8851/20101 ( 44%)], Train Loss: 0.54973\n","Epoch: 00 [ 8861/20101 ( 44%)], Train Loss: 0.54940\n","Epoch: 00 [ 8871/20101 ( 44%)], Train Loss: 0.54928\n","Epoch: 00 [ 8881/20101 ( 44%)], Train Loss: 0.54900\n","Epoch: 00 [ 8891/20101 ( 44%)], Train Loss: 0.54842\n","Epoch: 00 [ 8901/20101 ( 44%)], Train Loss: 0.54825\n","Epoch: 00 [ 8911/20101 ( 44%)], Train Loss: 0.54795\n","Epoch: 00 [ 8921/20101 ( 44%)], Train Loss: 0.54840\n","Epoch: 00 [ 8931/20101 ( 44%)], Train Loss: 0.54830\n","Epoch: 00 [ 8941/20101 ( 44%)], Train Loss: 0.54818\n","Epoch: 00 [ 8951/20101 ( 45%)], Train Loss: 0.54778\n","Epoch: 00 [ 8961/20101 ( 45%)], Train Loss: 0.54775\n","Epoch: 00 [ 8971/20101 ( 45%)], Train Loss: 0.54744\n","Epoch: 00 [ 8981/20101 ( 45%)], Train Loss: 0.54728\n","Epoch: 00 [ 8991/20101 ( 45%)], Train Loss: 0.54691\n","Epoch: 00 [ 9001/20101 ( 45%)], Train Loss: 0.54657\n","Epoch: 00 [ 9011/20101 ( 45%)], Train Loss: 0.54669\n","Epoch: 00 [ 9021/20101 ( 45%)], Train Loss: 0.54691\n","Epoch: 00 [ 9031/20101 ( 45%)], Train Loss: 0.54646\n","Epoch: 00 [ 9041/20101 ( 45%)], Train Loss: 0.54612\n","Epoch: 00 [ 9051/20101 ( 45%)], Train Loss: 0.54577\n","Epoch: 00 [ 9061/20101 ( 45%)], Train Loss: 0.54548\n","Epoch: 00 [ 9071/20101 ( 45%)], Train Loss: 0.54533\n","Epoch: 00 [ 9081/20101 ( 45%)], Train Loss: 0.54509\n","Epoch: 00 [ 9091/20101 ( 45%)], Train Loss: 0.54469\n","Epoch: 00 [ 9101/20101 ( 45%)], Train Loss: 0.54469\n","Epoch: 00 [ 9111/20101 ( 45%)], Train Loss: 0.54441\n","Epoch: 00 [ 9121/20101 ( 45%)], Train Loss: 0.54394\n","Epoch: 00 [ 9131/20101 ( 45%)], Train Loss: 0.54435\n","Epoch: 00 [ 9141/20101 ( 45%)], Train Loss: 0.54451\n","Epoch: 00 [ 9151/20101 ( 46%)], Train Loss: 0.54449\n","Epoch: 00 [ 9161/20101 ( 46%)], Train Loss: 0.54494\n","Epoch: 00 [ 9171/20101 ( 46%)], Train Loss: 0.54532\n","Epoch: 00 [ 9181/20101 ( 46%)], Train Loss: 0.54506\n","Epoch: 00 [ 9191/20101 ( 46%)], Train Loss: 0.54507\n","Epoch: 00 [ 9201/20101 ( 46%)], Train Loss: 0.54469\n","Epoch: 00 [ 9211/20101 ( 46%)], Train Loss: 0.54446\n","Epoch: 00 [ 9221/20101 ( 46%)], Train Loss: 0.54422\n","Epoch: 00 [ 9231/20101 ( 46%)], Train Loss: 0.54473\n","Epoch: 00 [ 9241/20101 ( 46%)], Train Loss: 0.54491\n","Epoch: 00 [ 9251/20101 ( 46%)], Train Loss: 0.54512\n","Epoch: 00 [ 9261/20101 ( 46%)], Train Loss: 0.54553\n","Epoch: 00 [ 9271/20101 ( 46%)], Train Loss: 0.54549\n","Epoch: 00 [ 9281/20101 ( 46%)], Train Loss: 0.54543\n","Epoch: 00 [ 9291/20101 ( 46%)], Train Loss: 0.54536\n","Epoch: 00 [ 9301/20101 ( 46%)], Train Loss: 0.54525\n","Epoch: 00 [ 9311/20101 ( 46%)], Train Loss: 0.54526\n","Epoch: 00 [ 9321/20101 ( 46%)], Train Loss: 0.54516\n","Epoch: 00 [ 9331/20101 ( 46%)], Train Loss: 0.54474\n","Epoch: 00 [ 9341/20101 ( 46%)], Train Loss: 0.54428\n","Epoch: 00 [ 9351/20101 ( 47%)], Train Loss: 0.54408\n","Epoch: 00 [ 9361/20101 ( 47%)], Train Loss: 0.54382\n","Epoch: 00 [ 9371/20101 ( 47%)], Train Loss: 0.54355\n","Epoch: 00 [ 9381/20101 ( 47%)], Train Loss: 0.54349\n","Epoch: 00 [ 9391/20101 ( 47%)], Train Loss: 0.54322\n","Epoch: 00 [ 9401/20101 ( 47%)], Train Loss: 0.54295\n","Epoch: 00 [ 9411/20101 ( 47%)], Train Loss: 0.54293\n","Epoch: 00 [ 9421/20101 ( 47%)], Train Loss: 0.54267\n","Epoch: 00 [ 9431/20101 ( 47%)], Train Loss: 0.54289\n","Epoch: 00 [ 9441/20101 ( 47%)], Train Loss: 0.54238\n","Epoch: 00 [ 9451/20101 ( 47%)], Train Loss: 0.54205\n","Epoch: 00 [ 9461/20101 ( 47%)], Train Loss: 0.54206\n","Epoch: 00 [ 9471/20101 ( 47%)], Train Loss: 0.54185\n","Epoch: 00 [ 9481/20101 ( 47%)], Train Loss: 0.54171\n","Epoch: 00 [ 9491/20101 ( 47%)], Train Loss: 0.54147\n","Epoch: 00 [ 9501/20101 ( 47%)], Train Loss: 0.54127\n","Epoch: 00 [ 9511/20101 ( 47%)], Train Loss: 0.54121\n","Epoch: 00 [ 9521/20101 ( 47%)], Train Loss: 0.54138\n","Epoch: 00 [ 9531/20101 ( 47%)], Train Loss: 0.54103\n","Epoch: 00 [ 9541/20101 ( 47%)], Train Loss: 0.54146\n","Epoch: 00 [ 9551/20101 ( 48%)], Train Loss: 0.54106\n","Epoch: 00 [ 9561/20101 ( 48%)], Train Loss: 0.54094\n","Epoch: 00 [ 9571/20101 ( 48%)], Train Loss: 0.54073\n","Epoch: 00 [ 9581/20101 ( 48%)], Train Loss: 0.54067\n","Epoch: 00 [ 9591/20101 ( 48%)], Train Loss: 0.54047\n","Epoch: 00 [ 9601/20101 ( 48%)], Train Loss: 0.54048\n","Epoch: 00 [ 9611/20101 ( 48%)], Train Loss: 0.54029\n","Epoch: 00 [ 9621/20101 ( 48%)], Train Loss: 0.54012\n","Epoch: 00 [ 9631/20101 ( 48%)], Train Loss: 0.53995\n","Epoch: 00 [ 9641/20101 ( 48%)], Train Loss: 0.53978\n","Epoch: 00 [ 9651/20101 ( 48%)], Train Loss: 0.53945\n","Epoch: 00 [ 9661/20101 ( 48%)], Train Loss: 0.53926\n","Epoch: 00 [ 9671/20101 ( 48%)], Train Loss: 0.53892\n","Epoch: 00 [ 9681/20101 ( 48%)], Train Loss: 0.53865\n","Epoch: 00 [ 9691/20101 ( 48%)], Train Loss: 0.53827\n","Epoch: 00 [ 9701/20101 ( 48%)], Train Loss: 0.53811\n","Epoch: 00 [ 9711/20101 ( 48%)], Train Loss: 0.53805\n","Epoch: 00 [ 9721/20101 ( 48%)], Train Loss: 0.53818\n","Epoch: 00 [ 9731/20101 ( 48%)], Train Loss: 0.53815\n","Epoch: 00 [ 9741/20101 ( 48%)], Train Loss: 0.53791\n","Epoch: 00 [ 9751/20101 ( 49%)], Train Loss: 0.53792\n","Epoch: 00 [ 9761/20101 ( 49%)], Train Loss: 0.53770\n","Epoch: 00 [ 9771/20101 ( 49%)], Train Loss: 0.53763\n","Epoch: 00 [ 9781/20101 ( 49%)], Train Loss: 0.53780\n","Epoch: 00 [ 9791/20101 ( 49%)], Train Loss: 0.53751\n","Epoch: 00 [ 9801/20101 ( 49%)], Train Loss: 0.53795\n","Epoch: 00 [ 9811/20101 ( 49%)], Train Loss: 0.53792\n","Epoch: 00 [ 9821/20101 ( 49%)], Train Loss: 0.53801\n","Epoch: 00 [ 9831/20101 ( 49%)], Train Loss: 0.53772\n","Epoch: 00 [ 9841/20101 ( 49%)], Train Loss: 0.53805\n","Epoch: 00 [ 9851/20101 ( 49%)], Train Loss: 0.53792\n","Epoch: 00 [ 9861/20101 ( 49%)], Train Loss: 0.53785\n","Epoch: 00 [ 9871/20101 ( 49%)], Train Loss: 0.53758\n","Epoch: 00 [ 9881/20101 ( 49%)], Train Loss: 0.53711\n","Epoch: 00 [ 9891/20101 ( 49%)], Train Loss: 0.53699\n","Epoch: 00 [ 9901/20101 ( 49%)], Train Loss: 0.53687\n","Epoch: 00 [ 9911/20101 ( 49%)], Train Loss: 0.53652\n","Epoch: 00 [ 9921/20101 ( 49%)], Train Loss: 0.53645\n","Epoch: 00 [ 9931/20101 ( 49%)], Train Loss: 0.53664\n","Epoch: 00 [ 9941/20101 ( 49%)], Train Loss: 0.53674\n","Epoch: 00 [ 9951/20101 ( 50%)], Train Loss: 0.53638\n","Epoch: 00 [ 9961/20101 ( 50%)], Train Loss: 0.53641\n","Epoch: 00 [ 9971/20101 ( 50%)], Train Loss: 0.53611\n","Epoch: 00 [ 9981/20101 ( 50%)], Train Loss: 0.53653\n","Epoch: 00 [ 9991/20101 ( 50%)], Train Loss: 0.53625\n","Epoch: 00 [10001/20101 ( 50%)], Train Loss: 0.53590\n","Epoch: 00 [10011/20101 ( 50%)], Train Loss: 0.53567\n","Epoch: 00 [10021/20101 ( 50%)], Train Loss: 0.53557\n","Epoch: 00 [10031/20101 ( 50%)], Train Loss: 0.53568\n","Epoch: 00 [10041/20101 ( 50%)], Train Loss: 0.53559\n","Epoch: 00 [10051/20101 ( 50%)], Train Loss: 0.53539\n","Epoch: 00 [10061/20101 ( 50%)], Train Loss: 0.53506\n","Epoch: 00 [10071/20101 ( 50%)], Train Loss: 0.53470\n","Epoch: 00 [10081/20101 ( 50%)], Train Loss: 0.53470\n","Epoch: 00 [10091/20101 ( 50%)], Train Loss: 0.53493\n","Epoch: 00 [10101/20101 ( 50%)], Train Loss: 0.53460\n","Epoch: 00 [10111/20101 ( 50%)], Train Loss: 0.53455\n","Epoch: 00 [10121/20101 ( 50%)], Train Loss: 0.53469\n","Epoch: 00 [10131/20101 ( 50%)], Train Loss: 0.53457\n","Epoch: 00 [10141/20101 ( 50%)], Train Loss: 0.53430\n","Epoch: 00 [10151/20101 ( 50%)], Train Loss: 0.53442\n","Epoch: 00 [10161/20101 ( 51%)], Train Loss: 0.53446\n","Epoch: 00 [10171/20101 ( 51%)], Train Loss: 0.53422\n","Epoch: 00 [10181/20101 ( 51%)], Train Loss: 0.53427\n","Epoch: 00 [10191/20101 ( 51%)], Train Loss: 0.53407\n","Epoch: 00 [10201/20101 ( 51%)], Train Loss: 0.53381\n","Epoch: 00 [10211/20101 ( 51%)], Train Loss: 0.53367\n","Epoch: 00 [10221/20101 ( 51%)], Train Loss: 0.53340\n","Epoch: 00 [10231/20101 ( 51%)], Train Loss: 0.53313\n","Epoch: 00 [10241/20101 ( 51%)], Train Loss: 0.53323\n","Epoch: 00 [10251/20101 ( 51%)], Train Loss: 0.53322\n","Epoch: 00 [10261/20101 ( 51%)], Train Loss: 0.53317\n","Epoch: 00 [10271/20101 ( 51%)], Train Loss: 0.53318\n","Epoch: 00 [10281/20101 ( 51%)], Train Loss: 0.53295\n","Epoch: 00 [10291/20101 ( 51%)], Train Loss: 0.53264\n","Epoch: 00 [10301/20101 ( 51%)], Train Loss: 0.53277\n","Epoch: 00 [10311/20101 ( 51%)], Train Loss: 0.53253\n","Epoch: 00 [10321/20101 ( 51%)], Train Loss: 0.53231\n","Epoch: 00 [10331/20101 ( 51%)], Train Loss: 0.53186\n","Epoch: 00 [10341/20101 ( 51%)], Train Loss: 0.53169\n","Epoch: 00 [10351/20101 ( 51%)], Train Loss: 0.53211\n","Epoch: 00 [10361/20101 ( 52%)], Train Loss: 0.53178\n","Epoch: 00 [10371/20101 ( 52%)], Train Loss: 0.53168\n","Epoch: 00 [10381/20101 ( 52%)], Train Loss: 0.53135\n","Epoch: 00 [10391/20101 ( 52%)], Train Loss: 0.53147\n","Epoch: 00 [10401/20101 ( 52%)], Train Loss: 0.53121\n","Epoch: 00 [10411/20101 ( 52%)], Train Loss: 0.53091\n","Epoch: 00 [10421/20101 ( 52%)], Train Loss: 0.53066\n","Epoch: 00 [10431/20101 ( 52%)], Train Loss: 0.53051\n","Epoch: 00 [10441/20101 ( 52%)], Train Loss: 0.53061\n","Epoch: 00 [10451/20101 ( 52%)], Train Loss: 0.53067\n","Epoch: 00 [10461/20101 ( 52%)], Train Loss: 0.53049\n","Epoch: 00 [10471/20101 ( 52%)], Train Loss: 0.53037\n","Epoch: 00 [10481/20101 ( 52%)], Train Loss: 0.53023\n","Epoch: 00 [10491/20101 ( 52%)], Train Loss: 0.53010\n","Epoch: 00 [10501/20101 ( 52%)], Train Loss: 0.53006\n","Epoch: 00 [10511/20101 ( 52%)], Train Loss: 0.52966\n","Epoch: 00 [10521/20101 ( 52%)], Train Loss: 0.52992\n","Epoch: 00 [10531/20101 ( 52%)], Train Loss: 0.52959\n","Epoch: 00 [10541/20101 ( 52%)], Train Loss: 0.52943\n","Epoch: 00 [10551/20101 ( 52%)], Train Loss: 0.52913\n","Epoch: 00 [10561/20101 ( 53%)], Train Loss: 0.52908\n","Epoch: 00 [10571/20101 ( 53%)], Train Loss: 0.52878\n","Epoch: 00 [10581/20101 ( 53%)], Train Loss: 0.52853\n","Epoch: 00 [10591/20101 ( 53%)], Train Loss: 0.52840\n","Epoch: 00 [10601/20101 ( 53%)], Train Loss: 0.52823\n","Epoch: 00 [10611/20101 ( 53%)], Train Loss: 0.52836\n","Epoch: 00 [10621/20101 ( 53%)], Train Loss: 0.52875\n","Epoch: 00 [10631/20101 ( 53%)], Train Loss: 0.52851\n","Epoch: 00 [10641/20101 ( 53%)], Train Loss: 0.52820\n","Epoch: 00 [10651/20101 ( 53%)], Train Loss: 0.52822\n","Epoch: 00 [10661/20101 ( 53%)], Train Loss: 0.52848\n","Epoch: 00 [10671/20101 ( 53%)], Train Loss: 0.52836\n","Epoch: 00 [10681/20101 ( 53%)], Train Loss: 0.52818\n","Epoch: 00 [10691/20101 ( 53%)], Train Loss: 0.52802\n","Epoch: 00 [10701/20101 ( 53%)], Train Loss: 0.52763\n","Epoch: 00 [10711/20101 ( 53%)], Train Loss: 0.52756\n","Epoch: 00 [10721/20101 ( 53%)], Train Loss: 0.52764\n","Epoch: 00 [10731/20101 ( 53%)], Train Loss: 0.52744\n","Epoch: 00 [10741/20101 ( 53%)], Train Loss: 0.52760\n","Epoch: 00 [10751/20101 ( 53%)], Train Loss: 0.52775\n","Epoch: 00 [10761/20101 ( 54%)], Train Loss: 0.52811\n","Epoch: 00 [10771/20101 ( 54%)], Train Loss: 0.52803\n","Epoch: 00 [10781/20101 ( 54%)], Train Loss: 0.52795\n","Epoch: 00 [10791/20101 ( 54%)], Train Loss: 0.52775\n","Epoch: 00 [10801/20101 ( 54%)], Train Loss: 0.52776\n","Epoch: 00 [10811/20101 ( 54%)], Train Loss: 0.52751\n","Epoch: 00 [10821/20101 ( 54%)], Train Loss: 0.52719\n","Epoch: 00 [10831/20101 ( 54%)], Train Loss: 0.52712\n","Epoch: 00 [10841/20101 ( 54%)], Train Loss: 0.52682\n","Epoch: 00 [10851/20101 ( 54%)], Train Loss: 0.52686\n","Epoch: 00 [10861/20101 ( 54%)], Train Loss: 0.52665\n","Epoch: 00 [10871/20101 ( 54%)], Train Loss: 0.52706\n","Epoch: 00 [10881/20101 ( 54%)], Train Loss: 0.52685\n","Epoch: 00 [10891/20101 ( 54%)], Train Loss: 0.52698\n","Epoch: 00 [10901/20101 ( 54%)], Train Loss: 0.52683\n","Epoch: 00 [10911/20101 ( 54%)], Train Loss: 0.52693\n","Epoch: 00 [10921/20101 ( 54%)], Train Loss: 0.52701\n","Epoch: 00 [10931/20101 ( 54%)], Train Loss: 0.52744\n","Epoch: 00 [10941/20101 ( 54%)], Train Loss: 0.52739\n","Epoch: 00 [10951/20101 ( 54%)], Train Loss: 0.52713\n","Epoch: 00 [10961/20101 ( 55%)], Train Loss: 0.52692\n","Epoch: 00 [10971/20101 ( 55%)], Train Loss: 0.52674\n","Epoch: 00 [10981/20101 ( 55%)], Train Loss: 0.52637\n","Epoch: 00 [10991/20101 ( 55%)], Train Loss: 0.52667\n","Epoch: 00 [11001/20101 ( 55%)], Train Loss: 0.52630\n","Epoch: 00 [11011/20101 ( 55%)], Train Loss: 0.52610\n","Epoch: 00 [11021/20101 ( 55%)], Train Loss: 0.52584\n","Epoch: 00 [11031/20101 ( 55%)], Train Loss: 0.52583\n","Epoch: 00 [11041/20101 ( 55%)], Train Loss: 0.52604\n","Epoch: 00 [11051/20101 ( 55%)], Train Loss: 0.52584\n","Epoch: 00 [11061/20101 ( 55%)], Train Loss: 0.52587\n","Epoch: 00 [11071/20101 ( 55%)], Train Loss: 0.52654\n","Epoch: 00 [11081/20101 ( 55%)], Train Loss: 0.52640\n","Epoch: 00 [11091/20101 ( 55%)], Train Loss: 0.52629\n","Epoch: 00 [11101/20101 ( 55%)], Train Loss: 0.52615\n","Epoch: 00 [11111/20101 ( 55%)], Train Loss: 0.52602\n","Epoch: 00 [11121/20101 ( 55%)], Train Loss: 0.52573\n","Epoch: 00 [11131/20101 ( 55%)], Train Loss: 0.52584\n","Epoch: 00 [11141/20101 ( 55%)], Train Loss: 0.52548\n","Epoch: 00 [11151/20101 ( 55%)], Train Loss: 0.52514\n","Epoch: 00 [11161/20101 ( 56%)], Train Loss: 0.52472\n","Epoch: 00 [11171/20101 ( 56%)], Train Loss: 0.52449\n","Epoch: 00 [11181/20101 ( 56%)], Train Loss: 0.52432\n","Epoch: 00 [11191/20101 ( 56%)], Train Loss: 0.52440\n","Epoch: 00 [11201/20101 ( 56%)], Train Loss: 0.52396\n","Epoch: 00 [11211/20101 ( 56%)], Train Loss: 0.52395\n","Epoch: 00 [11221/20101 ( 56%)], Train Loss: 0.52360\n","Epoch: 00 [11231/20101 ( 56%)], Train Loss: 0.52347\n","Epoch: 00 [11241/20101 ( 56%)], Train Loss: 0.52305\n","Epoch: 00 [11251/20101 ( 56%)], Train Loss: 0.52386\n","Epoch: 00 [11261/20101 ( 56%)], Train Loss: 0.52383\n","Epoch: 00 [11271/20101 ( 56%)], Train Loss: 0.52351\n","Epoch: 00 [11281/20101 ( 56%)], Train Loss: 0.52329\n","Epoch: 00 [11291/20101 ( 56%)], Train Loss: 0.52309\n","Epoch: 00 [11301/20101 ( 56%)], Train Loss: 0.52305\n","Epoch: 00 [11311/20101 ( 56%)], Train Loss: 0.52300\n","Epoch: 00 [11321/20101 ( 56%)], Train Loss: 0.52285\n","Epoch: 00 [11331/20101 ( 56%)], Train Loss: 0.52263\n","Epoch: 00 [11341/20101 ( 56%)], Train Loss: 0.52281\n","Epoch: 00 [11351/20101 ( 56%)], Train Loss: 0.52250\n","Epoch: 00 [11361/20101 ( 57%)], Train Loss: 0.52229\n","Epoch: 00 [11371/20101 ( 57%)], Train Loss: 0.52199\n","Epoch: 00 [11381/20101 ( 57%)], Train Loss: 0.52194\n","Epoch: 00 [11391/20101 ( 57%)], Train Loss: 0.52191\n","Epoch: 00 [11401/20101 ( 57%)], Train Loss: 0.52231\n","Epoch: 00 [11411/20101 ( 57%)], Train Loss: 0.52200\n","Epoch: 00 [11421/20101 ( 57%)], Train Loss: 0.52160\n","Epoch: 00 [11431/20101 ( 57%)], Train Loss: 0.52147\n","Epoch: 00 [11441/20101 ( 57%)], Train Loss: 0.52138\n","Epoch: 00 [11451/20101 ( 57%)], Train Loss: 0.52113\n","Epoch: 00 [11461/20101 ( 57%)], Train Loss: 0.52081\n","Epoch: 00 [11471/20101 ( 57%)], Train Loss: 0.52046\n","Epoch: 00 [11481/20101 ( 57%)], Train Loss: 0.52031\n","Epoch: 00 [11491/20101 ( 57%)], Train Loss: 0.52024\n","Epoch: 00 [11501/20101 ( 57%)], Train Loss: 0.52004\n","Epoch: 00 [11511/20101 ( 57%)], Train Loss: 0.51992\n","Epoch: 00 [11521/20101 ( 57%)], Train Loss: 0.51952\n","Epoch: 00 [11531/20101 ( 57%)], Train Loss: 0.51963\n","Epoch: 00 [11541/20101 ( 57%)], Train Loss: 0.51941\n","Epoch: 00 [11551/20101 ( 57%)], Train Loss: 0.51950\n","Epoch: 00 [11561/20101 ( 58%)], Train Loss: 0.51940\n","Epoch: 00 [11571/20101 ( 58%)], Train Loss: 0.51914\n","Epoch: 00 [11581/20101 ( 58%)], Train Loss: 0.51915\n","Epoch: 00 [11591/20101 ( 58%)], Train Loss: 0.51935\n","Epoch: 00 [11601/20101 ( 58%)], Train Loss: 0.51936\n","Epoch: 00 [11611/20101 ( 58%)], Train Loss: 0.51934\n","Epoch: 00 [11621/20101 ( 58%)], Train Loss: 0.51942\n","Epoch: 00 [11631/20101 ( 58%)], Train Loss: 0.51917\n","Epoch: 00 [11641/20101 ( 58%)], Train Loss: 0.51922\n","Epoch: 00 [11651/20101 ( 58%)], Train Loss: 0.51940\n","Epoch: 00 [11661/20101 ( 58%)], Train Loss: 0.51916\n","Epoch: 00 [11671/20101 ( 58%)], Train Loss: 0.51910\n","Epoch: 00 [11681/20101 ( 58%)], Train Loss: 0.51896\n","Epoch: 00 [11691/20101 ( 58%)], Train Loss: 0.51901\n","Epoch: 00 [11701/20101 ( 58%)], Train Loss: 0.51872\n","Epoch: 00 [11711/20101 ( 58%)], Train Loss: 0.51854\n","Epoch: 00 [11721/20101 ( 58%)], Train Loss: 0.51815\n","Epoch: 00 [11731/20101 ( 58%)], Train Loss: 0.51788\n","Epoch: 00 [11741/20101 ( 58%)], Train Loss: 0.51747\n","Epoch: 00 [11751/20101 ( 58%)], Train Loss: 0.51703\n","Epoch: 00 [11761/20101 ( 59%)], Train Loss: 0.51705\n","Epoch: 00 [11771/20101 ( 59%)], Train Loss: 0.51710\n","Epoch: 00 [11781/20101 ( 59%)], Train Loss: 0.51683\n","Epoch: 00 [11791/20101 ( 59%)], Train Loss: 0.51646\n","Epoch: 00 [11801/20101 ( 59%)], Train Loss: 0.51629\n","Epoch: 00 [11811/20101 ( 59%)], Train Loss: 0.51635\n","Epoch: 00 [11821/20101 ( 59%)], Train Loss: 0.51610\n","Epoch: 00 [11831/20101 ( 59%)], Train Loss: 0.51578\n","Epoch: 00 [11841/20101 ( 59%)], Train Loss: 0.51553\n","Epoch: 00 [11851/20101 ( 59%)], Train Loss: 0.51542\n","Epoch: 00 [11861/20101 ( 59%)], Train Loss: 0.51508\n","Epoch: 00 [11871/20101 ( 59%)], Train Loss: 0.51535\n","Epoch: 00 [11881/20101 ( 59%)], Train Loss: 0.51527\n","Epoch: 00 [11891/20101 ( 59%)], Train Loss: 0.51510\n","Epoch: 00 [11901/20101 ( 59%)], Train Loss: 0.51475\n","Epoch: 00 [11911/20101 ( 59%)], Train Loss: 0.51452\n","Epoch: 00 [11921/20101 ( 59%)], Train Loss: 0.51445\n","Epoch: 00 [11931/20101 ( 59%)], Train Loss: 0.51434\n","Epoch: 00 [11941/20101 ( 59%)], Train Loss: 0.51401\n","Epoch: 00 [11951/20101 ( 59%)], Train Loss: 0.51375\n","Epoch: 00 [11961/20101 ( 60%)], Train Loss: 0.51341\n","Epoch: 00 [11971/20101 ( 60%)], Train Loss: 0.51318\n","Epoch: 00 [11981/20101 ( 60%)], Train Loss: 0.51294\n","Epoch: 00 [11991/20101 ( 60%)], Train Loss: 0.51283\n","Epoch: 00 [12001/20101 ( 60%)], Train Loss: 0.51254\n","Epoch: 00 [12011/20101 ( 60%)], Train Loss: 0.51242\n","Epoch: 00 [12021/20101 ( 60%)], Train Loss: 0.51254\n","Epoch: 00 [12031/20101 ( 60%)], Train Loss: 0.51228\n","Epoch: 00 [12041/20101 ( 60%)], Train Loss: 0.51203\n","Epoch: 00 [12051/20101 ( 60%)], Train Loss: 0.51202\n","Epoch: 00 [12061/20101 ( 60%)], Train Loss: 0.51207\n","Epoch: 00 [12071/20101 ( 60%)], Train Loss: 0.51218\n","Epoch: 00 [12081/20101 ( 60%)], Train Loss: 0.51204\n","Epoch: 00 [12091/20101 ( 60%)], Train Loss: 0.51184\n","Epoch: 00 [12101/20101 ( 60%)], Train Loss: 0.51166\n","Epoch: 00 [12111/20101 ( 60%)], Train Loss: 0.51142\n","Epoch: 00 [12121/20101 ( 60%)], Train Loss: 0.51127\n","Epoch: 00 [12131/20101 ( 60%)], Train Loss: 0.51107\n","Epoch: 00 [12141/20101 ( 60%)], Train Loss: 0.51093\n","Epoch: 00 [12151/20101 ( 60%)], Train Loss: 0.51056\n","Epoch: 00 [12161/20101 ( 60%)], Train Loss: 0.51031\n","Epoch: 00 [12171/20101 ( 61%)], Train Loss: 0.51010\n","Epoch: 00 [12181/20101 ( 61%)], Train Loss: 0.50991\n","Epoch: 00 [12191/20101 ( 61%)], Train Loss: 0.50969\n","Epoch: 00 [12201/20101 ( 61%)], Train Loss: 0.50950\n","Epoch: 00 [12211/20101 ( 61%)], Train Loss: 0.50927\n","Epoch: 00 [12221/20101 ( 61%)], Train Loss: 0.50940\n","Epoch: 00 [12231/20101 ( 61%)], Train Loss: 0.50905\n","Epoch: 00 [12241/20101 ( 61%)], Train Loss: 0.50974\n","Epoch: 00 [12251/20101 ( 61%)], Train Loss: 0.50995\n","Epoch: 00 [12261/20101 ( 61%)], Train Loss: 0.50993\n","Epoch: 00 [12271/20101 ( 61%)], Train Loss: 0.50972\n","Epoch: 00 [12281/20101 ( 61%)], Train Loss: 0.51003\n","Epoch: 00 [12291/20101 ( 61%)], Train Loss: 0.51030\n","Epoch: 00 [12301/20101 ( 61%)], Train Loss: 0.51023\n","Epoch: 00 [12311/20101 ( 61%)], Train Loss: 0.51007\n","Epoch: 00 [12321/20101 ( 61%)], Train Loss: 0.50995\n","Epoch: 00 [12331/20101 ( 61%)], Train Loss: 0.50978\n","Epoch: 00 [12341/20101 ( 61%)], Train Loss: 0.51008\n","Epoch: 00 [12351/20101 ( 61%)], Train Loss: 0.50992\n","Epoch: 00 [12361/20101 ( 61%)], Train Loss: 0.51016\n","Epoch: 00 [12371/20101 ( 62%)], Train Loss: 0.51002\n","Epoch: 00 [12381/20101 ( 62%)], Train Loss: 0.50986\n","Epoch: 00 [12391/20101 ( 62%)], Train Loss: 0.50962\n","Epoch: 00 [12401/20101 ( 62%)], Train Loss: 0.50955\n","Epoch: 00 [12411/20101 ( 62%)], Train Loss: 0.50951\n","Epoch: 00 [12421/20101 ( 62%)], Train Loss: 0.50942\n","Epoch: 00 [12431/20101 ( 62%)], Train Loss: 0.50928\n","Epoch: 00 [12441/20101 ( 62%)], Train Loss: 0.50908\n","Epoch: 00 [12451/20101 ( 62%)], Train Loss: 0.50895\n","Epoch: 00 [12461/20101 ( 62%)], Train Loss: 0.50875\n","Epoch: 00 [12471/20101 ( 62%)], Train Loss: 0.50884\n","Epoch: 00 [12481/20101 ( 62%)], Train Loss: 0.50851\n","Epoch: 00 [12491/20101 ( 62%)], Train Loss: 0.50870\n","Epoch: 00 [12501/20101 ( 62%)], Train Loss: 0.50889\n","Epoch: 00 [12511/20101 ( 62%)], Train Loss: 0.50884\n","Epoch: 00 [12521/20101 ( 62%)], Train Loss: 0.50865\n","Epoch: 00 [12531/20101 ( 62%)], Train Loss: 0.50832\n","Epoch: 00 [12541/20101 ( 62%)], Train Loss: 0.50808\n","Epoch: 00 [12551/20101 ( 62%)], Train Loss: 0.50798\n","Epoch: 00 [12561/20101 ( 62%)], Train Loss: 0.50789\n","Epoch: 00 [12571/20101 ( 63%)], Train Loss: 0.50790\n","Epoch: 00 [12581/20101 ( 63%)], Train Loss: 0.50815\n","Epoch: 00 [12591/20101 ( 63%)], Train Loss: 0.50802\n","Epoch: 00 [12601/20101 ( 63%)], Train Loss: 0.50801\n","Epoch: 00 [12611/20101 ( 63%)], Train Loss: 0.50775\n","Epoch: 00 [12621/20101 ( 63%)], Train Loss: 0.50745\n","Epoch: 00 [12631/20101 ( 63%)], Train Loss: 0.50727\n","Epoch: 00 [12641/20101 ( 63%)], Train Loss: 0.50747\n","Epoch: 00 [12651/20101 ( 63%)], Train Loss: 0.50726\n","Epoch: 00 [12661/20101 ( 63%)], Train Loss: 0.50722\n","Epoch: 00 [12671/20101 ( 63%)], Train Loss: 0.50728\n","Epoch: 00 [12681/20101 ( 63%)], Train Loss: 0.50730\n","Epoch: 00 [12691/20101 ( 63%)], Train Loss: 0.50710\n","Epoch: 00 [12701/20101 ( 63%)], Train Loss: 0.50739\n","Epoch: 00 [12711/20101 ( 63%)], Train Loss: 0.50735\n","Epoch: 00 [12721/20101 ( 63%)], Train Loss: 0.50744\n","Epoch: 00 [12731/20101 ( 63%)], Train Loss: 0.50727\n","Epoch: 00 [12741/20101 ( 63%)], Train Loss: 0.50705\n","Epoch: 00 [12751/20101 ( 63%)], Train Loss: 0.50702\n","Epoch: 00 [12761/20101 ( 63%)], Train Loss: 0.50680\n","Epoch: 00 [12771/20101 ( 64%)], Train Loss: 0.50666\n","Epoch: 00 [12781/20101 ( 64%)], Train Loss: 0.50698\n","Epoch: 00 [12791/20101 ( 64%)], Train Loss: 0.50715\n","Epoch: 00 [12801/20101 ( 64%)], Train Loss: 0.50718\n","Epoch: 00 [12811/20101 ( 64%)], Train Loss: 0.50704\n","Epoch: 00 [12821/20101 ( 64%)], Train Loss: 0.50699\n","Epoch: 00 [12831/20101 ( 64%)], Train Loss: 0.50680\n","Epoch: 00 [12841/20101 ( 64%)], Train Loss: 0.50680\n","Epoch: 00 [12851/20101 ( 64%)], Train Loss: 0.50658\n","Epoch: 00 [12861/20101 ( 64%)], Train Loss: 0.50640\n","Epoch: 00 [12871/20101 ( 64%)], Train Loss: 0.50638\n","Epoch: 00 [12881/20101 ( 64%)], Train Loss: 0.50642\n","Epoch: 00 [12891/20101 ( 64%)], Train Loss: 0.50635\n","Epoch: 00 [12901/20101 ( 64%)], Train Loss: 0.50629\n","Epoch: 00 [12911/20101 ( 64%)], Train Loss: 0.50611\n","Epoch: 00 [12921/20101 ( 64%)], Train Loss: 0.50576\n","Epoch: 00 [12931/20101 ( 64%)], Train Loss: 0.50539\n","Epoch: 00 [12941/20101 ( 64%)], Train Loss: 0.50528\n","Epoch: 00 [12951/20101 ( 64%)], Train Loss: 0.50504\n","Epoch: 00 [12961/20101 ( 64%)], Train Loss: 0.50500\n","Epoch: 00 [12971/20101 ( 65%)], Train Loss: 0.50462\n","Epoch: 00 [12981/20101 ( 65%)], Train Loss: 0.50471\n","Epoch: 00 [12991/20101 ( 65%)], Train Loss: 0.50466\n","Epoch: 00 [13001/20101 ( 65%)], Train Loss: 0.50445\n","Epoch: 00 [13011/20101 ( 65%)], Train Loss: 0.50423\n","Epoch: 00 [13021/20101 ( 65%)], Train Loss: 0.50427\n","Epoch: 00 [13031/20101 ( 65%)], Train Loss: 0.50392\n","Epoch: 00 [13041/20101 ( 65%)], Train Loss: 0.50362\n","Epoch: 00 [13051/20101 ( 65%)], Train Loss: 0.50347\n","Epoch: 00 [13061/20101 ( 65%)], Train Loss: 0.50353\n","Epoch: 00 [13071/20101 ( 65%)], Train Loss: 0.50344\n","Epoch: 00 [13081/20101 ( 65%)], Train Loss: 0.50321\n","Epoch: 00 [13091/20101 ( 65%)], Train Loss: 0.50303\n","Epoch: 00 [13101/20101 ( 65%)], Train Loss: 0.50308\n","Epoch: 00 [13111/20101 ( 65%)], Train Loss: 0.50300\n","Epoch: 00 [13121/20101 ( 65%)], Train Loss: 0.50291\n","Epoch: 00 [13131/20101 ( 65%)], Train Loss: 0.50307\n","Epoch: 00 [13141/20101 ( 65%)], Train Loss: 0.50340\n","Epoch: 00 [13151/20101 ( 65%)], Train Loss: 0.50319\n","Epoch: 00 [13161/20101 ( 65%)], Train Loss: 0.50289\n","Epoch: 00 [13171/20101 ( 66%)], Train Loss: 0.50276\n","Epoch: 00 [13181/20101 ( 66%)], Train Loss: 0.50283\n","Epoch: 00 [13191/20101 ( 66%)], Train Loss: 0.50256\n","Epoch: 00 [13201/20101 ( 66%)], Train Loss: 0.50222\n","Epoch: 00 [13211/20101 ( 66%)], Train Loss: 0.50195\n","Epoch: 00 [13221/20101 ( 66%)], Train Loss: 0.50205\n","Epoch: 00 [13231/20101 ( 66%)], Train Loss: 0.50209\n","Epoch: 00 [13241/20101 ( 66%)], Train Loss: 0.50209\n","Epoch: 00 [13251/20101 ( 66%)], Train Loss: 0.50204\n","Epoch: 00 [13261/20101 ( 66%)], Train Loss: 0.50217\n","Epoch: 00 [13271/20101 ( 66%)], Train Loss: 0.50211\n","Epoch: 00 [13281/20101 ( 66%)], Train Loss: 0.50205\n","Epoch: 00 [13291/20101 ( 66%)], Train Loss: 0.50204\n","Epoch: 00 [13301/20101 ( 66%)], Train Loss: 0.50257\n","Epoch: 00 [13311/20101 ( 66%)], Train Loss: 0.50233\n","Epoch: 00 [13321/20101 ( 66%)], Train Loss: 0.50217\n","Epoch: 00 [13331/20101 ( 66%)], Train Loss: 0.50256\n","Epoch: 00 [13341/20101 ( 66%)], Train Loss: 0.50235\n","Epoch: 00 [13351/20101 ( 66%)], Train Loss: 0.50252\n","Epoch: 00 [13361/20101 ( 66%)], Train Loss: 0.50246\n","Epoch: 00 [13371/20101 ( 67%)], Train Loss: 0.50276\n","Epoch: 00 [13381/20101 ( 67%)], Train Loss: 0.50300\n","Epoch: 00 [13391/20101 ( 67%)], Train Loss: 0.50283\n","Epoch: 00 [13401/20101 ( 67%)], Train Loss: 0.50269\n","Epoch: 00 [13411/20101 ( 67%)], Train Loss: 0.50245\n","Epoch: 00 [13421/20101 ( 67%)], Train Loss: 0.50240\n","Epoch: 00 [13431/20101 ( 67%)], Train Loss: 0.50218\n","Epoch: 00 [13441/20101 ( 67%)], Train Loss: 0.50216\n","Epoch: 00 [13451/20101 ( 67%)], Train Loss: 0.50259\n","Epoch: 00 [13461/20101 ( 67%)], Train Loss: 0.50260\n","Epoch: 00 [13471/20101 ( 67%)], Train Loss: 0.50234\n","Epoch: 00 [13481/20101 ( 67%)], Train Loss: 0.50207\n","Epoch: 00 [13491/20101 ( 67%)], Train Loss: 0.50208\n","Epoch: 00 [13501/20101 ( 67%)], Train Loss: 0.50200\n","Epoch: 00 [13511/20101 ( 67%)], Train Loss: 0.50216\n","Epoch: 00 [13521/20101 ( 67%)], Train Loss: 0.50179\n","Epoch: 00 [13531/20101 ( 67%)], Train Loss: 0.50162\n","Epoch: 00 [13541/20101 ( 67%)], Train Loss: 0.50137\n","Epoch: 00 [13551/20101 ( 67%)], Train Loss: 0.50127\n","Epoch: 00 [13561/20101 ( 67%)], Train Loss: 0.50120\n","Epoch: 00 [13571/20101 ( 68%)], Train Loss: 0.50107\n","Epoch: 00 [13581/20101 ( 68%)], Train Loss: 0.50084\n","Epoch: 00 [13591/20101 ( 68%)], Train Loss: 0.50067\n","Epoch: 00 [13601/20101 ( 68%)], Train Loss: 0.50056\n","Epoch: 00 [13611/20101 ( 68%)], Train Loss: 0.50034\n","Epoch: 00 [13621/20101 ( 68%)], Train Loss: 0.50000\n","Epoch: 00 [13631/20101 ( 68%)], Train Loss: 0.49995\n","Epoch: 00 [13641/20101 ( 68%)], Train Loss: 0.49981\n","Epoch: 00 [13651/20101 ( 68%)], Train Loss: 0.49954\n","Epoch: 00 [13661/20101 ( 68%)], Train Loss: 0.49942\n","Epoch: 00 [13671/20101 ( 68%)], Train Loss: 0.49946\n","Epoch: 00 [13681/20101 ( 68%)], Train Loss: 0.49946\n","Epoch: 00 [13691/20101 ( 68%)], Train Loss: 0.49921\n","Epoch: 00 [13701/20101 ( 68%)], Train Loss: 0.49917\n","Epoch: 00 [13711/20101 ( 68%)], Train Loss: 0.49893\n","Epoch: 00 [13721/20101 ( 68%)], Train Loss: 0.49890\n","Epoch: 00 [13731/20101 ( 68%)], Train Loss: 0.49921\n","Epoch: 00 [13741/20101 ( 68%)], Train Loss: 0.49936\n","Epoch: 00 [13751/20101 ( 68%)], Train Loss: 0.49914\n","Epoch: 00 [13761/20101 ( 68%)], Train Loss: 0.49896\n","Epoch: 00 [13771/20101 ( 69%)], Train Loss: 0.49870\n","Epoch: 00 [13781/20101 ( 69%)], Train Loss: 0.49861\n","Epoch: 00 [13791/20101 ( 69%)], Train Loss: 0.49862\n","Epoch: 00 [13801/20101 ( 69%)], Train Loss: 0.49879\n","Epoch: 00 [13811/20101 ( 69%)], Train Loss: 0.49854\n","Epoch: 00 [13821/20101 ( 69%)], Train Loss: 0.49823\n","Epoch: 00 [13831/20101 ( 69%)], Train Loss: 0.49805\n","Epoch: 00 [13841/20101 ( 69%)], Train Loss: 0.49818\n","Epoch: 00 [13851/20101 ( 69%)], Train Loss: 0.49797\n","Epoch: 00 [13861/20101 ( 69%)], Train Loss: 0.49775\n","Epoch: 00 [13871/20101 ( 69%)], Train Loss: 0.49801\n","Epoch: 00 [13881/20101 ( 69%)], Train Loss: 0.49803\n","Epoch: 00 [13891/20101 ( 69%)], Train Loss: 0.49818\n","Epoch: 00 [13901/20101 ( 69%)], Train Loss: 0.49803\n","Epoch: 00 [13911/20101 ( 69%)], Train Loss: 0.49785\n","Epoch: 00 [13921/20101 ( 69%)], Train Loss: 0.49762\n","Epoch: 00 [13931/20101 ( 69%)], Train Loss: 0.49736\n","Epoch: 00 [13941/20101 ( 69%)], Train Loss: 0.49718\n","Epoch: 00 [13951/20101 ( 69%)], Train Loss: 0.49717\n","Epoch: 00 [13961/20101 ( 69%)], Train Loss: 0.49695\n","Epoch: 00 [13971/20101 ( 70%)], Train Loss: 0.49670\n","Epoch: 00 [13981/20101 ( 70%)], Train Loss: 0.49655\n","Epoch: 00 [13991/20101 ( 70%)], Train Loss: 0.49650\n","Epoch: 00 [14001/20101 ( 70%)], Train Loss: 0.49623\n","Epoch: 00 [14011/20101 ( 70%)], Train Loss: 0.49601\n","Epoch: 00 [14021/20101 ( 70%)], Train Loss: 0.49593\n","Epoch: 00 [14031/20101 ( 70%)], Train Loss: 0.49580\n","Epoch: 00 [14041/20101 ( 70%)], Train Loss: 0.49549\n","Epoch: 00 [14051/20101 ( 70%)], Train Loss: 0.49544\n","Epoch: 00 [14061/20101 ( 70%)], Train Loss: 0.49512\n","Epoch: 00 [14071/20101 ( 70%)], Train Loss: 0.49500\n","Epoch: 00 [14081/20101 ( 70%)], Train Loss: 0.49502\n","Epoch: 00 [14091/20101 ( 70%)], Train Loss: 0.49486\n","Epoch: 00 [14101/20101 ( 70%)], Train Loss: 0.49472\n","Epoch: 00 [14111/20101 ( 70%)], Train Loss: 0.49478\n","Epoch: 00 [14121/20101 ( 70%)], Train Loss: 0.49459\n","Epoch: 00 [14131/20101 ( 70%)], Train Loss: 0.49431\n","Epoch: 00 [14141/20101 ( 70%)], Train Loss: 0.49447\n","Epoch: 00 [14151/20101 ( 70%)], Train Loss: 0.49449\n","Epoch: 00 [14161/20101 ( 70%)], Train Loss: 0.49453\n","Epoch: 00 [14171/20101 ( 70%)], Train Loss: 0.49445\n","Epoch: 00 [14181/20101 ( 71%)], Train Loss: 0.49442\n","Epoch: 00 [14191/20101 ( 71%)], Train Loss: 0.49421\n","Epoch: 00 [14201/20101 ( 71%)], Train Loss: 0.49408\n","Epoch: 00 [14211/20101 ( 71%)], Train Loss: 0.49395\n","Epoch: 00 [14221/20101 ( 71%)], Train Loss: 0.49394\n","Epoch: 00 [14231/20101 ( 71%)], Train Loss: 0.49368\n","Epoch: 00 [14241/20101 ( 71%)], Train Loss: 0.49350\n","Epoch: 00 [14251/20101 ( 71%)], Train Loss: 0.49334\n","Epoch: 00 [14261/20101 ( 71%)], Train Loss: 0.49315\n","Epoch: 00 [14271/20101 ( 71%)], Train Loss: 0.49303\n","Epoch: 00 [14281/20101 ( 71%)], Train Loss: 0.49335\n","Epoch: 00 [14291/20101 ( 71%)], Train Loss: 0.49319\n","Epoch: 00 [14301/20101 ( 71%)], Train Loss: 0.49327\n","Epoch: 00 [14311/20101 ( 71%)], Train Loss: 0.49327\n","Epoch: 00 [14321/20101 ( 71%)], Train Loss: 0.49319\n","Epoch: 00 [14331/20101 ( 71%)], Train Loss: 0.49299\n","Epoch: 00 [14341/20101 ( 71%)], Train Loss: 0.49269\n","Epoch: 00 [14351/20101 ( 71%)], Train Loss: 0.49297\n","Epoch: 00 [14361/20101 ( 71%)], Train Loss: 0.49280\n","Epoch: 00 [14371/20101 ( 71%)], Train Loss: 0.49283\n","Epoch: 00 [14381/20101 ( 72%)], Train Loss: 0.49283\n","Epoch: 00 [14391/20101 ( 72%)], Train Loss: 0.49270\n","Epoch: 00 [14401/20101 ( 72%)], Train Loss: 0.49250\n","Epoch: 00 [14411/20101 ( 72%)], Train Loss: 0.49306\n","Epoch: 00 [14421/20101 ( 72%)], Train Loss: 0.49306\n","Epoch: 00 [14431/20101 ( 72%)], Train Loss: 0.49319\n","Epoch: 00 [14441/20101 ( 72%)], Train Loss: 0.49316\n","Epoch: 00 [14451/20101 ( 72%)], Train Loss: 0.49314\n","Epoch: 00 [14461/20101 ( 72%)], Train Loss: 0.49300\n","Epoch: 00 [14471/20101 ( 72%)], Train Loss: 0.49292\n","Epoch: 00 [14481/20101 ( 72%)], Train Loss: 0.49263\n","Epoch: 00 [14491/20101 ( 72%)], Train Loss: 0.49250\n","Epoch: 00 [14501/20101 ( 72%)], Train Loss: 0.49229\n","Epoch: 00 [14511/20101 ( 72%)], Train Loss: 0.49220\n","Epoch: 00 [14521/20101 ( 72%)], Train Loss: 0.49215\n","Epoch: 00 [14531/20101 ( 72%)], Train Loss: 0.49182\n","Epoch: 00 [14541/20101 ( 72%)], Train Loss: 0.49180\n","Epoch: 00 [14551/20101 ( 72%)], Train Loss: 0.49169\n","Epoch: 00 [14561/20101 ( 72%)], Train Loss: 0.49148\n","Epoch: 00 [14571/20101 ( 72%)], Train Loss: 0.49170\n","Epoch: 00 [14581/20101 ( 73%)], Train Loss: 0.49158\n","Epoch: 00 [14591/20101 ( 73%)], Train Loss: 0.49141\n","Epoch: 00 [14601/20101 ( 73%)], Train Loss: 0.49174\n","Epoch: 00 [14611/20101 ( 73%)], Train Loss: 0.49170\n","Epoch: 00 [14621/20101 ( 73%)], Train Loss: 0.49158\n","Epoch: 00 [14631/20101 ( 73%)], Train Loss: 0.49141\n","Epoch: 00 [14641/20101 ( 73%)], Train Loss: 0.49145\n","Epoch: 00 [14651/20101 ( 73%)], Train Loss: 0.49125\n","Epoch: 00 [14661/20101 ( 73%)], Train Loss: 0.49151\n","Epoch: 00 [14671/20101 ( 73%)], Train Loss: 0.49119\n","Epoch: 00 [14681/20101 ( 73%)], Train Loss: 0.49115\n","Epoch: 00 [14691/20101 ( 73%)], Train Loss: 0.49136\n","Epoch: 00 [14701/20101 ( 73%)], Train Loss: 0.49167\n","Epoch: 00 [14711/20101 ( 73%)], Train Loss: 0.49154\n","Epoch: 00 [14721/20101 ( 73%)], Train Loss: 0.49123\n","Epoch: 00 [14731/20101 ( 73%)], Train Loss: 0.49113\n","Epoch: 00 [14741/20101 ( 73%)], Train Loss: 0.49103\n","Epoch: 00 [14751/20101 ( 73%)], Train Loss: 0.49097\n","Epoch: 00 [14761/20101 ( 73%)], Train Loss: 0.49098\n","Epoch: 00 [14771/20101 ( 73%)], Train Loss: 0.49089\n","Epoch: 00 [14781/20101 ( 74%)], Train Loss: 0.49084\n","Epoch: 00 [14791/20101 ( 74%)], Train Loss: 0.49092\n","Epoch: 00 [14801/20101 ( 74%)], Train Loss: 0.49068\n","Epoch: 00 [14811/20101 ( 74%)], Train Loss: 0.49072\n","Epoch: 00 [14821/20101 ( 74%)], Train Loss: 0.49075\n","Epoch: 00 [14831/20101 ( 74%)], Train Loss: 0.49068\n","Epoch: 00 [14841/20101 ( 74%)], Train Loss: 0.49094\n","Epoch: 00 [14851/20101 ( 74%)], Train Loss: 0.49127\n","Epoch: 00 [14861/20101 ( 74%)], Train Loss: 0.49131\n","Epoch: 00 [14871/20101 ( 74%)], Train Loss: 0.49144\n","Epoch: 00 [14881/20101 ( 74%)], Train Loss: 0.49155\n","Epoch: 00 [14891/20101 ( 74%)], Train Loss: 0.49156\n","Epoch: 00 [14901/20101 ( 74%)], Train Loss: 0.49150\n","Epoch: 00 [14911/20101 ( 74%)], Train Loss: 0.49154\n","Epoch: 00 [14921/20101 ( 74%)], Train Loss: 0.49171\n","Epoch: 00 [14931/20101 ( 74%)], Train Loss: 0.49164\n","Epoch: 00 [14941/20101 ( 74%)], Train Loss: 0.49156\n","Epoch: 00 [14951/20101 ( 74%)], Train Loss: 0.49152\n","Epoch: 00 [14961/20101 ( 74%)], Train Loss: 0.49127\n","Epoch: 00 [14971/20101 ( 74%)], Train Loss: 0.49105\n","Epoch: 00 [14981/20101 ( 75%)], Train Loss: 0.49091\n","Epoch: 00 [14991/20101 ( 75%)], Train Loss: 0.49094\n","Epoch: 00 [15001/20101 ( 75%)], Train Loss: 0.49070\n","Epoch: 00 [15011/20101 ( 75%)], Train Loss: 0.49081\n","Epoch: 00 [15021/20101 ( 75%)], Train Loss: 0.49078\n","Epoch: 00 [15031/20101 ( 75%)], Train Loss: 0.49064\n","Epoch: 00 [15041/20101 ( 75%)], Train Loss: 0.49055\n","Epoch: 00 [15051/20101 ( 75%)], Train Loss: 0.49040\n","Epoch: 00 [15061/20101 ( 75%)], Train Loss: 0.49023\n","Epoch: 00 [15071/20101 ( 75%)], Train Loss: 0.49001\n","Epoch: 00 [15081/20101 ( 75%)], Train Loss: 0.48982\n","Epoch: 00 [15091/20101 ( 75%)], Train Loss: 0.48966\n","Epoch: 00 [15101/20101 ( 75%)], Train Loss: 0.48962\n","Epoch: 00 [15111/20101 ( 75%)], Train Loss: 0.48941\n","Epoch: 00 [15121/20101 ( 75%)], Train Loss: 0.48941\n","Epoch: 00 [15131/20101 ( 75%)], Train Loss: 0.48913\n","Epoch: 00 [15141/20101 ( 75%)], Train Loss: 0.48885\n","Epoch: 00 [15151/20101 ( 75%)], Train Loss: 0.48859\n","Epoch: 00 [15161/20101 ( 75%)], Train Loss: 0.48834\n","Epoch: 00 [15171/20101 ( 75%)], Train Loss: 0.48829\n","Epoch: 00 [15181/20101 ( 76%)], Train Loss: 0.48843\n","Epoch: 00 [15191/20101 ( 76%)], Train Loss: 0.48822\n","Epoch: 00 [15201/20101 ( 76%)], Train Loss: 0.48831\n","Epoch: 00 [15211/20101 ( 76%)], Train Loss: 0.48830\n","Epoch: 00 [15221/20101 ( 76%)], Train Loss: 0.48827\n","Epoch: 00 [15231/20101 ( 76%)], Train Loss: 0.48820\n","Epoch: 00 [15241/20101 ( 76%)], Train Loss: 0.48800\n","Epoch: 00 [15251/20101 ( 76%)], Train Loss: 0.48795\n","Epoch: 00 [15261/20101 ( 76%)], Train Loss: 0.48797\n","Epoch: 00 [15271/20101 ( 76%)], Train Loss: 0.48767\n","Epoch: 00 [15281/20101 ( 76%)], Train Loss: 0.48760\n","Epoch: 00 [15291/20101 ( 76%)], Train Loss: 0.48742\n","Epoch: 00 [15301/20101 ( 76%)], Train Loss: 0.48722\n","Epoch: 00 [15311/20101 ( 76%)], Train Loss: 0.48713\n","Epoch: 00 [15321/20101 ( 76%)], Train Loss: 0.48714\n","Epoch: 00 [15331/20101 ( 76%)], Train Loss: 0.48696\n","Epoch: 00 [15341/20101 ( 76%)], Train Loss: 0.48715\n","Epoch: 00 [15351/20101 ( 76%)], Train Loss: 0.48722\n","Epoch: 00 [15361/20101 ( 76%)], Train Loss: 0.48701\n","Epoch: 00 [15371/20101 ( 76%)], Train Loss: 0.48703\n","Epoch: 00 [15381/20101 ( 77%)], Train Loss: 0.48679\n","Epoch: 00 [15391/20101 ( 77%)], Train Loss: 0.48703\n","Epoch: 00 [15401/20101 ( 77%)], Train Loss: 0.48680\n","Epoch: 00 [15411/20101 ( 77%)], Train Loss: 0.48665\n","Epoch: 00 [15421/20101 ( 77%)], Train Loss: 0.48675\n","Epoch: 00 [15431/20101 ( 77%)], Train Loss: 0.48678\n","Epoch: 00 [15441/20101 ( 77%)], Train Loss: 0.48674\n","Epoch: 00 [15451/20101 ( 77%)], Train Loss: 0.48661\n","Epoch: 00 [15461/20101 ( 77%)], Train Loss: 0.48635\n","Epoch: 00 [15471/20101 ( 77%)], Train Loss: 0.48619\n","Epoch: 00 [15481/20101 ( 77%)], Train Loss: 0.48631\n","Epoch: 00 [15491/20101 ( 77%)], Train Loss: 0.48627\n","Epoch: 00 [15501/20101 ( 77%)], Train Loss: 0.48626\n","Epoch: 00 [15511/20101 ( 77%)], Train Loss: 0.48600\n","Epoch: 00 [15521/20101 ( 77%)], Train Loss: 0.48593\n","Epoch: 00 [15531/20101 ( 77%)], Train Loss: 0.48583\n","Epoch: 00 [15541/20101 ( 77%)], Train Loss: 0.48592\n","Epoch: 00 [15551/20101 ( 77%)], Train Loss: 0.48598\n","Epoch: 00 [15561/20101 ( 77%)], Train Loss: 0.48594\n","Epoch: 00 [15571/20101 ( 77%)], Train Loss: 0.48589\n","Epoch: 00 [15581/20101 ( 78%)], Train Loss: 0.48577\n","Epoch: 00 [15591/20101 ( 78%)], Train Loss: 0.48554\n","Epoch: 00 [15601/20101 ( 78%)], Train Loss: 0.48554\n","Epoch: 00 [15611/20101 ( 78%)], Train Loss: 0.48534\n","Epoch: 00 [15621/20101 ( 78%)], Train Loss: 0.48517\n","Epoch: 00 [15631/20101 ( 78%)], Train Loss: 0.48496\n","Epoch: 00 [15641/20101 ( 78%)], Train Loss: 0.48477\n","Epoch: 00 [15651/20101 ( 78%)], Train Loss: 0.48478\n","Epoch: 00 [15661/20101 ( 78%)], Train Loss: 0.48453\n","Epoch: 00 [15671/20101 ( 78%)], Train Loss: 0.48426\n","Epoch: 00 [15681/20101 ( 78%)], Train Loss: 0.48414\n","Epoch: 00 [15691/20101 ( 78%)], Train Loss: 0.48408\n","Epoch: 00 [15701/20101 ( 78%)], Train Loss: 0.48403\n","Epoch: 00 [15711/20101 ( 78%)], Train Loss: 0.48399\n","Epoch: 00 [15721/20101 ( 78%)], Train Loss: 0.48398\n","Epoch: 00 [15731/20101 ( 78%)], Train Loss: 0.48415\n","Epoch: 00 [15741/20101 ( 78%)], Train Loss: 0.48409\n","Epoch: 00 [15751/20101 ( 78%)], Train Loss: 0.48410\n","Epoch: 00 [15761/20101 ( 78%)], Train Loss: 0.48382\n","Epoch: 00 [15771/20101 ( 78%)], Train Loss: 0.48364\n","Epoch: 00 [15781/20101 ( 79%)], Train Loss: 0.48353\n","Epoch: 00 [15791/20101 ( 79%)], Train Loss: 0.48343\n","Epoch: 00 [15801/20101 ( 79%)], Train Loss: 0.48321\n","Epoch: 00 [15811/20101 ( 79%)], Train Loss: 0.48298\n","Epoch: 00 [15821/20101 ( 79%)], Train Loss: 0.48283\n","Epoch: 00 [15831/20101 ( 79%)], Train Loss: 0.48271\n","Epoch: 00 [15841/20101 ( 79%)], Train Loss: 0.48260\n","Epoch: 00 [15851/20101 ( 79%)], Train Loss: 0.48254\n","Epoch: 00 [15861/20101 ( 79%)], Train Loss: 0.48254\n","Epoch: 00 [15871/20101 ( 79%)], Train Loss: 0.48233\n","Epoch: 00 [15881/20101 ( 79%)], Train Loss: 0.48229\n","Epoch: 00 [15891/20101 ( 79%)], Train Loss: 0.48202\n","Epoch: 00 [15901/20101 ( 79%)], Train Loss: 0.48210\n","Epoch: 00 [15911/20101 ( 79%)], Train Loss: 0.48214\n","Epoch: 00 [15921/20101 ( 79%)], Train Loss: 0.48194\n","Epoch: 00 [15931/20101 ( 79%)], Train Loss: 0.48199\n","Epoch: 00 [15941/20101 ( 79%)], Train Loss: 0.48186\n","Epoch: 00 [15951/20101 ( 79%)], Train Loss: 0.48192\n","Epoch: 00 [15961/20101 ( 79%)], Train Loss: 0.48178\n","Epoch: 00 [15971/20101 ( 79%)], Train Loss: 0.48154\n","Epoch: 00 [15981/20101 ( 80%)], Train Loss: 0.48140\n","Epoch: 00 [15991/20101 ( 80%)], Train Loss: 0.48121\n","Epoch: 00 [16001/20101 ( 80%)], Train Loss: 0.48118\n","Epoch: 00 [16011/20101 ( 80%)], Train Loss: 0.48106\n","Epoch: 00 [16021/20101 ( 80%)], Train Loss: 0.48096\n","Epoch: 00 [16031/20101 ( 80%)], Train Loss: 0.48082\n","Epoch: 00 [16041/20101 ( 80%)], Train Loss: 0.48073\n","Epoch: 00 [16051/20101 ( 80%)], Train Loss: 0.48095\n","Epoch: 00 [16061/20101 ( 80%)], Train Loss: 0.48075\n","Epoch: 00 [16071/20101 ( 80%)], Train Loss: 0.48058\n","Epoch: 00 [16081/20101 ( 80%)], Train Loss: 0.48074\n","Epoch: 00 [16091/20101 ( 80%)], Train Loss: 0.48072\n","Epoch: 00 [16101/20101 ( 80%)], Train Loss: 0.48101\n","Epoch: 00 [16111/20101 ( 80%)], Train Loss: 0.48075\n","Epoch: 00 [16121/20101 ( 80%)], Train Loss: 0.48058\n","Epoch: 00 [16131/20101 ( 80%)], Train Loss: 0.48051\n","Epoch: 00 [16141/20101 ( 80%)], Train Loss: 0.48041\n","Epoch: 00 [16151/20101 ( 80%)], Train Loss: 0.48064\n","Epoch: 00 [16161/20101 ( 80%)], Train Loss: 0.48062\n","Epoch: 00 [16171/20101 ( 80%)], Train Loss: 0.48035\n","Epoch: 00 [16181/20101 ( 80%)], Train Loss: 0.48039\n","Epoch: 00 [16191/20101 ( 81%)], Train Loss: 0.48019\n","Epoch: 00 [16201/20101 ( 81%)], Train Loss: 0.48024\n","Epoch: 00 [16211/20101 ( 81%)], Train Loss: 0.48001\n","Epoch: 00 [16221/20101 ( 81%)], Train Loss: 0.47994\n","Epoch: 00 [16231/20101 ( 81%)], Train Loss: 0.48000\n","Epoch: 00 [16241/20101 ( 81%)], Train Loss: 0.47989\n","Epoch: 00 [16251/20101 ( 81%)], Train Loss: 0.47967\n","Epoch: 00 [16261/20101 ( 81%)], Train Loss: 0.47954\n","Epoch: 00 [16271/20101 ( 81%)], Train Loss: 0.47938\n","Epoch: 00 [16281/20101 ( 81%)], Train Loss: 0.47962\n","Epoch: 00 [16291/20101 ( 81%)], Train Loss: 0.47946\n","Epoch: 00 [16301/20101 ( 81%)], Train Loss: 0.47950\n","Epoch: 00 [16311/20101 ( 81%)], Train Loss: 0.47951\n","Epoch: 00 [16321/20101 ( 81%)], Train Loss: 0.47949\n","Epoch: 00 [16331/20101 ( 81%)], Train Loss: 0.47947\n","Epoch: 00 [16341/20101 ( 81%)], Train Loss: 0.47930\n","Epoch: 00 [16351/20101 ( 81%)], Train Loss: 0.47930\n","Epoch: 00 [16361/20101 ( 81%)], Train Loss: 0.47921\n","Epoch: 00 [16371/20101 ( 81%)], Train Loss: 0.47910\n","Epoch: 00 [16381/20101 ( 81%)], Train Loss: 0.47906\n","Epoch: 00 [16391/20101 ( 82%)], Train Loss: 0.47897\n","Epoch: 00 [16401/20101 ( 82%)], Train Loss: 0.47886\n","Epoch: 00 [16411/20101 ( 82%)], Train Loss: 0.47880\n","Epoch: 00 [16421/20101 ( 82%)], Train Loss: 0.47863\n","Epoch: 00 [16431/20101 ( 82%)], Train Loss: 0.47868\n","Epoch: 00 [16441/20101 ( 82%)], Train Loss: 0.47848\n","Epoch: 00 [16451/20101 ( 82%)], Train Loss: 0.47880\n","Epoch: 00 [16461/20101 ( 82%)], Train Loss: 0.47905\n","Epoch: 00 [16471/20101 ( 82%)], Train Loss: 0.47891\n","Epoch: 00 [16481/20101 ( 82%)], Train Loss: 0.47868\n","Epoch: 00 [16491/20101 ( 82%)], Train Loss: 0.47845\n","Epoch: 00 [16501/20101 ( 82%)], Train Loss: 0.47846\n","Epoch: 00 [16511/20101 ( 82%)], Train Loss: 0.47858\n","Epoch: 00 [16521/20101 ( 82%)], Train Loss: 0.47858\n","Epoch: 00 [16531/20101 ( 82%)], Train Loss: 0.47857\n","Epoch: 00 [16541/20101 ( 82%)], Train Loss: 0.47847\n","Epoch: 00 [16551/20101 ( 82%)], Train Loss: 0.47847\n","Epoch: 00 [16561/20101 ( 82%)], Train Loss: 0.47819\n","Epoch: 00 [16571/20101 ( 82%)], Train Loss: 0.47838\n","Epoch: 00 [16581/20101 ( 82%)], Train Loss: 0.47828\n","Epoch: 00 [16591/20101 ( 83%)], Train Loss: 0.47811\n","Epoch: 00 [16601/20101 ( 83%)], Train Loss: 0.47812\n","Epoch: 00 [16611/20101 ( 83%)], Train Loss: 0.47828\n","Epoch: 00 [16621/20101 ( 83%)], Train Loss: 0.47813\n","Epoch: 00 [16631/20101 ( 83%)], Train Loss: 0.47791\n","Epoch: 00 [16641/20101 ( 83%)], Train Loss: 0.47783\n","Epoch: 00 [16651/20101 ( 83%)], Train Loss: 0.47760\n","Epoch: 00 [16661/20101 ( 83%)], Train Loss: 0.47739\n","Epoch: 00 [16671/20101 ( 83%)], Train Loss: 0.47712\n","Epoch: 00 [16681/20101 ( 83%)], Train Loss: 0.47694\n","Epoch: 00 [16691/20101 ( 83%)], Train Loss: 0.47688\n","Epoch: 00 [16701/20101 ( 83%)], Train Loss: 0.47696\n","Epoch: 00 [16711/20101 ( 83%)], Train Loss: 0.47686\n","Epoch: 00 [16721/20101 ( 83%)], Train Loss: 0.47666\n","Epoch: 00 [16731/20101 ( 83%)], Train Loss: 0.47664\n","Epoch: 00 [16741/20101 ( 83%)], Train Loss: 0.47647\n","Epoch: 00 [16751/20101 ( 83%)], Train Loss: 0.47628\n","Epoch: 00 [16761/20101 ( 83%)], Train Loss: 0.47625\n","Epoch: 00 [16771/20101 ( 83%)], Train Loss: 0.47617\n","Epoch: 00 [16781/20101 ( 83%)], Train Loss: 0.47598\n","Epoch: 00 [16791/20101 ( 84%)], Train Loss: 0.47585\n","Epoch: 00 [16801/20101 ( 84%)], Train Loss: 0.47564\n","Epoch: 00 [16811/20101 ( 84%)], Train Loss: 0.47552\n","Epoch: 00 [16821/20101 ( 84%)], Train Loss: 0.47527\n","Epoch: 00 [16831/20101 ( 84%)], Train Loss: 0.47519\n","Epoch: 00 [16841/20101 ( 84%)], Train Loss: 0.47506\n","Epoch: 00 [16851/20101 ( 84%)], Train Loss: 0.47487\n","Epoch: 00 [16861/20101 ( 84%)], Train Loss: 0.47486\n","Epoch: 00 [16871/20101 ( 84%)], Train Loss: 0.47474\n","Epoch: 00 [16881/20101 ( 84%)], Train Loss: 0.47490\n","Epoch: 00 [16891/20101 ( 84%)], Train Loss: 0.47466\n","Epoch: 00 [16901/20101 ( 84%)], Train Loss: 0.47471\n","Epoch: 00 [16911/20101 ( 84%)], Train Loss: 0.47451\n","Epoch: 00 [16921/20101 ( 84%)], Train Loss: 0.47446\n","Epoch: 00 [16931/20101 ( 84%)], Train Loss: 0.47442\n","Epoch: 00 [16941/20101 ( 84%)], Train Loss: 0.47437\n","Epoch: 00 [16951/20101 ( 84%)], Train Loss: 0.47423\n","Epoch: 00 [16961/20101 ( 84%)], Train Loss: 0.47402\n","Epoch: 00 [16971/20101 ( 84%)], Train Loss: 0.47384\n","Epoch: 00 [16981/20101 ( 84%)], Train Loss: 0.47376\n","Epoch: 00 [16991/20101 ( 85%)], Train Loss: 0.47385\n","Epoch: 00 [17001/20101 ( 85%)], Train Loss: 0.47363\n","Epoch: 00 [17011/20101 ( 85%)], Train Loss: 0.47352\n","Epoch: 00 [17021/20101 ( 85%)], Train Loss: 0.47340\n","Epoch: 00 [17031/20101 ( 85%)], Train Loss: 0.47352\n","Epoch: 00 [17041/20101 ( 85%)], Train Loss: 0.47358\n","Epoch: 00 [17051/20101 ( 85%)], Train Loss: 0.47344\n","Epoch: 00 [17061/20101 ( 85%)], Train Loss: 0.47322\n","Epoch: 00 [17071/20101 ( 85%)], Train Loss: 0.47334\n","Epoch: 00 [17081/20101 ( 85%)], Train Loss: 0.47334\n","Epoch: 00 [17091/20101 ( 85%)], Train Loss: 0.47311\n","Epoch: 00 [17101/20101 ( 85%)], Train Loss: 0.47342\n","Epoch: 00 [17111/20101 ( 85%)], Train Loss: 0.47324\n","Epoch: 00 [17121/20101 ( 85%)], Train Loss: 0.47300\n","Epoch: 00 [17131/20101 ( 85%)], Train Loss: 0.47284\n","Epoch: 00 [17141/20101 ( 85%)], Train Loss: 0.47276\n","Epoch: 00 [17151/20101 ( 85%)], Train Loss: 0.47263\n","Epoch: 00 [17161/20101 ( 85%)], Train Loss: 0.47245\n","Epoch: 00 [17171/20101 ( 85%)], Train Loss: 0.47228\n","Epoch: 00 [17181/20101 ( 85%)], Train Loss: 0.47207\n","Epoch: 00 [17191/20101 ( 86%)], Train Loss: 0.47206\n","Epoch: 00 [17201/20101 ( 86%)], Train Loss: 0.47202\n","Epoch: 00 [17211/20101 ( 86%)], Train Loss: 0.47183\n","Epoch: 00 [17221/20101 ( 86%)], Train Loss: 0.47187\n","Epoch: 00 [17231/20101 ( 86%)], Train Loss: 0.47176\n","Epoch: 00 [17241/20101 ( 86%)], Train Loss: 0.47150\n","Epoch: 00 [17251/20101 ( 86%)], Train Loss: 0.47143\n","Epoch: 00 [17261/20101 ( 86%)], Train Loss: 0.47131\n","Epoch: 00 [17271/20101 ( 86%)], Train Loss: 0.47129\n","Epoch: 00 [17281/20101 ( 86%)], Train Loss: 0.47110\n","Epoch: 00 [17291/20101 ( 86%)], Train Loss: 0.47089\n","Epoch: 00 [17301/20101 ( 86%)], Train Loss: 0.47072\n","Epoch: 00 [17311/20101 ( 86%)], Train Loss: 0.47057\n","Epoch: 00 [17321/20101 ( 86%)], Train Loss: 0.47054\n","Epoch: 00 [17331/20101 ( 86%)], Train Loss: 0.47051\n","Epoch: 00 [17341/20101 ( 86%)], Train Loss: 0.47028\n","Epoch: 00 [17351/20101 ( 86%)], Train Loss: 0.47023\n","Epoch: 00 [17361/20101 ( 86%)], Train Loss: 0.47030\n","Epoch: 00 [17371/20101 ( 86%)], Train Loss: 0.47015\n","Epoch: 00 [17381/20101 ( 86%)], Train Loss: 0.46993\n","Epoch: 00 [17391/20101 ( 87%)], Train Loss: 0.46976\n","Epoch: 00 [17401/20101 ( 87%)], Train Loss: 0.46951\n","Epoch: 00 [17411/20101 ( 87%)], Train Loss: 0.46967\n","Epoch: 00 [17421/20101 ( 87%)], Train Loss: 0.46947\n","Epoch: 00 [17431/20101 ( 87%)], Train Loss: 0.46972\n","Epoch: 00 [17441/20101 ( 87%)], Train Loss: 0.46948\n","Epoch: 00 [17451/20101 ( 87%)], Train Loss: 0.46938\n","Epoch: 00 [17461/20101 ( 87%)], Train Loss: 0.46941\n","Epoch: 00 [17471/20101 ( 87%)], Train Loss: 0.46949\n","Epoch: 00 [17481/20101 ( 87%)], Train Loss: 0.46946\n","Epoch: 00 [17491/20101 ( 87%)], Train Loss: 0.46949\n","Epoch: 00 [17501/20101 ( 87%)], Train Loss: 0.46928\n","Epoch: 00 [17511/20101 ( 87%)], Train Loss: 0.46922\n","Epoch: 00 [17521/20101 ( 87%)], Train Loss: 0.46909\n","Epoch: 00 [17531/20101 ( 87%)], Train Loss: 0.46898\n","Epoch: 00 [17541/20101 ( 87%)], Train Loss: 0.46885\n","Epoch: 00 [17551/20101 ( 87%)], Train Loss: 0.46861\n","Epoch: 00 [17561/20101 ( 87%)], Train Loss: 0.46867\n","Epoch: 00 [17571/20101 ( 87%)], Train Loss: 0.46877\n","Epoch: 00 [17581/20101 ( 87%)], Train Loss: 0.46878\n","Epoch: 00 [17591/20101 ( 88%)], Train Loss: 0.46874\n","Epoch: 00 [17601/20101 ( 88%)], Train Loss: 0.46876\n","Epoch: 00 [17611/20101 ( 88%)], Train Loss: 0.46872\n","Epoch: 00 [17621/20101 ( 88%)], Train Loss: 0.46858\n","Epoch: 00 [17631/20101 ( 88%)], Train Loss: 0.46858\n","Epoch: 00 [17641/20101 ( 88%)], Train Loss: 0.46850\n","Epoch: 00 [17651/20101 ( 88%)], Train Loss: 0.46842\n","Epoch: 00 [17661/20101 ( 88%)], Train Loss: 0.46831\n","Epoch: 00 [17671/20101 ( 88%)], Train Loss: 0.46806\n","Epoch: 00 [17681/20101 ( 88%)], Train Loss: 0.46812\n","Epoch: 00 [17691/20101 ( 88%)], Train Loss: 0.46796\n","Epoch: 00 [17701/20101 ( 88%)], Train Loss: 0.46783\n","Epoch: 00 [17711/20101 ( 88%)], Train Loss: 0.46794\n","Epoch: 00 [17721/20101 ( 88%)], Train Loss: 0.46778\n","Epoch: 00 [17731/20101 ( 88%)], Train Loss: 0.46766\n","Epoch: 00 [17741/20101 ( 88%)], Train Loss: 0.46755\n","Epoch: 00 [17751/20101 ( 88%)], Train Loss: 0.46763\n","Epoch: 00 [17761/20101 ( 88%)], Train Loss: 0.46753\n","Epoch: 00 [17771/20101 ( 88%)], Train Loss: 0.46729\n","Epoch: 00 [17781/20101 ( 88%)], Train Loss: 0.46732\n","Epoch: 00 [17791/20101 ( 89%)], Train Loss: 0.46733\n","Epoch: 00 [17801/20101 ( 89%)], Train Loss: 0.46722\n","Epoch: 00 [17811/20101 ( 89%)], Train Loss: 0.46713\n","Epoch: 00 [17821/20101 ( 89%)], Train Loss: 0.46730\n","Epoch: 00 [17831/20101 ( 89%)], Train Loss: 0.46730\n","Epoch: 00 [17841/20101 ( 89%)], Train Loss: 0.46720\n","Epoch: 00 [17851/20101 ( 89%)], Train Loss: 0.46704\n","Epoch: 00 [17861/20101 ( 89%)], Train Loss: 0.46695\n","Epoch: 00 [17871/20101 ( 89%)], Train Loss: 0.46686\n","Epoch: 00 [17881/20101 ( 89%)], Train Loss: 0.46667\n","Epoch: 00 [17891/20101 ( 89%)], Train Loss: 0.46651\n","Epoch: 00 [17901/20101 ( 89%)], Train Loss: 0.46656\n","Epoch: 00 [17911/20101 ( 89%)], Train Loss: 0.46637\n","Epoch: 00 [17921/20101 ( 89%)], Train Loss: 0.46627\n","Epoch: 00 [17931/20101 ( 89%)], Train Loss: 0.46642\n","Epoch: 00 [17941/20101 ( 89%)], Train Loss: 0.46625\n","Epoch: 00 [17951/20101 ( 89%)], Train Loss: 0.46636\n","Epoch: 00 [17961/20101 ( 89%)], Train Loss: 0.46643\n","Epoch: 00 [17971/20101 ( 89%)], Train Loss: 0.46626\n","Epoch: 00 [17981/20101 ( 89%)], Train Loss: 0.46627\n","Epoch: 00 [17991/20101 ( 90%)], Train Loss: 0.46629\n","Epoch: 00 [18001/20101 ( 90%)], Train Loss: 0.46628\n","Epoch: 00 [18011/20101 ( 90%)], Train Loss: 0.46635\n","Epoch: 00 [18021/20101 ( 90%)], Train Loss: 0.46621\n","Epoch: 00 [18031/20101 ( 90%)], Train Loss: 0.46624\n","Epoch: 00 [18041/20101 ( 90%)], Train Loss: 0.46601\n","Epoch: 00 [18051/20101 ( 90%)], Train Loss: 0.46591\n","Epoch: 00 [18061/20101 ( 90%)], Train Loss: 0.46572\n","Epoch: 00 [18071/20101 ( 90%)], Train Loss: 0.46558\n","Epoch: 00 [18081/20101 ( 90%)], Train Loss: 0.46582\n","Epoch: 00 [18091/20101 ( 90%)], Train Loss: 0.46583\n","Epoch: 00 [18101/20101 ( 90%)], Train Loss: 0.46589\n","Epoch: 00 [18111/20101 ( 90%)], Train Loss: 0.46588\n","Epoch: 00 [18121/20101 ( 90%)], Train Loss: 0.46586\n","Epoch: 00 [18131/20101 ( 90%)], Train Loss: 0.46588\n","Epoch: 00 [18141/20101 ( 90%)], Train Loss: 0.46594\n","Epoch: 00 [18151/20101 ( 90%)], Train Loss: 0.46590\n","Epoch: 00 [18161/20101 ( 90%)], Train Loss: 0.46572\n","Epoch: 00 [18171/20101 ( 90%)], Train Loss: 0.46581\n","Epoch: 00 [18181/20101 ( 90%)], Train Loss: 0.46572\n","Epoch: 00 [18191/20101 ( 90%)], Train Loss: 0.46583\n","Epoch: 00 [18201/20101 ( 91%)], Train Loss: 0.46558\n","Epoch: 00 [18211/20101 ( 91%)], Train Loss: 0.46549\n","Epoch: 00 [18221/20101 ( 91%)], Train Loss: 0.46559\n","Epoch: 00 [18231/20101 ( 91%)], Train Loss: 0.46569\n","Epoch: 00 [18241/20101 ( 91%)], Train Loss: 0.46570\n","Epoch: 00 [18251/20101 ( 91%)], Train Loss: 0.46576\n","Epoch: 00 [18261/20101 ( 91%)], Train Loss: 0.46567\n","Epoch: 00 [18271/20101 ( 91%)], Train Loss: 0.46553\n","Epoch: 00 [18281/20101 ( 91%)], Train Loss: 0.46559\n","Epoch: 00 [18291/20101 ( 91%)], Train Loss: 0.46585\n","Epoch: 00 [18301/20101 ( 91%)], Train Loss: 0.46570\n","Epoch: 00 [18311/20101 ( 91%)], Train Loss: 0.46549\n","Epoch: 00 [18321/20101 ( 91%)], Train Loss: 0.46534\n","Epoch: 00 [18331/20101 ( 91%)], Train Loss: 0.46552\n","Epoch: 00 [18341/20101 ( 91%)], Train Loss: 0.46536\n","Epoch: 00 [18351/20101 ( 91%)], Train Loss: 0.46514\n","Epoch: 00 [18361/20101 ( 91%)], Train Loss: 0.46522\n","Epoch: 00 [18371/20101 ( 91%)], Train Loss: 0.46497\n","Epoch: 00 [18381/20101 ( 91%)], Train Loss: 0.46484\n","Epoch: 00 [18391/20101 ( 91%)], Train Loss: 0.46469\n","Epoch: 00 [18401/20101 ( 92%)], Train Loss: 0.46475\n","Epoch: 00 [18411/20101 ( 92%)], Train Loss: 0.46502\n","Epoch: 00 [18421/20101 ( 92%)], Train Loss: 0.46496\n","Epoch: 00 [18431/20101 ( 92%)], Train Loss: 0.46492\n","Epoch: 00 [18441/20101 ( 92%)], Train Loss: 0.46504\n","Epoch: 00 [18451/20101 ( 92%)], Train Loss: 0.46503\n","Epoch: 00 [18461/20101 ( 92%)], Train Loss: 0.46498\n","Epoch: 00 [18471/20101 ( 92%)], Train Loss: 0.46479\n","Epoch: 00 [18481/20101 ( 92%)], Train Loss: 0.46474\n","Epoch: 00 [18491/20101 ( 92%)], Train Loss: 0.46482\n","Epoch: 00 [18501/20101 ( 92%)], Train Loss: 0.46468\n","Epoch: 00 [18511/20101 ( 92%)], Train Loss: 0.46455\n","Epoch: 00 [18521/20101 ( 92%)], Train Loss: 0.46437\n","Epoch: 00 [18531/20101 ( 92%)], Train Loss: 0.46430\n","Epoch: 00 [18541/20101 ( 92%)], Train Loss: 0.46437\n","Epoch: 00 [18551/20101 ( 92%)], Train Loss: 0.46438\n","Epoch: 00 [18561/20101 ( 92%)], Train Loss: 0.46425\n","Epoch: 00 [18571/20101 ( 92%)], Train Loss: 0.46406\n","Epoch: 00 [18581/20101 ( 92%)], Train Loss: 0.46406\n","Epoch: 00 [18591/20101 ( 92%)], Train Loss: 0.46397\n","Epoch: 00 [18601/20101 ( 93%)], Train Loss: 0.46385\n","Epoch: 00 [18611/20101 ( 93%)], Train Loss: 0.46368\n","Epoch: 00 [18621/20101 ( 93%)], Train Loss: 0.46368\n","Epoch: 00 [18631/20101 ( 93%)], Train Loss: 0.46344\n","Epoch: 00 [18641/20101 ( 93%)], Train Loss: 0.46348\n","Epoch: 00 [18651/20101 ( 93%)], Train Loss: 0.46350\n","Epoch: 00 [18661/20101 ( 93%)], Train Loss: 0.46341\n","Epoch: 00 [18671/20101 ( 93%)], Train Loss: 0.46322\n","Epoch: 00 [18681/20101 ( 93%)], Train Loss: 0.46309\n","Epoch: 00 [18691/20101 ( 93%)], Train Loss: 0.46294\n","Epoch: 00 [18701/20101 ( 93%)], Train Loss: 0.46283\n","Epoch: 00 [18711/20101 ( 93%)], Train Loss: 0.46288\n","Epoch: 00 [18721/20101 ( 93%)], Train Loss: 0.46289\n","Epoch: 00 [18731/20101 ( 93%)], Train Loss: 0.46275\n","Epoch: 00 [18741/20101 ( 93%)], Train Loss: 0.46277\n","Epoch: 00 [18751/20101 ( 93%)], Train Loss: 0.46279\n","Epoch: 00 [18761/20101 ( 93%)], Train Loss: 0.46277\n","Epoch: 00 [18771/20101 ( 93%)], Train Loss: 0.46266\n","Epoch: 00 [18781/20101 ( 93%)], Train Loss: 0.46276\n","Epoch: 00 [18791/20101 ( 93%)], Train Loss: 0.46271\n","Epoch: 00 [18801/20101 ( 94%)], Train Loss: 0.46267\n","Epoch: 00 [18811/20101 ( 94%)], Train Loss: 0.46251\n","Epoch: 00 [18821/20101 ( 94%)], Train Loss: 0.46249\n","Epoch: 00 [18831/20101 ( 94%)], Train Loss: 0.46242\n","Epoch: 00 [18841/20101 ( 94%)], Train Loss: 0.46232\n","Epoch: 00 [18851/20101 ( 94%)], Train Loss: 0.46228\n","Epoch: 00 [18861/20101 ( 94%)], Train Loss: 0.46222\n","Epoch: 00 [18871/20101 ( 94%)], Train Loss: 0.46215\n","Epoch: 00 [18881/20101 ( 94%)], Train Loss: 0.46195\n","Epoch: 00 [18891/20101 ( 94%)], Train Loss: 0.46186\n","Epoch: 00 [18901/20101 ( 94%)], Train Loss: 0.46171\n","Epoch: 00 [18911/20101 ( 94%)], Train Loss: 0.46175\n","Epoch: 00 [18921/20101 ( 94%)], Train Loss: 0.46154\n"]}]},{"cell_type":"code","metadata":{"id":"o6-F5rzbHSK5"},"source":["! cp -r /content/output/checkpoint-fold-3 /content/drive/Shareddrives/NLP/Dataset/murli"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LdC5X_O7HFVz"},"source":[""],"execution_count":null,"outputs":[]}]}
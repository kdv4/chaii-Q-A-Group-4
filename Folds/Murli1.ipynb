{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Murli1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"faIyP9zZEd0G","executionInfo":{"status":"ok","timestamp":1635143758001,"user_tz":-330,"elapsed":4799,"user":{"displayName":"2020 11057","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16050524758694041188"}},"outputId":"5fb1a13b-d6d1-401c-9539-4889e29c882e"},"source":["! pip install transformers sentencepiece "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.11.3)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n","Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.19)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jDu4BWB1EJJ3","executionInfo":{"status":"ok","timestamp":1635143768272,"user_tz":-330,"elapsed":10275,"user":{"displayName":"2020 11057","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16050524758694041188"}},"outputId":"2198f52b-4dce-4c3f-8ed7-913d0d838cc0"},"source":["import os\n","import gc\n","gc.enable()\n","import math\n","import json\n","import time\n","import random\n","import multiprocessing\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm, trange\n","from sklearn import model_selection\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import Parameter\n","import torch.optim as optim\n","from torch.utils.data import (\n","    Dataset, DataLoader,\n","    SequentialSampler, RandomSampler\n",")\n","from torch.utils.data.distributed import DistributedSampler\n","\n","try:\n","    from apex import amp\n","    APEX_INSTALLED = True\n","except ImportError:\n","    APEX_INSTALLED = False\n","\n","import transformers\n","from transformers import (\n","    WEIGHTS_NAME,\n","    AdamW,\n","    AutoConfig,\n","    AutoModel,\n","    AutoTokenizer,\n","    get_cosine_schedule_with_warmup,\n","    get_linear_schedule_with_warmup,\n","    logging,\n","    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n",")\n","logging.set_verbosity_warning()\n","logging.set_verbosity_error()\n","\n","def fix_all_seeds(seed):\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","def optimal_num_of_loader_workers():\n","    num_cpus = multiprocessing.cpu_count()\n","    num_gpus = torch.cuda.device_count()\n","    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n","    return optimal_value\n","\n","print(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\n","MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n","MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Apex AMP Installed :: False\n"]}]},{"cell_type":"code","metadata":{"id":"t8HfXz6SHhE7"},"source":["class Config:\n","    # model\n","    model_type = 'bert'\n","    model_name_or_path = \"/content/drive/Shareddrives/NLP/Tarang/xlm-roberta-large-squad2/model\"\n","    config_name = \"/content/drive/Shareddrives/NLP/Tarang/xlm-roberta-large-squad2/model\"\n","    fp16 = True if APEX_INSTALLED else False\n","    fp16_opt_level = \"O1\"\n","    gradient_accumulation_steps = 2\n","\n","    # tokenizer\n","    tokenizer_name = \"/content/drive/Shareddrives/NLP/Tarang/xlm-roberta-large-squad2/tok\"\n","    max_seq_length = 384\n","    doc_stride = 128\n","\n","    # train\n","    epochs = 1\n","    train_batch_size = 1\n","    eval_batch_size = 1\n","\n","    # optimizer\n","    optimizer_type = 'AdamW'\n","    learning_rate = 1.5e-5\n","    weight_decay = 1e-2\n","    epsilon = 1e-8\n","    max_grad_norm = 1.0\n","\n","    # scheduler\n","    decay_name = 'linear-warmup'\n","    warmup_ratio = 0.1\n","\n","    # logging\n","    logging_steps = 10\n","\n","    # evaluate\n","    output_dir = 'output'\n","    seed = 42"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jzv3krJSETEs"},"source":["train = pd.read_csv('/content/drive/Shareddrives/NLP/Dataset/train.csv')\n","test = pd.read_csv('/content/drive/Shareddrives/NLP/Dataset/test.csv')\n","external_mlqa = pd.read_csv('/content/drive/Shareddrives/NLP/Dataset/mlqa_hindi.csv')\n","external_xquad = pd.read_csv('/content/drive/Shareddrives/NLP/Dataset/xquad.csv')\n","external_train = pd.concat([external_mlqa, external_xquad])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wNv3o7qeFmaB"},"source":["def create_folds(data, num_splits):\n","    data[\"kfold\"] = -1\n","    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=69)\n","    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n","        data.loc[v_, 'kfold'] = f\n","    return data\n","\n","train = create_folds(train, num_splits=5)\n","external_train[\"kfold\"] = -1\n","external_train['id'] = list(np.arange(1, len(external_train)+1))\n","train = pd.concat([train, external_train]).reset_index(drop=True)\n","\n","def convert_answers(row):\n","    return {'answer_start': [row[0]], 'text': [row[1]]}\n","\n","train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dVmPKiJZGKGc"},"source":["def prepare_train_features(args, example, tokenizer):\n","    example[\"question\"] = example[\"question\"].lstrip()\n","    tokenized_example = tokenizer(\n","        example[\"question\"],\n","        example[\"context\"],\n","        truncation=\"only_second\",\n","        max_length=args.max_seq_length,\n","        stride=args.doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n","    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n","\n","    features = []\n","    for i, offsets in enumerate(offset_mapping):\n","        feature = {}\n","\n","        input_ids = tokenized_example[\"input_ids\"][i]\n","        attention_mask = tokenized_example[\"attention_mask\"][i]\n","\n","        feature['input_ids'] = input_ids\n","        feature['attention_mask'] = attention_mask\n","        feature['offset_mapping'] = offsets\n","\n","        cls_index = input_ids.index(tokenizer.cls_token_id)\n","        sequence_ids = tokenized_example.sequence_ids(i)\n","\n","        sample_index = sample_mapping[i]\n","        answers = example[\"answers\"]\n","\n","        if len(answers[\"answer_start\"]) == 0:\n","            feature[\"start_position\"] = cls_index\n","            feature[\"end_position\"] = cls_index\n","        else:\n","            start_char = answers[\"answer_start\"][0]\n","            end_char = start_char + len(answers[\"text\"][0])\n","\n","            token_start_index = 0\n","            while sequence_ids[token_start_index] != 1:\n","                token_start_index += 1\n","\n","            token_end_index = len(input_ids) - 1\n","            while sequence_ids[token_end_index] != 1:\n","                token_end_index -= 1\n","\n","            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n","                feature[\"start_position\"] = cls_index\n","                feature[\"end_position\"] = cls_index\n","            else:\n","                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n","                    token_start_index += 1\n","                feature[\"start_position\"] = token_start_index - 1\n","                while offsets[token_end_index][1] >= end_char:\n","                    token_end_index -= 1\n","                feature[\"end_position\"] = token_end_index + 1\n","\n","        features.append(feature)\n","    return features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1X5iXyuyGRuH"},"source":["class DatasetRetriever(Dataset):\n","    def __init__(self, features, mode='train'):\n","        super(DatasetRetriever, self).__init__()\n","        self.features = features\n","        self.mode = mode\n","        \n","    def __len__(self):\n","        return len(self.features)\n","    \n","    def __getitem__(self, item):   \n","        feature = self.features[item]\n","        if self.mode == 'train':\n","            return {\n","                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n","                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n","                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n","                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n","                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n","            }\n","        else:\n","            return {\n","                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n","                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n","                'offset_mapping':feature['offset_mapping'],\n","                'sequence_ids':feature['sequence_ids'],\n","                'id':feature['example_id'],\n","                'context': feature['context'],\n","                'question': feature['question']\n","            }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c83EghN4GXie"},"source":["class Model(nn.Module):\n","    def __init__(self, modelname_or_path, config):\n","        super(Model, self).__init__()\n","        self.config = config\n","        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n","        self.linear_layer = nn.Linear(config.hidden_size, 64)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.qa_outputs = nn.Linear(64, 2)\n","        self._init_weights(self.qa_outputs)\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","\n","    def forward(\n","        self, \n","        input_ids, \n","        attention_mask=None, \n","    ):\n","        outputs = self.xlm_roberta(input_ids,attention_mask=attention_mask)\n","\n","        sequence_output = outputs[0]\n","        pooled_output = outputs[1]\n","        \n","        linear_output = self.linear_layer(sequence_output)\n","        linear_output = self.dropout(linear_output)\n","        qa_logits = self.qa_outputs(linear_output)\n","        \n","        start_logits, end_logits = qa_logits.split(1, dim=-1)\n","        start_logits = start_logits.squeeze(-1)\n","        end_logits = end_logits.squeeze(-1)\n","    \n","        return start_logits, end_logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gKn4pHnvGc4H"},"source":["def loss_fn(preds, labels):\n","    start_preds, end_preds = preds\n","    start_labels, end_labels = labels\n","    \n","    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n","    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n","    total_loss = (start_loss + end_loss) / 2\n","    return total_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qoV8_pn6GgdW"},"source":["def get_optimizer_grouped_parameters(args, model):\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n","    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n","    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n","    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': args.weight_decay},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': args.weight_decay, 'lr': args.learning_rate/2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': args.weight_decay, 'lr': args.learning_rate},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': args.weight_decay, 'lr': args.learning_rate*2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': args.learning_rate/2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': args.learning_rate},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': args.learning_rate*2.6},\n","        {'params': [p for n, p in model.named_parameters() if args.model_type not in n], 'lr':args.learning_rate*20, \"weight_decay\": 0.0},\n","    ]\n","    return optimizer_grouped_parameters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xp_uEUijGrCv"},"source":["class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","        self.max = 0\n","        self.min = 1e5\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","        if val > self.max:\n","            self.max = val\n","        if val < self.min:\n","            self.min = val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RKcK3xENGvQ4"},"source":["def make_model(args):\n","    config = AutoConfig.from_pretrained(args.config_name)\n","    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n","    model = Model(args.model_name_or_path, config=config)\n","    return config, tokenizer, model\n","\n","def make_optimizer(args, model):\n","    optimizer_grouped_parameters = get_optimizer_grouped_parameters(args, model)\n","#     no_decay = [\"bias\", \"LayerNorm.weight\"]\n","#     optimizer_grouped_parameters = [\n","#         {\n","#             \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","#             \"weight_decay\": args.weight_decay,\n","#         },\n","#         {\n","#             \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","#             \"weight_decay\": 0.0,\n","#         },\n","#     ]\n","    if args.optimizer_type == \"AdamW\":\n","        optimizer = AdamW(\n","            optimizer_grouped_parameters,\n","            lr=args.learning_rate,\n","            eps=args.epsilon,\n","            correct_bias=True\n","        )\n","        return optimizer\n","\n","def make_scheduler(\n","    args, optimizer, \n","    num_warmup_steps, \n","    num_training_steps\n","):\n","    if args.decay_name == \"cosine-warmup\":\n","        scheduler = get_cosine_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps\n","        )\n","    else:\n","        scheduler = get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps\n","        )\n","    return scheduler    \n","\n","def make_loader(\n","    args, data, \n","    tokenizer, fold\n","):\n","    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n","    \n","    train_features, valid_features = [[] for _ in range(2)]\n","    for i, row in train_set.iterrows():\n","        train_features += prepare_train_features(args, row, tokenizer)\n","    for i, row in valid_set.iterrows():\n","        valid_features += prepare_train_features(args, row, tokenizer)\n","\n","    train_dataset = DatasetRetriever(train_features)\n","    valid_dataset = DatasetRetriever(valid_features)\n","    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n","    \n","    train_sampler = RandomSampler(train_dataset)\n","    valid_sampler = SequentialSampler(valid_dataset)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=args.train_batch_size,\n","        sampler=train_sampler,\n","        num_workers=optimal_num_of_loader_workers(),\n","        pin_memory=True,\n","        drop_last=False \n","    )\n","\n","    valid_dataloader = DataLoader(\n","        valid_dataset,\n","        batch_size=args.eval_batch_size, \n","        sampler=valid_sampler,\n","        num_workers=optimal_num_of_loader_workers(),\n","        pin_memory=True, \n","        drop_last=False\n","    )\n","\n","    return train_dataloader, valid_dataloader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YYXpJ2UaG4ft"},"source":["class Trainer:\n","    def __init__(\n","        self, model, tokenizer, \n","        optimizer, scheduler\n","    ):\n","        self.model = model\n","        self.tokenizer = tokenizer\n","        self.optimizer = optimizer\n","        self.scheduler = scheduler\n","\n","    def train(\n","        self, args, \n","        train_dataloader, \n","        epoch, result_dict\n","    ):\n","        count = 0\n","        losses = AverageMeter()\n","        \n","        self.model.zero_grad()\n","        self.model.train()\n","        \n","        fix_all_seeds(args.seed)\n","        \n","        for batch_idx, batch_data in enumerate(train_dataloader):\n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                batch_data['input_ids'], batch_data['attention_mask'], \\\n","                    batch_data['start_position'], batch_data['end_position']\n","            \n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n","\n","            outputs_start, outputs_end = self.model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","            )\n","            \n","            loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n","            loss = loss / args.gradient_accumulation_steps\n","\n","            if args.fp16:\n","                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","            else:\n","                loss.backward()\n","\n","            count += input_ids.size(0)\n","            losses.update(loss.item(), input_ids.size(0))\n","\n","            # if args.fp16:\n","            #     torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), args.max_grad_norm)\n","            # else:\n","            #     torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.max_grad_norm)\n","\n","            if batch_idx % args.gradient_accumulation_steps == 0 or batch_idx == len(train_dataloader) - 1:\n","                self.optimizer.step()\n","                self.scheduler.step()\n","                self.optimizer.zero_grad()\n","\n","            if (batch_idx % args.logging_steps == 0) or (batch_idx+1)==len(train_dataloader):\n","                _s = str(len(str(len(train_dataloader.sampler))))\n","                ret = [\n","                    ('Epoch: {:0>2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_dataloader.sampler), 100 * count / len(train_dataloader.sampler)),\n","                    'Train Loss: {: >4.5f}'.format(losses.avg),\n","                ]\n","                print(', '.join(ret))\n","\n","        result_dict['train_loss'].append(losses.avg)\n","        return result_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c7lo_72cG88t"},"source":["class Evaluator:\n","    def __init__(self, model):\n","        self.model = model\n","    \n","    def save(self, result, output_dir):\n","        with open(f'{output_dir}/result_dict.json', 'w') as f:\n","            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n","\n","    def evaluate(self, valid_dataloader, epoch, result_dict):\n","        losses = AverageMeter()\n","        for batch_idx, batch_data in enumerate(valid_dataloader):\n","            self.model = self.model.eval()\n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                batch_data['input_ids'], batch_data['attention_mask'], \\\n","                    batch_data['start_position'], batch_data['end_position']\n","            \n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n","            \n","            with torch.no_grad():            \n","                outputs_start, outputs_end = self.model(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                )\n","                \n","                loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n","                losses.update(loss.item(), input_ids.size(0))\n","                \n","        print('----Validation Results Summary----')\n","        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n","        result_dict['val_loss'].append(losses.avg)        \n","        return result_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wv14GLumHBTO"},"source":["def init_training(args, data, fold):\n","    fix_all_seeds(args.seed)\n","    \n","    if not os.path.exists(args.output_dir):\n","        os.makedirs(args.output_dir)\n","    \n","    # model\n","    model_config, tokenizer, model = make_model(args)\n","    if torch.cuda.device_count() >= 1:\n","        print('Model pushed to {} GPU(s), type {}.'.format(\n","            torch.cuda.device_count(), \n","            torch.cuda.get_device_name(0))\n","        )\n","        model = model.cuda() \n","    else:\n","        raise ValueError('CPU training is not supported')\n","    \n","    # data loaders\n","    train_dataloader, valid_dataloader = make_loader(args, data, tokenizer, fold)\n","\n","    # optimizer\n","    optimizer = make_optimizer(args, model)\n","\n","    # scheduler\n","    num_training_steps = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps) * args.epochs\n","    if args.warmup_ratio > 0:\n","        num_warmup_steps = int(args.warmup_ratio * num_training_steps)\n","    else:\n","        num_warmup_steps = 0\n","    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n","    scheduler = make_scheduler(args, optimizer, num_warmup_steps, num_training_steps)\n","\n","    # mixed precision training with NVIDIA Apex\n","    if args.fp16:\n","        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n","    \n","    result_dict = {\n","        'epoch':[], \n","        'train_loss': [], \n","        'val_loss' : [], \n","        'best_val_loss': np.inf\n","    }\n","\n","    return (\n","        model, model_config, tokenizer, optimizer, scheduler, \n","        train_dataloader, valid_dataloader, result_dict\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XUjSnwFpHKAi"},"source":["def run(data, fold):\n","    args = Config()\n","    model, model_config, tokenizer, optimizer, scheduler, train_dataloader, \\\n","        valid_dataloader, result_dict = init_training(args, data, fold)\n","    \n","    trainer = Trainer(model, tokenizer, optimizer, scheduler)\n","    evaluator = Evaluator(model)\n","\n","    train_time_list = []\n","    valid_time_list = []\n","\n","    for epoch in range(args.epochs):\n","        result_dict['epoch'].append(epoch)\n","\n","        # Train\n","        torch.cuda.synchronize()\n","        tic1 = time.time()\n","        result_dict = trainer.train(\n","            args, train_dataloader, \n","            epoch, result_dict\n","        )\n","        torch.cuda.synchronize()\n","        tic2 = time.time() \n","        train_time_list.append(tic2 - tic1)\n","        \n","        # Evaluate\n","        torch.cuda.synchronize()\n","        tic3 = time.time()\n","        result_dict = evaluator.evaluate(\n","            valid_dataloader, epoch, result_dict\n","        )\n","        torch.cuda.synchronize()\n","        tic4 = time.time() \n","        valid_time_list.append(tic4 - tic3)\n","            \n","        output_dir = os.path.join(args.output_dir, f\"checkpoint-fold-{fold}\")\n","        if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n","            print(\"{} Epoch, Best epoch was updated! Valid Loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n","            result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]        \n","            \n","            os.makedirs(output_dir, exist_ok=True)\n","            torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n","            model_config.save_pretrained(output_dir)\n","            tokenizer.save_pretrained(output_dir)\n","            print(f\"Saving model checkpoint to {output_dir}.\")\n","            \n","        print()\n","\n","    evaluator.save(result_dict, output_dir)\n","    \n","    print(f\"Total Training Time: {np.sum(train_time_list)}secs, Average Training Time per Epoch: {np.mean(train_time_list)}secs.\")\n","    print(f\"Total Validation Time: {np.sum(valid_time_list)}secs, Average Validation Time per Epoch: {np.mean(valid_time_list)}secs.\")\n","    \n","    torch.cuda.empty_cache()\n","    del trainer, evaluator\n","    del model, model_config, tokenizer\n","    del optimizer, scheduler\n","    del train_dataloader, valid_dataloader, result_dict\n","    gc.collect()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bduls9AyHO8p","executionInfo":{"status":"ok","timestamp":1635158598598,"user_tz":-330,"elapsed":14828130,"user":{"displayName":"2020 11057","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16050524758694041188"}},"outputId":"3e826b2c-ac21-4ae7-b34d-66b3b277ba4a"},"source":["for fold in range(1):\n","    print();print()\n","    print('-'*50)\n","    print(f'FOLD: {fold}')\n","    print('-'*50)\n","    run(train, fold)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","--------------------------------------------------\n","FOLD: 0\n","--------------------------------------------------\n","Model pushed to 1 GPU(s), type Tesla K80.\n","Num examples Train= 20026, Num examples Valid=3043\n","Total Training Steps: 10013, Total Warmup Steps: 1001\n","Epoch: 00 [    1/20026 (  0%)], Train Loss: 2.96061\n","Epoch: 00 [   11/20026 (  0%)], Train Loss: 2.95922\n","Epoch: 00 [   21/20026 (  0%)], Train Loss: 2.95785\n","Epoch: 00 [   31/20026 (  0%)], Train Loss: 2.95389\n","Epoch: 00 [   41/20026 (  0%)], Train Loss: 2.95591\n","Epoch: 00 [   51/20026 (  0%)], Train Loss: 2.95279\n","Epoch: 00 [   61/20026 (  0%)], Train Loss: 2.94782\n","Epoch: 00 [   71/20026 (  0%)], Train Loss: 2.94106\n","Epoch: 00 [   81/20026 (  0%)], Train Loss: 2.93145\n","Epoch: 00 [   91/20026 (  0%)], Train Loss: 2.92442\n","Epoch: 00 [  101/20026 (  1%)], Train Loss: 2.91380\n","Epoch: 00 [  111/20026 (  1%)], Train Loss: 2.90608\n","Epoch: 00 [  121/20026 (  1%)], Train Loss: 2.89613\n","Epoch: 00 [  131/20026 (  1%)], Train Loss: 2.88489\n","Epoch: 00 [  141/20026 (  1%)], Train Loss: 2.87318\n","Epoch: 00 [  151/20026 (  1%)], Train Loss: 2.86041\n","Epoch: 00 [  161/20026 (  1%)], Train Loss: 2.83976\n","Epoch: 00 [  171/20026 (  1%)], Train Loss: 2.82325\n","Epoch: 00 [  181/20026 (  1%)], Train Loss: 2.80489\n","Epoch: 00 [  191/20026 (  1%)], Train Loss: 2.78083\n","Epoch: 00 [  201/20026 (  1%)], Train Loss: 2.74982\n","Epoch: 00 [  211/20026 (  1%)], Train Loss: 2.71994\n","Epoch: 00 [  221/20026 (  1%)], Train Loss: 2.68763\n","Epoch: 00 [  231/20026 (  1%)], Train Loss: 2.65307\n","Epoch: 00 [  241/20026 (  1%)], Train Loss: 2.61268\n","Epoch: 00 [  251/20026 (  1%)], Train Loss: 2.56152\n","Epoch: 00 [  261/20026 (  1%)], Train Loss: 2.51586\n","Epoch: 00 [  271/20026 (  1%)], Train Loss: 2.46192\n","Epoch: 00 [  281/20026 (  1%)], Train Loss: 2.41123\n","Epoch: 00 [  291/20026 (  1%)], Train Loss: 2.35889\n","Epoch: 00 [  301/20026 (  2%)], Train Loss: 2.29916\n","Epoch: 00 [  311/20026 (  2%)], Train Loss: 2.23666\n","Epoch: 00 [  321/20026 (  2%)], Train Loss: 2.19072\n","Epoch: 00 [  331/20026 (  2%)], Train Loss: 2.15551\n","Epoch: 00 [  341/20026 (  2%)], Train Loss: 2.10868\n","Epoch: 00 [  351/20026 (  2%)], Train Loss: 2.06240\n","Epoch: 00 [  361/20026 (  2%)], Train Loss: 2.01853\n","Epoch: 00 [  371/20026 (  2%)], Train Loss: 1.97229\n","Epoch: 00 [  381/20026 (  2%)], Train Loss: 1.94314\n","Epoch: 00 [  391/20026 (  2%)], Train Loss: 1.90928\n","Epoch: 00 [  401/20026 (  2%)], Train Loss: 1.86762\n","Epoch: 00 [  411/20026 (  2%)], Train Loss: 1.83361\n","Epoch: 00 [  421/20026 (  2%)], Train Loss: 1.79385\n","Epoch: 00 [  431/20026 (  2%)], Train Loss: 1.76339\n","Epoch: 00 [  441/20026 (  2%)], Train Loss: 1.73157\n","Epoch: 00 [  451/20026 (  2%)], Train Loss: 1.70473\n","Epoch: 00 [  461/20026 (  2%)], Train Loss: 1.67344\n","Epoch: 00 [  471/20026 (  2%)], Train Loss: 1.64353\n","Epoch: 00 [  481/20026 (  2%)], Train Loss: 1.61071\n","Epoch: 00 [  491/20026 (  2%)], Train Loss: 1.60387\n","Epoch: 00 [  501/20026 (  3%)], Train Loss: 1.57751\n","Epoch: 00 [  511/20026 (  3%)], Train Loss: 1.55175\n","Epoch: 00 [  521/20026 (  3%)], Train Loss: 1.53586\n","Epoch: 00 [  531/20026 (  3%)], Train Loss: 1.51337\n","Epoch: 00 [  541/20026 (  3%)], Train Loss: 1.49716\n","Epoch: 00 [  551/20026 (  3%)], Train Loss: 1.48077\n","Epoch: 00 [  561/20026 (  3%)], Train Loss: 1.46040\n","Epoch: 00 [  571/20026 (  3%)], Train Loss: 1.43788\n","Epoch: 00 [  581/20026 (  3%)], Train Loss: 1.42427\n","Epoch: 00 [  591/20026 (  3%)], Train Loss: 1.41235\n","Epoch: 00 [  601/20026 (  3%)], Train Loss: 1.41406\n","Epoch: 00 [  611/20026 (  3%)], Train Loss: 1.39775\n","Epoch: 00 [  621/20026 (  3%)], Train Loss: 1.38322\n","Epoch: 00 [  631/20026 (  3%)], Train Loss: 1.36623\n","Epoch: 00 [  641/20026 (  3%)], Train Loss: 1.34728\n","Epoch: 00 [  651/20026 (  3%)], Train Loss: 1.33615\n","Epoch: 00 [  661/20026 (  3%)], Train Loss: 1.32730\n","Epoch: 00 [  671/20026 (  3%)], Train Loss: 1.31602\n","Epoch: 00 [  681/20026 (  3%)], Train Loss: 1.30241\n","Epoch: 00 [  691/20026 (  3%)], Train Loss: 1.28792\n","Epoch: 00 [  701/20026 (  4%)], Train Loss: 1.28109\n","Epoch: 00 [  711/20026 (  4%)], Train Loss: 1.26885\n","Epoch: 00 [  721/20026 (  4%)], Train Loss: 1.25854\n","Epoch: 00 [  731/20026 (  4%)], Train Loss: 1.24617\n","Epoch: 00 [  741/20026 (  4%)], Train Loss: 1.24280\n","Epoch: 00 [  751/20026 (  4%)], Train Loss: 1.23467\n","Epoch: 00 [  761/20026 (  4%)], Train Loss: 1.22255\n","Epoch: 00 [  771/20026 (  4%)], Train Loss: 1.21883\n","Epoch: 00 [  781/20026 (  4%)], Train Loss: 1.21163\n","Epoch: 00 [  791/20026 (  4%)], Train Loss: 1.19992\n","Epoch: 00 [  801/20026 (  4%)], Train Loss: 1.19160\n","Epoch: 00 [  811/20026 (  4%)], Train Loss: 1.17812\n","Epoch: 00 [  821/20026 (  4%)], Train Loss: 1.17192\n","Epoch: 00 [  831/20026 (  4%)], Train Loss: 1.16788\n","Epoch: 00 [  841/20026 (  4%)], Train Loss: 1.16200\n","Epoch: 00 [  851/20026 (  4%)], Train Loss: 1.15465\n","Epoch: 00 [  861/20026 (  4%)], Train Loss: 1.14421\n","Epoch: 00 [  871/20026 (  4%)], Train Loss: 1.13710\n","Epoch: 00 [  881/20026 (  4%)], Train Loss: 1.13272\n","Epoch: 00 [  891/20026 (  4%)], Train Loss: 1.12555\n","Epoch: 00 [  901/20026 (  4%)], Train Loss: 1.11826\n","Epoch: 00 [  911/20026 (  5%)], Train Loss: 1.10850\n","Epoch: 00 [  921/20026 (  5%)], Train Loss: 1.10319\n","Epoch: 00 [  931/20026 (  5%)], Train Loss: 1.09551\n","Epoch: 00 [  941/20026 (  5%)], Train Loss: 1.08581\n","Epoch: 00 [  951/20026 (  5%)], Train Loss: 1.07965\n","Epoch: 00 [  961/20026 (  5%)], Train Loss: 1.07483\n","Epoch: 00 [  971/20026 (  5%)], Train Loss: 1.07365\n","Epoch: 00 [  981/20026 (  5%)], Train Loss: 1.06870\n","Epoch: 00 [  991/20026 (  5%)], Train Loss: 1.06125\n","Epoch: 00 [ 1001/20026 (  5%)], Train Loss: 1.05898\n","Epoch: 00 [ 1011/20026 (  5%)], Train Loss: 1.05275\n","Epoch: 00 [ 1021/20026 (  5%)], Train Loss: 1.04298\n","Epoch: 00 [ 1031/20026 (  5%)], Train Loss: 1.03842\n","Epoch: 00 [ 1041/20026 (  5%)], Train Loss: 1.03185\n","Epoch: 00 [ 1051/20026 (  5%)], Train Loss: 1.02701\n","Epoch: 00 [ 1061/20026 (  5%)], Train Loss: 1.01979\n","Epoch: 00 [ 1071/20026 (  5%)], Train Loss: 1.01862\n","Epoch: 00 [ 1081/20026 (  5%)], Train Loss: 1.01291\n","Epoch: 00 [ 1091/20026 (  5%)], Train Loss: 1.01120\n","Epoch: 00 [ 1101/20026 (  5%)], Train Loss: 1.00915\n","Epoch: 00 [ 1111/20026 (  6%)], Train Loss: 1.00449\n","Epoch: 00 [ 1121/20026 (  6%)], Train Loss: 1.00486\n","Epoch: 00 [ 1131/20026 (  6%)], Train Loss: 1.00096\n","Epoch: 00 [ 1141/20026 (  6%)], Train Loss: 0.99360\n","Epoch: 00 [ 1151/20026 (  6%)], Train Loss: 0.98805\n","Epoch: 00 [ 1161/20026 (  6%)], Train Loss: 0.98680\n","Epoch: 00 [ 1171/20026 (  6%)], Train Loss: 0.98247\n","Epoch: 00 [ 1181/20026 (  6%)], Train Loss: 0.97676\n","Epoch: 00 [ 1191/20026 (  6%)], Train Loss: 0.96972\n","Epoch: 00 [ 1201/20026 (  6%)], Train Loss: 0.96478\n","Epoch: 00 [ 1211/20026 (  6%)], Train Loss: 0.96260\n","Epoch: 00 [ 1221/20026 (  6%)], Train Loss: 0.95998\n","Epoch: 00 [ 1231/20026 (  6%)], Train Loss: 0.95647\n","Epoch: 00 [ 1241/20026 (  6%)], Train Loss: 0.95225\n","Epoch: 00 [ 1251/20026 (  6%)], Train Loss: 0.95113\n","Epoch: 00 [ 1261/20026 (  6%)], Train Loss: 0.94659\n","Epoch: 00 [ 1271/20026 (  6%)], Train Loss: 0.94088\n","Epoch: 00 [ 1281/20026 (  6%)], Train Loss: 0.93798\n","Epoch: 00 [ 1291/20026 (  6%)], Train Loss: 0.93330\n","Epoch: 00 [ 1301/20026 (  6%)], Train Loss: 0.92786\n","Epoch: 00 [ 1311/20026 (  7%)], Train Loss: 0.92440\n","Epoch: 00 [ 1321/20026 (  7%)], Train Loss: 0.92239\n","Epoch: 00 [ 1331/20026 (  7%)], Train Loss: 0.92161\n","Epoch: 00 [ 1341/20026 (  7%)], Train Loss: 0.91896\n","Epoch: 00 [ 1351/20026 (  7%)], Train Loss: 0.91712\n","Epoch: 00 [ 1361/20026 (  7%)], Train Loss: 0.91082\n","Epoch: 00 [ 1371/20026 (  7%)], Train Loss: 0.90777\n","Epoch: 00 [ 1381/20026 (  7%)], Train Loss: 0.90623\n","Epoch: 00 [ 1391/20026 (  7%)], Train Loss: 0.90282\n","Epoch: 00 [ 1401/20026 (  7%)], Train Loss: 0.90114\n","Epoch: 00 [ 1411/20026 (  7%)], Train Loss: 0.90131\n","Epoch: 00 [ 1421/20026 (  7%)], Train Loss: 0.89712\n","Epoch: 00 [ 1431/20026 (  7%)], Train Loss: 0.89642\n","Epoch: 00 [ 1441/20026 (  7%)], Train Loss: 0.89346\n","Epoch: 00 [ 1451/20026 (  7%)], Train Loss: 0.89068\n","Epoch: 00 [ 1461/20026 (  7%)], Train Loss: 0.88711\n","Epoch: 00 [ 1471/20026 (  7%)], Train Loss: 0.88287\n","Epoch: 00 [ 1481/20026 (  7%)], Train Loss: 0.87792\n","Epoch: 00 [ 1491/20026 (  7%)], Train Loss: 0.87581\n","Epoch: 00 [ 1501/20026 (  7%)], Train Loss: 0.87516\n","Epoch: 00 [ 1511/20026 (  8%)], Train Loss: 0.86983\n","Epoch: 00 [ 1521/20026 (  8%)], Train Loss: 0.86901\n","Epoch: 00 [ 1531/20026 (  8%)], Train Loss: 0.86635\n","Epoch: 00 [ 1541/20026 (  8%)], Train Loss: 0.86542\n","Epoch: 00 [ 1551/20026 (  8%)], Train Loss: 0.86473\n","Epoch: 00 [ 1561/20026 (  8%)], Train Loss: 0.86028\n","Epoch: 00 [ 1571/20026 (  8%)], Train Loss: 0.85693\n","Epoch: 00 [ 1581/20026 (  8%)], Train Loss: 0.85409\n","Epoch: 00 [ 1591/20026 (  8%)], Train Loss: 0.85187\n","Epoch: 00 [ 1601/20026 (  8%)], Train Loss: 0.84848\n","Epoch: 00 [ 1611/20026 (  8%)], Train Loss: 0.84505\n","Epoch: 00 [ 1621/20026 (  8%)], Train Loss: 0.84429\n","Epoch: 00 [ 1631/20026 (  8%)], Train Loss: 0.84279\n","Epoch: 00 [ 1641/20026 (  8%)], Train Loss: 0.84048\n","Epoch: 00 [ 1651/20026 (  8%)], Train Loss: 0.83583\n","Epoch: 00 [ 1661/20026 (  8%)], Train Loss: 0.83515\n","Epoch: 00 [ 1671/20026 (  8%)], Train Loss: 0.83411\n","Epoch: 00 [ 1681/20026 (  8%)], Train Loss: 0.83030\n","Epoch: 00 [ 1691/20026 (  8%)], Train Loss: 0.82788\n","Epoch: 00 [ 1701/20026 (  8%)], Train Loss: 0.82538\n","Epoch: 00 [ 1711/20026 (  9%)], Train Loss: 0.82349\n","Epoch: 00 [ 1721/20026 (  9%)], Train Loss: 0.82068\n","Epoch: 00 [ 1731/20026 (  9%)], Train Loss: 0.81826\n","Epoch: 00 [ 1741/20026 (  9%)], Train Loss: 0.81774\n","Epoch: 00 [ 1751/20026 (  9%)], Train Loss: 0.81512\n","Epoch: 00 [ 1761/20026 (  9%)], Train Loss: 0.81247\n","Epoch: 00 [ 1771/20026 (  9%)], Train Loss: 0.81056\n","Epoch: 00 [ 1781/20026 (  9%)], Train Loss: 0.80952\n","Epoch: 00 [ 1791/20026 (  9%)], Train Loss: 0.80643\n","Epoch: 00 [ 1801/20026 (  9%)], Train Loss: 0.80308\n","Epoch: 00 [ 1811/20026 (  9%)], Train Loss: 0.80057\n","Epoch: 00 [ 1821/20026 (  9%)], Train Loss: 0.79939\n","Epoch: 00 [ 1831/20026 (  9%)], Train Loss: 0.79968\n","Epoch: 00 [ 1841/20026 (  9%)], Train Loss: 0.79867\n","Epoch: 00 [ 1851/20026 (  9%)], Train Loss: 0.79641\n","Epoch: 00 [ 1861/20026 (  9%)], Train Loss: 0.79753\n","Epoch: 00 [ 1871/20026 (  9%)], Train Loss: 0.79538\n","Epoch: 00 [ 1881/20026 (  9%)], Train Loss: 0.79538\n","Epoch: 00 [ 1891/20026 (  9%)], Train Loss: 0.79276\n","Epoch: 00 [ 1901/20026 (  9%)], Train Loss: 0.79209\n","Epoch: 00 [ 1911/20026 ( 10%)], Train Loss: 0.78876\n","Epoch: 00 [ 1921/20026 ( 10%)], Train Loss: 0.78666\n","Epoch: 00 [ 1931/20026 ( 10%)], Train Loss: 0.78508\n","Epoch: 00 [ 1941/20026 ( 10%)], Train Loss: 0.78330\n","Epoch: 00 [ 1951/20026 ( 10%)], Train Loss: 0.78024\n","Epoch: 00 [ 1961/20026 ( 10%)], Train Loss: 0.77862\n","Epoch: 00 [ 1971/20026 ( 10%)], Train Loss: 0.77621\n","Epoch: 00 [ 1981/20026 ( 10%)], Train Loss: 0.77342\n","Epoch: 00 [ 1991/20026 ( 10%)], Train Loss: 0.77245\n","Epoch: 00 [ 2001/20026 ( 10%)], Train Loss: 0.77107\n","Epoch: 00 [ 2011/20026 ( 10%)], Train Loss: 0.77117\n","Epoch: 00 [ 2021/20026 ( 10%)], Train Loss: 0.76950\n","Epoch: 00 [ 2031/20026 ( 10%)], Train Loss: 0.76768\n","Epoch: 00 [ 2041/20026 ( 10%)], Train Loss: 0.76517\n","Epoch: 00 [ 2051/20026 ( 10%)], Train Loss: 0.76193\n","Epoch: 00 [ 2061/20026 ( 10%)], Train Loss: 0.76051\n","Epoch: 00 [ 2071/20026 ( 10%)], Train Loss: 0.75869\n","Epoch: 00 [ 2081/20026 ( 10%)], Train Loss: 0.75666\n","Epoch: 00 [ 2091/20026 ( 10%)], Train Loss: 0.75730\n","Epoch: 00 [ 2101/20026 ( 10%)], Train Loss: 0.75818\n","Epoch: 00 [ 2111/20026 ( 11%)], Train Loss: 0.75814\n","Epoch: 00 [ 2121/20026 ( 11%)], Train Loss: 0.75953\n","Epoch: 00 [ 2131/20026 ( 11%)], Train Loss: 0.75816\n","Epoch: 00 [ 2141/20026 ( 11%)], Train Loss: 0.75818\n","Epoch: 00 [ 2151/20026 ( 11%)], Train Loss: 0.75732\n","Epoch: 00 [ 2161/20026 ( 11%)], Train Loss: 0.75577\n","Epoch: 00 [ 2171/20026 ( 11%)], Train Loss: 0.75295\n","Epoch: 00 [ 2181/20026 ( 11%)], Train Loss: 0.75352\n","Epoch: 00 [ 2191/20026 ( 11%)], Train Loss: 0.75194\n","Epoch: 00 [ 2201/20026 ( 11%)], Train Loss: 0.75054\n","Epoch: 00 [ 2211/20026 ( 11%)], Train Loss: 0.75078\n","Epoch: 00 [ 2221/20026 ( 11%)], Train Loss: 0.75097\n","Epoch: 00 [ 2231/20026 ( 11%)], Train Loss: 0.74910\n","Epoch: 00 [ 2241/20026 ( 11%)], Train Loss: 0.74992\n","Epoch: 00 [ 2251/20026 ( 11%)], Train Loss: 0.74816\n","Epoch: 00 [ 2261/20026 ( 11%)], Train Loss: 0.74593\n","Epoch: 00 [ 2271/20026 ( 11%)], Train Loss: 0.74421\n","Epoch: 00 [ 2281/20026 ( 11%)], Train Loss: 0.74434\n","Epoch: 00 [ 2291/20026 ( 11%)], Train Loss: 0.74489\n","Epoch: 00 [ 2301/20026 ( 11%)], Train Loss: 0.74296\n","Epoch: 00 [ 2311/20026 ( 12%)], Train Loss: 0.74278\n","Epoch: 00 [ 2321/20026 ( 12%)], Train Loss: 0.74024\n","Epoch: 00 [ 2331/20026 ( 12%)], Train Loss: 0.73960\n","Epoch: 00 [ 2341/20026 ( 12%)], Train Loss: 0.73990\n","Epoch: 00 [ 2351/20026 ( 12%)], Train Loss: 0.74190\n","Epoch: 00 [ 2361/20026 ( 12%)], Train Loss: 0.74031\n","Epoch: 00 [ 2371/20026 ( 12%)], Train Loss: 0.74020\n","Epoch: 00 [ 2381/20026 ( 12%)], Train Loss: 0.73949\n","Epoch: 00 [ 2391/20026 ( 12%)], Train Loss: 0.73691\n","Epoch: 00 [ 2401/20026 ( 12%)], Train Loss: 0.73531\n","Epoch: 00 [ 2411/20026 ( 12%)], Train Loss: 0.73357\n","Epoch: 00 [ 2421/20026 ( 12%)], Train Loss: 0.73212\n","Epoch: 00 [ 2431/20026 ( 12%)], Train Loss: 0.73281\n","Epoch: 00 [ 2441/20026 ( 12%)], Train Loss: 0.73369\n","Epoch: 00 [ 2451/20026 ( 12%)], Train Loss: 0.73234\n","Epoch: 00 [ 2461/20026 ( 12%)], Train Loss: 0.73273\n","Epoch: 00 [ 2471/20026 ( 12%)], Train Loss: 0.73057\n","Epoch: 00 [ 2481/20026 ( 12%)], Train Loss: 0.72841\n","Epoch: 00 [ 2491/20026 ( 12%)], Train Loss: 0.72662\n","Epoch: 00 [ 2501/20026 ( 12%)], Train Loss: 0.72450\n","Epoch: 00 [ 2511/20026 ( 13%)], Train Loss: 0.72244\n","Epoch: 00 [ 2521/20026 ( 13%)], Train Loss: 0.72117\n","Epoch: 00 [ 2531/20026 ( 13%)], Train Loss: 0.72064\n","Epoch: 00 [ 2541/20026 ( 13%)], Train Loss: 0.72083\n","Epoch: 00 [ 2551/20026 ( 13%)], Train Loss: 0.72016\n","Epoch: 00 [ 2561/20026 ( 13%)], Train Loss: 0.72075\n","Epoch: 00 [ 2571/20026 ( 13%)], Train Loss: 0.71912\n","Epoch: 00 [ 2581/20026 ( 13%)], Train Loss: 0.71946\n","Epoch: 00 [ 2591/20026 ( 13%)], Train Loss: 0.71745\n","Epoch: 00 [ 2601/20026 ( 13%)], Train Loss: 0.71789\n","Epoch: 00 [ 2611/20026 ( 13%)], Train Loss: 0.71672\n","Epoch: 00 [ 2621/20026 ( 13%)], Train Loss: 0.71510\n","Epoch: 00 [ 2631/20026 ( 13%)], Train Loss: 0.71411\n","Epoch: 00 [ 2641/20026 ( 13%)], Train Loss: 0.71320\n","Epoch: 00 [ 2651/20026 ( 13%)], Train Loss: 0.71218\n","Epoch: 00 [ 2661/20026 ( 13%)], Train Loss: 0.71040\n","Epoch: 00 [ 2671/20026 ( 13%)], Train Loss: 0.71020\n","Epoch: 00 [ 2681/20026 ( 13%)], Train Loss: 0.71015\n","Epoch: 00 [ 2691/20026 ( 13%)], Train Loss: 0.71031\n","Epoch: 00 [ 2701/20026 ( 13%)], Train Loss: 0.70952\n","Epoch: 00 [ 2711/20026 ( 14%)], Train Loss: 0.70768\n","Epoch: 00 [ 2721/20026 ( 14%)], Train Loss: 0.70548\n","Epoch: 00 [ 2731/20026 ( 14%)], Train Loss: 0.70472\n","Epoch: 00 [ 2741/20026 ( 14%)], Train Loss: 0.70572\n","Epoch: 00 [ 2751/20026 ( 14%)], Train Loss: 0.70424\n","Epoch: 00 [ 2761/20026 ( 14%)], Train Loss: 0.70227\n","Epoch: 00 [ 2771/20026 ( 14%)], Train Loss: 0.70227\n","Epoch: 00 [ 2781/20026 ( 14%)], Train Loss: 0.70020\n","Epoch: 00 [ 2791/20026 ( 14%)], Train Loss: 0.69945\n","Epoch: 00 [ 2801/20026 ( 14%)], Train Loss: 0.69993\n","Epoch: 00 [ 2811/20026 ( 14%)], Train Loss: 0.69842\n","Epoch: 00 [ 2821/20026 ( 14%)], Train Loss: 0.69706\n","Epoch: 00 [ 2831/20026 ( 14%)], Train Loss: 0.69514\n","Epoch: 00 [ 2841/20026 ( 14%)], Train Loss: 0.69454\n","Epoch: 00 [ 2851/20026 ( 14%)], Train Loss: 0.69400\n","Epoch: 00 [ 2861/20026 ( 14%)], Train Loss: 0.69395\n","Epoch: 00 [ 2871/20026 ( 14%)], Train Loss: 0.69337\n","Epoch: 00 [ 2881/20026 ( 14%)], Train Loss: 0.69175\n","Epoch: 00 [ 2891/20026 ( 14%)], Train Loss: 0.69068\n","Epoch: 00 [ 2901/20026 ( 14%)], Train Loss: 0.68930\n","Epoch: 00 [ 2911/20026 ( 15%)], Train Loss: 0.68865\n","Epoch: 00 [ 2921/20026 ( 15%)], Train Loss: 0.68812\n","Epoch: 00 [ 2931/20026 ( 15%)], Train Loss: 0.68845\n","Epoch: 00 [ 2941/20026 ( 15%)], Train Loss: 0.69065\n","Epoch: 00 [ 2951/20026 ( 15%)], Train Loss: 0.69010\n","Epoch: 00 [ 2961/20026 ( 15%)], Train Loss: 0.69125\n","Epoch: 00 [ 2971/20026 ( 15%)], Train Loss: 0.69282\n","Epoch: 00 [ 2981/20026 ( 15%)], Train Loss: 0.69291\n","Epoch: 00 [ 2991/20026 ( 15%)], Train Loss: 0.69189\n","Epoch: 00 [ 3001/20026 ( 15%)], Train Loss: 0.69092\n","Epoch: 00 [ 3011/20026 ( 15%)], Train Loss: 0.68998\n","Epoch: 00 [ 3021/20026 ( 15%)], Train Loss: 0.69126\n","Epoch: 00 [ 3031/20026 ( 15%)], Train Loss: 0.69061\n","Epoch: 00 [ 3041/20026 ( 15%)], Train Loss: 0.69034\n","Epoch: 00 [ 3051/20026 ( 15%)], Train Loss: 0.68923\n","Epoch: 00 [ 3061/20026 ( 15%)], Train Loss: 0.68786\n","Epoch: 00 [ 3071/20026 ( 15%)], Train Loss: 0.68710\n","Epoch: 00 [ 3081/20026 ( 15%)], Train Loss: 0.68865\n","Epoch: 00 [ 3091/20026 ( 15%)], Train Loss: 0.68877\n","Epoch: 00 [ 3101/20026 ( 15%)], Train Loss: 0.68767\n","Epoch: 00 [ 3111/20026 ( 16%)], Train Loss: 0.68596\n","Epoch: 00 [ 3121/20026 ( 16%)], Train Loss: 0.68486\n","Epoch: 00 [ 3131/20026 ( 16%)], Train Loss: 0.68427\n","Epoch: 00 [ 3141/20026 ( 16%)], Train Loss: 0.68271\n","Epoch: 00 [ 3151/20026 ( 16%)], Train Loss: 0.68222\n","Epoch: 00 [ 3161/20026 ( 16%)], Train Loss: 0.68138\n","Epoch: 00 [ 3171/20026 ( 16%)], Train Loss: 0.68027\n","Epoch: 00 [ 3181/20026 ( 16%)], Train Loss: 0.68142\n","Epoch: 00 [ 3191/20026 ( 16%)], Train Loss: 0.68032\n","Epoch: 00 [ 3201/20026 ( 16%)], Train Loss: 0.67946\n","Epoch: 00 [ 3211/20026 ( 16%)], Train Loss: 0.67868\n","Epoch: 00 [ 3221/20026 ( 16%)], Train Loss: 0.67990\n","Epoch: 00 [ 3231/20026 ( 16%)], Train Loss: 0.67871\n","Epoch: 00 [ 3241/20026 ( 16%)], Train Loss: 0.67801\n","Epoch: 00 [ 3251/20026 ( 16%)], Train Loss: 0.67642\n","Epoch: 00 [ 3261/20026 ( 16%)], Train Loss: 0.67629\n","Epoch: 00 [ 3271/20026 ( 16%)], Train Loss: 0.67640\n","Epoch: 00 [ 3281/20026 ( 16%)], Train Loss: 0.67605\n","Epoch: 00 [ 3291/20026 ( 16%)], Train Loss: 0.67560\n","Epoch: 00 [ 3301/20026 ( 16%)], Train Loss: 0.67570\n","Epoch: 00 [ 3311/20026 ( 17%)], Train Loss: 0.67642\n","Epoch: 00 [ 3321/20026 ( 17%)], Train Loss: 0.67603\n","Epoch: 00 [ 3331/20026 ( 17%)], Train Loss: 0.67616\n","Epoch: 00 [ 3341/20026 ( 17%)], Train Loss: 0.67561\n","Epoch: 00 [ 3351/20026 ( 17%)], Train Loss: 0.67520\n","Epoch: 00 [ 3361/20026 ( 17%)], Train Loss: 0.67499\n","Epoch: 00 [ 3371/20026 ( 17%)], Train Loss: 0.67524\n","Epoch: 00 [ 3381/20026 ( 17%)], Train Loss: 0.67386\n","Epoch: 00 [ 3391/20026 ( 17%)], Train Loss: 0.67449\n","Epoch: 00 [ 3401/20026 ( 17%)], Train Loss: 0.67344\n","Epoch: 00 [ 3411/20026 ( 17%)], Train Loss: 0.67388\n","Epoch: 00 [ 3421/20026 ( 17%)], Train Loss: 0.67300\n","Epoch: 00 [ 3431/20026 ( 17%)], Train Loss: 0.67268\n","Epoch: 00 [ 3441/20026 ( 17%)], Train Loss: 0.67213\n","Epoch: 00 [ 3451/20026 ( 17%)], Train Loss: 0.67206\n","Epoch: 00 [ 3461/20026 ( 17%)], Train Loss: 0.67049\n","Epoch: 00 [ 3471/20026 ( 17%)], Train Loss: 0.66973\n","Epoch: 00 [ 3481/20026 ( 17%)], Train Loss: 0.66924\n","Epoch: 00 [ 3491/20026 ( 17%)], Train Loss: 0.66940\n","Epoch: 00 [ 3501/20026 ( 17%)], Train Loss: 0.66906\n","Epoch: 00 [ 3511/20026 ( 18%)], Train Loss: 0.66788\n","Epoch: 00 [ 3521/20026 ( 18%)], Train Loss: 0.66862\n","Epoch: 00 [ 3531/20026 ( 18%)], Train Loss: 0.66781\n","Epoch: 00 [ 3541/20026 ( 18%)], Train Loss: 0.66738\n","Epoch: 00 [ 3551/20026 ( 18%)], Train Loss: 0.66648\n","Epoch: 00 [ 3561/20026 ( 18%)], Train Loss: 0.66581\n","Epoch: 00 [ 3571/20026 ( 18%)], Train Loss: 0.66424\n","Epoch: 00 [ 3581/20026 ( 18%)], Train Loss: 0.66454\n","Epoch: 00 [ 3591/20026 ( 18%)], Train Loss: 0.66612\n","Epoch: 00 [ 3601/20026 ( 18%)], Train Loss: 0.66538\n","Epoch: 00 [ 3611/20026 ( 18%)], Train Loss: 0.66595\n","Epoch: 00 [ 3621/20026 ( 18%)], Train Loss: 0.66462\n","Epoch: 00 [ 3631/20026 ( 18%)], Train Loss: 0.66426\n","Epoch: 00 [ 3641/20026 ( 18%)], Train Loss: 0.66353\n","Epoch: 00 [ 3651/20026 ( 18%)], Train Loss: 0.66519\n","Epoch: 00 [ 3661/20026 ( 18%)], Train Loss: 0.66469\n","Epoch: 00 [ 3671/20026 ( 18%)], Train Loss: 0.66413\n","Epoch: 00 [ 3681/20026 ( 18%)], Train Loss: 0.66315\n","Epoch: 00 [ 3691/20026 ( 18%)], Train Loss: 0.66281\n","Epoch: 00 [ 3701/20026 ( 18%)], Train Loss: 0.66110\n","Epoch: 00 [ 3711/20026 ( 19%)], Train Loss: 0.66005\n","Epoch: 00 [ 3721/20026 ( 19%)], Train Loss: 0.65925\n","Epoch: 00 [ 3731/20026 ( 19%)], Train Loss: 0.65937\n","Epoch: 00 [ 3741/20026 ( 19%)], Train Loss: 0.65845\n","Epoch: 00 [ 3751/20026 ( 19%)], Train Loss: 0.65766\n","Epoch: 00 [ 3761/20026 ( 19%)], Train Loss: 0.65714\n","Epoch: 00 [ 3771/20026 ( 19%)], Train Loss: 0.65834\n","Epoch: 00 [ 3781/20026 ( 19%)], Train Loss: 0.65974\n","Epoch: 00 [ 3791/20026 ( 19%)], Train Loss: 0.65932\n","Epoch: 00 [ 3801/20026 ( 19%)], Train Loss: 0.66028\n","Epoch: 00 [ 3811/20026 ( 19%)], Train Loss: 0.66039\n","Epoch: 00 [ 3821/20026 ( 19%)], Train Loss: 0.66075\n","Epoch: 00 [ 3831/20026 ( 19%)], Train Loss: 0.66310\n","Epoch: 00 [ 3841/20026 ( 19%)], Train Loss: 0.66263\n","Epoch: 00 [ 3851/20026 ( 19%)], Train Loss: 0.66368\n","Epoch: 00 [ 3861/20026 ( 19%)], Train Loss: 0.66463\n","Epoch: 00 [ 3871/20026 ( 19%)], Train Loss: 0.66423\n","Epoch: 00 [ 3881/20026 ( 19%)], Train Loss: 0.66486\n","Epoch: 00 [ 3891/20026 ( 19%)], Train Loss: 0.66385\n","Epoch: 00 [ 3901/20026 ( 19%)], Train Loss: 0.66371\n","Epoch: 00 [ 3911/20026 ( 20%)], Train Loss: 0.66297\n","Epoch: 00 [ 3921/20026 ( 20%)], Train Loss: 0.66283\n","Epoch: 00 [ 3931/20026 ( 20%)], Train Loss: 0.66302\n","Epoch: 00 [ 3941/20026 ( 20%)], Train Loss: 0.66290\n","Epoch: 00 [ 3951/20026 ( 20%)], Train Loss: 0.66197\n","Epoch: 00 [ 3961/20026 ( 20%)], Train Loss: 0.66095\n","Epoch: 00 [ 3971/20026 ( 20%)], Train Loss: 0.66112\n","Epoch: 00 [ 3981/20026 ( 20%)], Train Loss: 0.66060\n","Epoch: 00 [ 3991/20026 ( 20%)], Train Loss: 0.66030\n","Epoch: 00 [ 4001/20026 ( 20%)], Train Loss: 0.65888\n","Epoch: 00 [ 4011/20026 ( 20%)], Train Loss: 0.65876\n","Epoch: 00 [ 4021/20026 ( 20%)], Train Loss: 0.65809\n","Epoch: 00 [ 4031/20026 ( 20%)], Train Loss: 0.65747\n","Epoch: 00 [ 4041/20026 ( 20%)], Train Loss: 0.65745\n","Epoch: 00 [ 4051/20026 ( 20%)], Train Loss: 0.65771\n","Epoch: 00 [ 4061/20026 ( 20%)], Train Loss: 0.65757\n","Epoch: 00 [ 4071/20026 ( 20%)], Train Loss: 0.65699\n","Epoch: 00 [ 4081/20026 ( 20%)], Train Loss: 0.65598\n","Epoch: 00 [ 4091/20026 ( 20%)], Train Loss: 0.65529\n","Epoch: 00 [ 4101/20026 ( 20%)], Train Loss: 0.65647\n","Epoch: 00 [ 4111/20026 ( 21%)], Train Loss: 0.65692\n","Epoch: 00 [ 4121/20026 ( 21%)], Train Loss: 0.65583\n","Epoch: 00 [ 4131/20026 ( 21%)], Train Loss: 0.65466\n","Epoch: 00 [ 4141/20026 ( 21%)], Train Loss: 0.65389\n","Epoch: 00 [ 4151/20026 ( 21%)], Train Loss: 0.65270\n","Epoch: 00 [ 4161/20026 ( 21%)], Train Loss: 0.65249\n","Epoch: 00 [ 4171/20026 ( 21%)], Train Loss: 0.65211\n","Epoch: 00 [ 4181/20026 ( 21%)], Train Loss: 0.65123\n","Epoch: 00 [ 4191/20026 ( 21%)], Train Loss: 0.65043\n","Epoch: 00 [ 4201/20026 ( 21%)], Train Loss: 0.64997\n","Epoch: 00 [ 4211/20026 ( 21%)], Train Loss: 0.64986\n","Epoch: 00 [ 4221/20026 ( 21%)], Train Loss: 0.64837\n","Epoch: 00 [ 4231/20026 ( 21%)], Train Loss: 0.64747\n","Epoch: 00 [ 4241/20026 ( 21%)], Train Loss: 0.64679\n","Epoch: 00 [ 4251/20026 ( 21%)], Train Loss: 0.64877\n","Epoch: 00 [ 4261/20026 ( 21%)], Train Loss: 0.64811\n","Epoch: 00 [ 4271/20026 ( 21%)], Train Loss: 0.64773\n","Epoch: 00 [ 4281/20026 ( 21%)], Train Loss: 0.64745\n","Epoch: 00 [ 4291/20026 ( 21%)], Train Loss: 0.64749\n","Epoch: 00 [ 4301/20026 ( 21%)], Train Loss: 0.64658\n","Epoch: 00 [ 4311/20026 ( 22%)], Train Loss: 0.64860\n","Epoch: 00 [ 4321/20026 ( 22%)], Train Loss: 0.64750\n","Epoch: 00 [ 4331/20026 ( 22%)], Train Loss: 0.64768\n","Epoch: 00 [ 4341/20026 ( 22%)], Train Loss: 0.64663\n","Epoch: 00 [ 4351/20026 ( 22%)], Train Loss: 0.64687\n","Epoch: 00 [ 4361/20026 ( 22%)], Train Loss: 0.64630\n","Epoch: 00 [ 4371/20026 ( 22%)], Train Loss: 0.64689\n","Epoch: 00 [ 4381/20026 ( 22%)], Train Loss: 0.64586\n","Epoch: 00 [ 4391/20026 ( 22%)], Train Loss: 0.64618\n","Epoch: 00 [ 4401/20026 ( 22%)], Train Loss: 0.64584\n","Epoch: 00 [ 4411/20026 ( 22%)], Train Loss: 0.64539\n","Epoch: 00 [ 4421/20026 ( 22%)], Train Loss: 0.64451\n","Epoch: 00 [ 4431/20026 ( 22%)], Train Loss: 0.64389\n","Epoch: 00 [ 4441/20026 ( 22%)], Train Loss: 0.64341\n","Epoch: 00 [ 4451/20026 ( 22%)], Train Loss: 0.64470\n","Epoch: 00 [ 4461/20026 ( 22%)], Train Loss: 0.64451\n","Epoch: 00 [ 4471/20026 ( 22%)], Train Loss: 0.64415\n","Epoch: 00 [ 4481/20026 ( 22%)], Train Loss: 0.64398\n","Epoch: 00 [ 4491/20026 ( 22%)], Train Loss: 0.64301\n","Epoch: 00 [ 4501/20026 ( 22%)], Train Loss: 0.64281\n","Epoch: 00 [ 4511/20026 ( 23%)], Train Loss: 0.64373\n","Epoch: 00 [ 4521/20026 ( 23%)], Train Loss: 0.64308\n","Epoch: 00 [ 4531/20026 ( 23%)], Train Loss: 0.64289\n","Epoch: 00 [ 4541/20026 ( 23%)], Train Loss: 0.64241\n","Epoch: 00 [ 4551/20026 ( 23%)], Train Loss: 0.64221\n","Epoch: 00 [ 4561/20026 ( 23%)], Train Loss: 0.64152\n","Epoch: 00 [ 4571/20026 ( 23%)], Train Loss: 0.64141\n","Epoch: 00 [ 4581/20026 ( 23%)], Train Loss: 0.64048\n","Epoch: 00 [ 4591/20026 ( 23%)], Train Loss: 0.63998\n","Epoch: 00 [ 4601/20026 ( 23%)], Train Loss: 0.64046\n","Epoch: 00 [ 4611/20026 ( 23%)], Train Loss: 0.64013\n","Epoch: 00 [ 4621/20026 ( 23%)], Train Loss: 0.63999\n","Epoch: 00 [ 4631/20026 ( 23%)], Train Loss: 0.63932\n","Epoch: 00 [ 4641/20026 ( 23%)], Train Loss: 0.64014\n","Epoch: 00 [ 4651/20026 ( 23%)], Train Loss: 0.64016\n","Epoch: 00 [ 4661/20026 ( 23%)], Train Loss: 0.64014\n","Epoch: 00 [ 4671/20026 ( 23%)], Train Loss: 0.63936\n","Epoch: 00 [ 4681/20026 ( 23%)], Train Loss: 0.63920\n","Epoch: 00 [ 4691/20026 ( 23%)], Train Loss: 0.63853\n","Epoch: 00 [ 4701/20026 ( 23%)], Train Loss: 0.63969\n","Epoch: 00 [ 4711/20026 ( 24%)], Train Loss: 0.63894\n","Epoch: 00 [ 4721/20026 ( 24%)], Train Loss: 0.63840\n","Epoch: 00 [ 4731/20026 ( 24%)], Train Loss: 0.63772\n","Epoch: 00 [ 4741/20026 ( 24%)], Train Loss: 0.63717\n","Epoch: 00 [ 4751/20026 ( 24%)], Train Loss: 0.63702\n","Epoch: 00 [ 4761/20026 ( 24%)], Train Loss: 0.63600\n","Epoch: 00 [ 4771/20026 ( 24%)], Train Loss: 0.63482\n","Epoch: 00 [ 4781/20026 ( 24%)], Train Loss: 0.63463\n","Epoch: 00 [ 4791/20026 ( 24%)], Train Loss: 0.63432\n","Epoch: 00 [ 4801/20026 ( 24%)], Train Loss: 0.63432\n","Epoch: 00 [ 4811/20026 ( 24%)], Train Loss: 0.63326\n","Epoch: 00 [ 4821/20026 ( 24%)], Train Loss: 0.63317\n","Epoch: 00 [ 4831/20026 ( 24%)], Train Loss: 0.63239\n","Epoch: 00 [ 4841/20026 ( 24%)], Train Loss: 0.63153\n","Epoch: 00 [ 4851/20026 ( 24%)], Train Loss: 0.63218\n","Epoch: 00 [ 4861/20026 ( 24%)], Train Loss: 0.63178\n","Epoch: 00 [ 4871/20026 ( 24%)], Train Loss: 0.63142\n","Epoch: 00 [ 4881/20026 ( 24%)], Train Loss: 0.63068\n","Epoch: 00 [ 4891/20026 ( 24%)], Train Loss: 0.62988\n","Epoch: 00 [ 4901/20026 ( 24%)], Train Loss: 0.62989\n","Epoch: 00 [ 4911/20026 ( 25%)], Train Loss: 0.62990\n","Epoch: 00 [ 4921/20026 ( 25%)], Train Loss: 0.62917\n","Epoch: 00 [ 4931/20026 ( 25%)], Train Loss: 0.62825\n","Epoch: 00 [ 4941/20026 ( 25%)], Train Loss: 0.62782\n","Epoch: 00 [ 4951/20026 ( 25%)], Train Loss: 0.62742\n","Epoch: 00 [ 4961/20026 ( 25%)], Train Loss: 0.62815\n","Epoch: 00 [ 4971/20026 ( 25%)], Train Loss: 0.62823\n","Epoch: 00 [ 4981/20026 ( 25%)], Train Loss: 0.62806\n","Epoch: 00 [ 4991/20026 ( 25%)], Train Loss: 0.62818\n","Epoch: 00 [ 5001/20026 ( 25%)], Train Loss: 0.62877\n","Epoch: 00 [ 5011/20026 ( 25%)], Train Loss: 0.62805\n","Epoch: 00 [ 5021/20026 ( 25%)], Train Loss: 0.62743\n","Epoch: 00 [ 5031/20026 ( 25%)], Train Loss: 0.62750\n","Epoch: 00 [ 5041/20026 ( 25%)], Train Loss: 0.62676\n","Epoch: 00 [ 5051/20026 ( 25%)], Train Loss: 0.62610\n","Epoch: 00 [ 5061/20026 ( 25%)], Train Loss: 0.62550\n","Epoch: 00 [ 5071/20026 ( 25%)], Train Loss: 0.62526\n","Epoch: 00 [ 5081/20026 ( 25%)], Train Loss: 0.62560\n","Epoch: 00 [ 5091/20026 ( 25%)], Train Loss: 0.62511\n","Epoch: 00 [ 5101/20026 ( 25%)], Train Loss: 0.62472\n","Epoch: 00 [ 5111/20026 ( 26%)], Train Loss: 0.62513\n","Epoch: 00 [ 5121/20026 ( 26%)], Train Loss: 0.62470\n","Epoch: 00 [ 5131/20026 ( 26%)], Train Loss: 0.62556\n","Epoch: 00 [ 5141/20026 ( 26%)], Train Loss: 0.62585\n","Epoch: 00 [ 5151/20026 ( 26%)], Train Loss: 0.62588\n","Epoch: 00 [ 5161/20026 ( 26%)], Train Loss: 0.62549\n","Epoch: 00 [ 5171/20026 ( 26%)], Train Loss: 0.62549\n","Epoch: 00 [ 5181/20026 ( 26%)], Train Loss: 0.62570\n","Epoch: 00 [ 5191/20026 ( 26%)], Train Loss: 0.62576\n","Epoch: 00 [ 5201/20026 ( 26%)], Train Loss: 0.62501\n","Epoch: 00 [ 5211/20026 ( 26%)], Train Loss: 0.62535\n","Epoch: 00 [ 5221/20026 ( 26%)], Train Loss: 0.62513\n","Epoch: 00 [ 5231/20026 ( 26%)], Train Loss: 0.62421\n","Epoch: 00 [ 5241/20026 ( 26%)], Train Loss: 0.62345\n","Epoch: 00 [ 5251/20026 ( 26%)], Train Loss: 0.62383\n","Epoch: 00 [ 5261/20026 ( 26%)], Train Loss: 0.62366\n","Epoch: 00 [ 5271/20026 ( 26%)], Train Loss: 0.62265\n","Epoch: 00 [ 5281/20026 ( 26%)], Train Loss: 0.62262\n","Epoch: 00 [ 5291/20026 ( 26%)], Train Loss: 0.62371\n","Epoch: 00 [ 5301/20026 ( 26%)], Train Loss: 0.62487\n","Epoch: 00 [ 5311/20026 ( 27%)], Train Loss: 0.62391\n","Epoch: 00 [ 5321/20026 ( 27%)], Train Loss: 0.62392\n","Epoch: 00 [ 5331/20026 ( 27%)], Train Loss: 0.62353\n","Epoch: 00 [ 5341/20026 ( 27%)], Train Loss: 0.62341\n","Epoch: 00 [ 5351/20026 ( 27%)], Train Loss: 0.62338\n","Epoch: 00 [ 5361/20026 ( 27%)], Train Loss: 0.62251\n","Epoch: 00 [ 5371/20026 ( 27%)], Train Loss: 0.62180\n","Epoch: 00 [ 5381/20026 ( 27%)], Train Loss: 0.62130\n","Epoch: 00 [ 5391/20026 ( 27%)], Train Loss: 0.62060\n","Epoch: 00 [ 5401/20026 ( 27%)], Train Loss: 0.62017\n","Epoch: 00 [ 5411/20026 ( 27%)], Train Loss: 0.62099\n","Epoch: 00 [ 5421/20026 ( 27%)], Train Loss: 0.62071\n","Epoch: 00 [ 5431/20026 ( 27%)], Train Loss: 0.61997\n","Epoch: 00 [ 5441/20026 ( 27%)], Train Loss: 0.61931\n","Epoch: 00 [ 5451/20026 ( 27%)], Train Loss: 0.62006\n","Epoch: 00 [ 5461/20026 ( 27%)], Train Loss: 0.62007\n","Epoch: 00 [ 5471/20026 ( 27%)], Train Loss: 0.61932\n","Epoch: 00 [ 5481/20026 ( 27%)], Train Loss: 0.61886\n","Epoch: 00 [ 5491/20026 ( 27%)], Train Loss: 0.61805\n","Epoch: 00 [ 5501/20026 ( 27%)], Train Loss: 0.61739\n","Epoch: 00 [ 5511/20026 ( 28%)], Train Loss: 0.61687\n","Epoch: 00 [ 5521/20026 ( 28%)], Train Loss: 0.61727\n","Epoch: 00 [ 5531/20026 ( 28%)], Train Loss: 0.61727\n","Epoch: 00 [ 5541/20026 ( 28%)], Train Loss: 0.61718\n","Epoch: 00 [ 5551/20026 ( 28%)], Train Loss: 0.61717\n","Epoch: 00 [ 5561/20026 ( 28%)], Train Loss: 0.61650\n","Epoch: 00 [ 5571/20026 ( 28%)], Train Loss: 0.61747\n","Epoch: 00 [ 5581/20026 ( 28%)], Train Loss: 0.61817\n","Epoch: 00 [ 5591/20026 ( 28%)], Train Loss: 0.61828\n","Epoch: 00 [ 5601/20026 ( 28%)], Train Loss: 0.61741\n","Epoch: 00 [ 5611/20026 ( 28%)], Train Loss: 0.61723\n","Epoch: 00 [ 5621/20026 ( 28%)], Train Loss: 0.61753\n","Epoch: 00 [ 5631/20026 ( 28%)], Train Loss: 0.61846\n","Epoch: 00 [ 5641/20026 ( 28%)], Train Loss: 0.61878\n","Epoch: 00 [ 5651/20026 ( 28%)], Train Loss: 0.61863\n","Epoch: 00 [ 5661/20026 ( 28%)], Train Loss: 0.61803\n","Epoch: 00 [ 5671/20026 ( 28%)], Train Loss: 0.61812\n","Epoch: 00 [ 5681/20026 ( 28%)], Train Loss: 0.61770\n","Epoch: 00 [ 5691/20026 ( 28%)], Train Loss: 0.61764\n","Epoch: 00 [ 5701/20026 ( 28%)], Train Loss: 0.61728\n","Epoch: 00 [ 5711/20026 ( 29%)], Train Loss: 0.61679\n","Epoch: 00 [ 5721/20026 ( 29%)], Train Loss: 0.61748\n","Epoch: 00 [ 5731/20026 ( 29%)], Train Loss: 0.61679\n","Epoch: 00 [ 5741/20026 ( 29%)], Train Loss: 0.61612\n","Epoch: 00 [ 5751/20026 ( 29%)], Train Loss: 0.61559\n","Epoch: 00 [ 5761/20026 ( 29%)], Train Loss: 0.61543\n","Epoch: 00 [ 5771/20026 ( 29%)], Train Loss: 0.61501\n","Epoch: 00 [ 5781/20026 ( 29%)], Train Loss: 0.61464\n","Epoch: 00 [ 5791/20026 ( 29%)], Train Loss: 0.61479\n","Epoch: 00 [ 5801/20026 ( 29%)], Train Loss: 0.61502\n","Epoch: 00 [ 5811/20026 ( 29%)], Train Loss: 0.61482\n","Epoch: 00 [ 5821/20026 ( 29%)], Train Loss: 0.61467\n","Epoch: 00 [ 5831/20026 ( 29%)], Train Loss: 0.61454\n","Epoch: 00 [ 5841/20026 ( 29%)], Train Loss: 0.61437\n","Epoch: 00 [ 5851/20026 ( 29%)], Train Loss: 0.61372\n","Epoch: 00 [ 5861/20026 ( 29%)], Train Loss: 0.61384\n","Epoch: 00 [ 5871/20026 ( 29%)], Train Loss: 0.61356\n","Epoch: 00 [ 5881/20026 ( 29%)], Train Loss: 0.61286\n","Epoch: 00 [ 5891/20026 ( 29%)], Train Loss: 0.61247\n","Epoch: 00 [ 5901/20026 ( 29%)], Train Loss: 0.61166\n","Epoch: 00 [ 5911/20026 ( 30%)], Train Loss: 0.61096\n","Epoch: 00 [ 5921/20026 ( 30%)], Train Loss: 0.61017\n","Epoch: 00 [ 5931/20026 ( 30%)], Train Loss: 0.60991\n","Epoch: 00 [ 5941/20026 ( 30%)], Train Loss: 0.60917\n","Epoch: 00 [ 5951/20026 ( 30%)], Train Loss: 0.60897\n","Epoch: 00 [ 5961/20026 ( 30%)], Train Loss: 0.60911\n","Epoch: 00 [ 5971/20026 ( 30%)], Train Loss: 0.60854\n","Epoch: 00 [ 5981/20026 ( 30%)], Train Loss: 0.60850\n","Epoch: 00 [ 5991/20026 ( 30%)], Train Loss: 0.60757\n","Epoch: 00 [ 6001/20026 ( 30%)], Train Loss: 0.60788\n","Epoch: 00 [ 6011/20026 ( 30%)], Train Loss: 0.60768\n","Epoch: 00 [ 6021/20026 ( 30%)], Train Loss: 0.60724\n","Epoch: 00 [ 6031/20026 ( 30%)], Train Loss: 0.60717\n","Epoch: 00 [ 6041/20026 ( 30%)], Train Loss: 0.60681\n","Epoch: 00 [ 6051/20026 ( 30%)], Train Loss: 0.60595\n","Epoch: 00 [ 6061/20026 ( 30%)], Train Loss: 0.60556\n","Epoch: 00 [ 6071/20026 ( 30%)], Train Loss: 0.60557\n","Epoch: 00 [ 6081/20026 ( 30%)], Train Loss: 0.60465\n","Epoch: 00 [ 6091/20026 ( 30%)], Train Loss: 0.60482\n","Epoch: 00 [ 6101/20026 ( 30%)], Train Loss: 0.60523\n","Epoch: 00 [ 6111/20026 ( 31%)], Train Loss: 0.60457\n","Epoch: 00 [ 6121/20026 ( 31%)], Train Loss: 0.60451\n","Epoch: 00 [ 6131/20026 ( 31%)], Train Loss: 0.60463\n","Epoch: 00 [ 6141/20026 ( 31%)], Train Loss: 0.60427\n","Epoch: 00 [ 6151/20026 ( 31%)], Train Loss: 0.60402\n","Epoch: 00 [ 6161/20026 ( 31%)], Train Loss: 0.60454\n","Epoch: 00 [ 6171/20026 ( 31%)], Train Loss: 0.60470\n","Epoch: 00 [ 6181/20026 ( 31%)], Train Loss: 0.60427\n","Epoch: 00 [ 6191/20026 ( 31%)], Train Loss: 0.60395\n","Epoch: 00 [ 6201/20026 ( 31%)], Train Loss: 0.60324\n","Epoch: 00 [ 6211/20026 ( 31%)], Train Loss: 0.60254\n","Epoch: 00 [ 6221/20026 ( 31%)], Train Loss: 0.60233\n","Epoch: 00 [ 6231/20026 ( 31%)], Train Loss: 0.60196\n","Epoch: 00 [ 6241/20026 ( 31%)], Train Loss: 0.60154\n","Epoch: 00 [ 6251/20026 ( 31%)], Train Loss: 0.60120\n","Epoch: 00 [ 6261/20026 ( 31%)], Train Loss: 0.60063\n","Epoch: 00 [ 6271/20026 ( 31%)], Train Loss: 0.60015\n","Epoch: 00 [ 6281/20026 ( 31%)], Train Loss: 0.59956\n","Epoch: 00 [ 6291/20026 ( 31%)], Train Loss: 0.59968\n","Epoch: 00 [ 6301/20026 ( 31%)], Train Loss: 0.59970\n","Epoch: 00 [ 6311/20026 ( 32%)], Train Loss: 0.59932\n","Epoch: 00 [ 6321/20026 ( 32%)], Train Loss: 0.59882\n","Epoch: 00 [ 6331/20026 ( 32%)], Train Loss: 0.59865\n","Epoch: 00 [ 6341/20026 ( 32%)], Train Loss: 0.59861\n","Epoch: 00 [ 6351/20026 ( 32%)], Train Loss: 0.59851\n","Epoch: 00 [ 6361/20026 ( 32%)], Train Loss: 0.59778\n","Epoch: 00 [ 6371/20026 ( 32%)], Train Loss: 0.59762\n","Epoch: 00 [ 6381/20026 ( 32%)], Train Loss: 0.59699\n","Epoch: 00 [ 6391/20026 ( 32%)], Train Loss: 0.59662\n","Epoch: 00 [ 6401/20026 ( 32%)], Train Loss: 0.59695\n","Epoch: 00 [ 6411/20026 ( 32%)], Train Loss: 0.59754\n","Epoch: 00 [ 6421/20026 ( 32%)], Train Loss: 0.59704\n","Epoch: 00 [ 6431/20026 ( 32%)], Train Loss: 0.59719\n","Epoch: 00 [ 6441/20026 ( 32%)], Train Loss: 0.59678\n","Epoch: 00 [ 6451/20026 ( 32%)], Train Loss: 0.59702\n","Epoch: 00 [ 6461/20026 ( 32%)], Train Loss: 0.59691\n","Epoch: 00 [ 6471/20026 ( 32%)], Train Loss: 0.59647\n","Epoch: 00 [ 6481/20026 ( 32%)], Train Loss: 0.59656\n","Epoch: 00 [ 6491/20026 ( 32%)], Train Loss: 0.59665\n","Epoch: 00 [ 6501/20026 ( 32%)], Train Loss: 0.59606\n","Epoch: 00 [ 6511/20026 ( 33%)], Train Loss: 0.59672\n","Epoch: 00 [ 6521/20026 ( 33%)], Train Loss: 0.59706\n","Epoch: 00 [ 6531/20026 ( 33%)], Train Loss: 0.59668\n","Epoch: 00 [ 6541/20026 ( 33%)], Train Loss: 0.59624\n","Epoch: 00 [ 6551/20026 ( 33%)], Train Loss: 0.59600\n","Epoch: 00 [ 6561/20026 ( 33%)], Train Loss: 0.59547\n","Epoch: 00 [ 6571/20026 ( 33%)], Train Loss: 0.59582\n","Epoch: 00 [ 6581/20026 ( 33%)], Train Loss: 0.59551\n","Epoch: 00 [ 6591/20026 ( 33%)], Train Loss: 0.59586\n","Epoch: 00 [ 6601/20026 ( 33%)], Train Loss: 0.59525\n","Epoch: 00 [ 6611/20026 ( 33%)], Train Loss: 0.59475\n","Epoch: 00 [ 6621/20026 ( 33%)], Train Loss: 0.59398\n","Epoch: 00 [ 6631/20026 ( 33%)], Train Loss: 0.59374\n","Epoch: 00 [ 6641/20026 ( 33%)], Train Loss: 0.59341\n","Epoch: 00 [ 6651/20026 ( 33%)], Train Loss: 0.59304\n","Epoch: 00 [ 6661/20026 ( 33%)], Train Loss: 0.59273\n","Epoch: 00 [ 6671/20026 ( 33%)], Train Loss: 0.59241\n","Epoch: 00 [ 6681/20026 ( 33%)], Train Loss: 0.59196\n","Epoch: 00 [ 6691/20026 ( 33%)], Train Loss: 0.59188\n","Epoch: 00 [ 6701/20026 ( 33%)], Train Loss: 0.59125\n","Epoch: 00 [ 6711/20026 ( 34%)], Train Loss: 0.59123\n","Epoch: 00 [ 6721/20026 ( 34%)], Train Loss: 0.59086\n","Epoch: 00 [ 6731/20026 ( 34%)], Train Loss: 0.59051\n","Epoch: 00 [ 6741/20026 ( 34%)], Train Loss: 0.59035\n","Epoch: 00 [ 6751/20026 ( 34%)], Train Loss: 0.58976\n","Epoch: 00 [ 6761/20026 ( 34%)], Train Loss: 0.58922\n","Epoch: 00 [ 6771/20026 ( 34%)], Train Loss: 0.58866\n","Epoch: 00 [ 6781/20026 ( 34%)], Train Loss: 0.58877\n","Epoch: 00 [ 6791/20026 ( 34%)], Train Loss: 0.58841\n","Epoch: 00 [ 6801/20026 ( 34%)], Train Loss: 0.58780\n","Epoch: 00 [ 6811/20026 ( 34%)], Train Loss: 0.58851\n","Epoch: 00 [ 6821/20026 ( 34%)], Train Loss: 0.58858\n","Epoch: 00 [ 6831/20026 ( 34%)], Train Loss: 0.58903\n","Epoch: 00 [ 6841/20026 ( 34%)], Train Loss: 0.58893\n","Epoch: 00 [ 6851/20026 ( 34%)], Train Loss: 0.58896\n","Epoch: 00 [ 6861/20026 ( 34%)], Train Loss: 0.58903\n","Epoch: 00 [ 6871/20026 ( 34%)], Train Loss: 0.58939\n","Epoch: 00 [ 6881/20026 ( 34%)], Train Loss: 0.58982\n","Epoch: 00 [ 6891/20026 ( 34%)], Train Loss: 0.58981\n","Epoch: 00 [ 6901/20026 ( 34%)], Train Loss: 0.58915\n","Epoch: 00 [ 6911/20026 ( 35%)], Train Loss: 0.58895\n","Epoch: 00 [ 6921/20026 ( 35%)], Train Loss: 0.58879\n","Epoch: 00 [ 6931/20026 ( 35%)], Train Loss: 0.58869\n","Epoch: 00 [ 6941/20026 ( 35%)], Train Loss: 0.58847\n","Epoch: 00 [ 6951/20026 ( 35%)], Train Loss: 0.58850\n","Epoch: 00 [ 6961/20026 ( 35%)], Train Loss: 0.58843\n","Epoch: 00 [ 6971/20026 ( 35%)], Train Loss: 0.58873\n","Epoch: 00 [ 6981/20026 ( 35%)], Train Loss: 0.58811\n","Epoch: 00 [ 6991/20026 ( 35%)], Train Loss: 0.58857\n","Epoch: 00 [ 7001/20026 ( 35%)], Train Loss: 0.58803\n","Epoch: 00 [ 7011/20026 ( 35%)], Train Loss: 0.58775\n","Epoch: 00 [ 7021/20026 ( 35%)], Train Loss: 0.58793\n","Epoch: 00 [ 7031/20026 ( 35%)], Train Loss: 0.58829\n","Epoch: 00 [ 7041/20026 ( 35%)], Train Loss: 0.58784\n","Epoch: 00 [ 7051/20026 ( 35%)], Train Loss: 0.58768\n","Epoch: 00 [ 7061/20026 ( 35%)], Train Loss: 0.58747\n","Epoch: 00 [ 7071/20026 ( 35%)], Train Loss: 0.58815\n","Epoch: 00 [ 7081/20026 ( 35%)], Train Loss: 0.58744\n","Epoch: 00 [ 7091/20026 ( 35%)], Train Loss: 0.58770\n","Epoch: 00 [ 7101/20026 ( 35%)], Train Loss: 0.58808\n","Epoch: 00 [ 7111/20026 ( 36%)], Train Loss: 0.58880\n","Epoch: 00 [ 7121/20026 ( 36%)], Train Loss: 0.58913\n","Epoch: 00 [ 7131/20026 ( 36%)], Train Loss: 0.58972\n","Epoch: 00 [ 7141/20026 ( 36%)], Train Loss: 0.58977\n","Epoch: 00 [ 7151/20026 ( 36%)], Train Loss: 0.59112\n","Epoch: 00 [ 7161/20026 ( 36%)], Train Loss: 0.59171\n","Epoch: 00 [ 7171/20026 ( 36%)], Train Loss: 0.59154\n","Epoch: 00 [ 7181/20026 ( 36%)], Train Loss: 0.59104\n","Epoch: 00 [ 7191/20026 ( 36%)], Train Loss: 0.59109\n","Epoch: 00 [ 7201/20026 ( 36%)], Train Loss: 0.59061\n","Epoch: 00 [ 7211/20026 ( 36%)], Train Loss: 0.59043\n","Epoch: 00 [ 7221/20026 ( 36%)], Train Loss: 0.59068\n","Epoch: 00 [ 7231/20026 ( 36%)], Train Loss: 0.58992\n","Epoch: 00 [ 7241/20026 ( 36%)], Train Loss: 0.59027\n","Epoch: 00 [ 7251/20026 ( 36%)], Train Loss: 0.58973\n","Epoch: 00 [ 7261/20026 ( 36%)], Train Loss: 0.58931\n","Epoch: 00 [ 7271/20026 ( 36%)], Train Loss: 0.58898\n","Epoch: 00 [ 7281/20026 ( 36%)], Train Loss: 0.58839\n","Epoch: 00 [ 7291/20026 ( 36%)], Train Loss: 0.58779\n","Epoch: 00 [ 7301/20026 ( 36%)], Train Loss: 0.58776\n","Epoch: 00 [ 7311/20026 ( 37%)], Train Loss: 0.58789\n","Epoch: 00 [ 7321/20026 ( 37%)], Train Loss: 0.58730\n","Epoch: 00 [ 7331/20026 ( 37%)], Train Loss: 0.58709\n","Epoch: 00 [ 7341/20026 ( 37%)], Train Loss: 0.58684\n","Epoch: 00 [ 7351/20026 ( 37%)], Train Loss: 0.58691\n","Epoch: 00 [ 7361/20026 ( 37%)], Train Loss: 0.58666\n","Epoch: 00 [ 7371/20026 ( 37%)], Train Loss: 0.58635\n","Epoch: 00 [ 7381/20026 ( 37%)], Train Loss: 0.58619\n","Epoch: 00 [ 7391/20026 ( 37%)], Train Loss: 0.58670\n","Epoch: 00 [ 7401/20026 ( 37%)], Train Loss: 0.58668\n","Epoch: 00 [ 7411/20026 ( 37%)], Train Loss: 0.58705\n","Epoch: 00 [ 7421/20026 ( 37%)], Train Loss: 0.58710\n","Epoch: 00 [ 7431/20026 ( 37%)], Train Loss: 0.58677\n","Epoch: 00 [ 7441/20026 ( 37%)], Train Loss: 0.58654\n","Epoch: 00 [ 7451/20026 ( 37%)], Train Loss: 0.58660\n","Epoch: 00 [ 7461/20026 ( 37%)], Train Loss: 0.58597\n","Epoch: 00 [ 7471/20026 ( 37%)], Train Loss: 0.58565\n","Epoch: 00 [ 7481/20026 ( 37%)], Train Loss: 0.58616\n","Epoch: 00 [ 7491/20026 ( 37%)], Train Loss: 0.58652\n","Epoch: 00 [ 7501/20026 ( 37%)], Train Loss: 0.58711\n","Epoch: 00 [ 7511/20026 ( 38%)], Train Loss: 0.58708\n","Epoch: 00 [ 7521/20026 ( 38%)], Train Loss: 0.58703\n","Epoch: 00 [ 7531/20026 ( 38%)], Train Loss: 0.58692\n","Epoch: 00 [ 7541/20026 ( 38%)], Train Loss: 0.58627\n","Epoch: 00 [ 7551/20026 ( 38%)], Train Loss: 0.58638\n","Epoch: 00 [ 7561/20026 ( 38%)], Train Loss: 0.58647\n","Epoch: 00 [ 7571/20026 ( 38%)], Train Loss: 0.58598\n","Epoch: 00 [ 7581/20026 ( 38%)], Train Loss: 0.58553\n","Epoch: 00 [ 7591/20026 ( 38%)], Train Loss: 0.58554\n","Epoch: 00 [ 7601/20026 ( 38%)], Train Loss: 0.58542\n","Epoch: 00 [ 7611/20026 ( 38%)], Train Loss: 0.58504\n","Epoch: 00 [ 7621/20026 ( 38%)], Train Loss: 0.58457\n","Epoch: 00 [ 7631/20026 ( 38%)], Train Loss: 0.58501\n","Epoch: 00 [ 7641/20026 ( 38%)], Train Loss: 0.58534\n","Epoch: 00 [ 7651/20026 ( 38%)], Train Loss: 0.58520\n","Epoch: 00 [ 7661/20026 ( 38%)], Train Loss: 0.58530\n","Epoch: 00 [ 7671/20026 ( 38%)], Train Loss: 0.58534\n","Epoch: 00 [ 7681/20026 ( 38%)], Train Loss: 0.58497\n","Epoch: 00 [ 7691/20026 ( 38%)], Train Loss: 0.58506\n","Epoch: 00 [ 7701/20026 ( 38%)], Train Loss: 0.58458\n","Epoch: 00 [ 7711/20026 ( 39%)], Train Loss: 0.58392\n","Epoch: 00 [ 7721/20026 ( 39%)], Train Loss: 0.58339\n","Epoch: 00 [ 7731/20026 ( 39%)], Train Loss: 0.58411\n","Epoch: 00 [ 7741/20026 ( 39%)], Train Loss: 0.58431\n","Epoch: 00 [ 7751/20026 ( 39%)], Train Loss: 0.58407\n","Epoch: 00 [ 7761/20026 ( 39%)], Train Loss: 0.58407\n","Epoch: 00 [ 7771/20026 ( 39%)], Train Loss: 0.58405\n","Epoch: 00 [ 7781/20026 ( 39%)], Train Loss: 0.58346\n","Epoch: 00 [ 7791/20026 ( 39%)], Train Loss: 0.58293\n","Epoch: 00 [ 7801/20026 ( 39%)], Train Loss: 0.58272\n","Epoch: 00 [ 7811/20026 ( 39%)], Train Loss: 0.58337\n","Epoch: 00 [ 7821/20026 ( 39%)], Train Loss: 0.58318\n","Epoch: 00 [ 7831/20026 ( 39%)], Train Loss: 0.58316\n","Epoch: 00 [ 7841/20026 ( 39%)], Train Loss: 0.58273\n","Epoch: 00 [ 7851/20026 ( 39%)], Train Loss: 0.58281\n","Epoch: 00 [ 7861/20026 ( 39%)], Train Loss: 0.58229\n","Epoch: 00 [ 7871/20026 ( 39%)], Train Loss: 0.58237\n","Epoch: 00 [ 7881/20026 ( 39%)], Train Loss: 0.58178\n","Epoch: 00 [ 7891/20026 ( 39%)], Train Loss: 0.58149\n","Epoch: 00 [ 7901/20026 ( 39%)], Train Loss: 0.58109\n","Epoch: 00 [ 7911/20026 ( 40%)], Train Loss: 0.58115\n","Epoch: 00 [ 7921/20026 ( 40%)], Train Loss: 0.58099\n","Epoch: 00 [ 7931/20026 ( 40%)], Train Loss: 0.58079\n","Epoch: 00 [ 7941/20026 ( 40%)], Train Loss: 0.58128\n","Epoch: 00 [ 7951/20026 ( 40%)], Train Loss: 0.58118\n","Epoch: 00 [ 7961/20026 ( 40%)], Train Loss: 0.58090\n","Epoch: 00 [ 7971/20026 ( 40%)], Train Loss: 0.58141\n","Epoch: 00 [ 7981/20026 ( 40%)], Train Loss: 0.58122\n","Epoch: 00 [ 7991/20026 ( 40%)], Train Loss: 0.58082\n","Epoch: 00 [ 8001/20026 ( 40%)], Train Loss: 0.58118\n","Epoch: 00 [ 8011/20026 ( 40%)], Train Loss: 0.58125\n","Epoch: 00 [ 8021/20026 ( 40%)], Train Loss: 0.58091\n","Epoch: 00 [ 8031/20026 ( 40%)], Train Loss: 0.58108\n","Epoch: 00 [ 8041/20026 ( 40%)], Train Loss: 0.58060\n","Epoch: 00 [ 8051/20026 ( 40%)], Train Loss: 0.58070\n","Epoch: 00 [ 8061/20026 ( 40%)], Train Loss: 0.58083\n","Epoch: 00 [ 8071/20026 ( 40%)], Train Loss: 0.58022\n","Epoch: 00 [ 8081/20026 ( 40%)], Train Loss: 0.57990\n","Epoch: 00 [ 8091/20026 ( 40%)], Train Loss: 0.57960\n","Epoch: 00 [ 8101/20026 ( 40%)], Train Loss: 0.57906\n","Epoch: 00 [ 8111/20026 ( 41%)], Train Loss: 0.57872\n","Epoch: 00 [ 8121/20026 ( 41%)], Train Loss: 0.57809\n","Epoch: 00 [ 8131/20026 ( 41%)], Train Loss: 0.57813\n","Epoch: 00 [ 8141/20026 ( 41%)], Train Loss: 0.57773\n","Epoch: 00 [ 8151/20026 ( 41%)], Train Loss: 0.57787\n","Epoch: 00 [ 8161/20026 ( 41%)], Train Loss: 0.57747\n","Epoch: 00 [ 8171/20026 ( 41%)], Train Loss: 0.57712\n","Epoch: 00 [ 8181/20026 ( 41%)], Train Loss: 0.57685\n","Epoch: 00 [ 8191/20026 ( 41%)], Train Loss: 0.57689\n","Epoch: 00 [ 8201/20026 ( 41%)], Train Loss: 0.57697\n","Epoch: 00 [ 8211/20026 ( 41%)], Train Loss: 0.57644\n","Epoch: 00 [ 8221/20026 ( 41%)], Train Loss: 0.57623\n","Epoch: 00 [ 8231/20026 ( 41%)], Train Loss: 0.57639\n","Epoch: 00 [ 8241/20026 ( 41%)], Train Loss: 0.57602\n","Epoch: 00 [ 8251/20026 ( 41%)], Train Loss: 0.57637\n","Epoch: 00 [ 8261/20026 ( 41%)], Train Loss: 0.57592\n","Epoch: 00 [ 8271/20026 ( 41%)], Train Loss: 0.57525\n","Epoch: 00 [ 8281/20026 ( 41%)], Train Loss: 0.57491\n","Epoch: 00 [ 8291/20026 ( 41%)], Train Loss: 0.57468\n","Epoch: 00 [ 8301/20026 ( 41%)], Train Loss: 0.57422\n","Epoch: 00 [ 8311/20026 ( 42%)], Train Loss: 0.57382\n","Epoch: 00 [ 8321/20026 ( 42%)], Train Loss: 0.57370\n","Epoch: 00 [ 8331/20026 ( 42%)], Train Loss: 0.57326\n","Epoch: 00 [ 8341/20026 ( 42%)], Train Loss: 0.57291\n","Epoch: 00 [ 8351/20026 ( 42%)], Train Loss: 0.57345\n","Epoch: 00 [ 8361/20026 ( 42%)], Train Loss: 0.57311\n","Epoch: 00 [ 8371/20026 ( 42%)], Train Loss: 0.57288\n","Epoch: 00 [ 8381/20026 ( 42%)], Train Loss: 0.57264\n","Epoch: 00 [ 8391/20026 ( 42%)], Train Loss: 0.57224\n","Epoch: 00 [ 8401/20026 ( 42%)], Train Loss: 0.57179\n","Epoch: 00 [ 8411/20026 ( 42%)], Train Loss: 0.57142\n","Epoch: 00 [ 8421/20026 ( 42%)], Train Loss: 0.57120\n","Epoch: 00 [ 8431/20026 ( 42%)], Train Loss: 0.57104\n","Epoch: 00 [ 8441/20026 ( 42%)], Train Loss: 0.57084\n","Epoch: 00 [ 8451/20026 ( 42%)], Train Loss: 0.57020\n","Epoch: 00 [ 8461/20026 ( 42%)], Train Loss: 0.57100\n","Epoch: 00 [ 8471/20026 ( 42%)], Train Loss: 0.57131\n","Epoch: 00 [ 8481/20026 ( 42%)], Train Loss: 0.57093\n","Epoch: 00 [ 8491/20026 ( 42%)], Train Loss: 0.57113\n","Epoch: 00 [ 8501/20026 ( 42%)], Train Loss: 0.57120\n","Epoch: 00 [ 8511/20026 ( 42%)], Train Loss: 0.57070\n","Epoch: 00 [ 8521/20026 ( 43%)], Train Loss: 0.57052\n","Epoch: 00 [ 8531/20026 ( 43%)], Train Loss: 0.57026\n","Epoch: 00 [ 8541/20026 ( 43%)], Train Loss: 0.57018\n","Epoch: 00 [ 8551/20026 ( 43%)], Train Loss: 0.56997\n","Epoch: 00 [ 8561/20026 ( 43%)], Train Loss: 0.56990\n","Epoch: 00 [ 8571/20026 ( 43%)], Train Loss: 0.56989\n","Epoch: 00 [ 8581/20026 ( 43%)], Train Loss: 0.56988\n","Epoch: 00 [ 8591/20026 ( 43%)], Train Loss: 0.56939\n","Epoch: 00 [ 8601/20026 ( 43%)], Train Loss: 0.56891\n","Epoch: 00 [ 8611/20026 ( 43%)], Train Loss: 0.56907\n","Epoch: 00 [ 8621/20026 ( 43%)], Train Loss: 0.56882\n","Epoch: 00 [ 8631/20026 ( 43%)], Train Loss: 0.56879\n","Epoch: 00 [ 8641/20026 ( 43%)], Train Loss: 0.56822\n","Epoch: 00 [ 8651/20026 ( 43%)], Train Loss: 0.56783\n","Epoch: 00 [ 8661/20026 ( 43%)], Train Loss: 0.56778\n","Epoch: 00 [ 8671/20026 ( 43%)], Train Loss: 0.56720\n","Epoch: 00 [ 8681/20026 ( 43%)], Train Loss: 0.56679\n","Epoch: 00 [ 8691/20026 ( 43%)], Train Loss: 0.56682\n","Epoch: 00 [ 8701/20026 ( 43%)], Train Loss: 0.56653\n","Epoch: 00 [ 8711/20026 ( 43%)], Train Loss: 0.56617\n","Epoch: 00 [ 8721/20026 ( 44%)], Train Loss: 0.56590\n","Epoch: 00 [ 8731/20026 ( 44%)], Train Loss: 0.56583\n","Epoch: 00 [ 8741/20026 ( 44%)], Train Loss: 0.56627\n","Epoch: 00 [ 8751/20026 ( 44%)], Train Loss: 0.56614\n","Epoch: 00 [ 8761/20026 ( 44%)], Train Loss: 0.56586\n","Epoch: 00 [ 8771/20026 ( 44%)], Train Loss: 0.56536\n","Epoch: 00 [ 8781/20026 ( 44%)], Train Loss: 0.56500\n","Epoch: 00 [ 8791/20026 ( 44%)], Train Loss: 0.56499\n","Epoch: 00 [ 8801/20026 ( 44%)], Train Loss: 0.56441\n","Epoch: 00 [ 8811/20026 ( 44%)], Train Loss: 0.56467\n","Epoch: 00 [ 8821/20026 ( 44%)], Train Loss: 0.56462\n","Epoch: 00 [ 8831/20026 ( 44%)], Train Loss: 0.56538\n","Epoch: 00 [ 8841/20026 ( 44%)], Train Loss: 0.56528\n","Epoch: 00 [ 8851/20026 ( 44%)], Train Loss: 0.56538\n","Epoch: 00 [ 8861/20026 ( 44%)], Train Loss: 0.56532\n","Epoch: 00 [ 8871/20026 ( 44%)], Train Loss: 0.56515\n","Epoch: 00 [ 8881/20026 ( 44%)], Train Loss: 0.56557\n","Epoch: 00 [ 8891/20026 ( 44%)], Train Loss: 0.56521\n","Epoch: 00 [ 8901/20026 ( 44%)], Train Loss: 0.56553\n","Epoch: 00 [ 8911/20026 ( 44%)], Train Loss: 0.56539\n","Epoch: 00 [ 8921/20026 ( 45%)], Train Loss: 0.56541\n","Epoch: 00 [ 8931/20026 ( 45%)], Train Loss: 0.56544\n","Epoch: 00 [ 8941/20026 ( 45%)], Train Loss: 0.56530\n","Epoch: 00 [ 8951/20026 ( 45%)], Train Loss: 0.56509\n","Epoch: 00 [ 8961/20026 ( 45%)], Train Loss: 0.56448\n","Epoch: 00 [ 8971/20026 ( 45%)], Train Loss: 0.56441\n","Epoch: 00 [ 8981/20026 ( 45%)], Train Loss: 0.56471\n","Epoch: 00 [ 8991/20026 ( 45%)], Train Loss: 0.56439\n","Epoch: 00 [ 9001/20026 ( 45%)], Train Loss: 0.56392\n","Epoch: 00 [ 9011/20026 ( 45%)], Train Loss: 0.56436\n","Epoch: 00 [ 9021/20026 ( 45%)], Train Loss: 0.56407\n","Epoch: 00 [ 9031/20026 ( 45%)], Train Loss: 0.56380\n","Epoch: 00 [ 9041/20026 ( 45%)], Train Loss: 0.56393\n","Epoch: 00 [ 9051/20026 ( 45%)], Train Loss: 0.56406\n","Epoch: 00 [ 9061/20026 ( 45%)], Train Loss: 0.56424\n","Epoch: 00 [ 9071/20026 ( 45%)], Train Loss: 0.56420\n","Epoch: 00 [ 9081/20026 ( 45%)], Train Loss: 0.56395\n","Epoch: 00 [ 9091/20026 ( 45%)], Train Loss: 0.56443\n","Epoch: 00 [ 9101/20026 ( 45%)], Train Loss: 0.56402\n","Epoch: 00 [ 9111/20026 ( 45%)], Train Loss: 0.56431\n","Epoch: 00 [ 9121/20026 ( 46%)], Train Loss: 0.56439\n","Epoch: 00 [ 9131/20026 ( 46%)], Train Loss: 0.56405\n","Epoch: 00 [ 9141/20026 ( 46%)], Train Loss: 0.56443\n","Epoch: 00 [ 9151/20026 ( 46%)], Train Loss: 0.56438\n","Epoch: 00 [ 9161/20026 ( 46%)], Train Loss: 0.56421\n","Epoch: 00 [ 9171/20026 ( 46%)], Train Loss: 0.56401\n","Epoch: 00 [ 9181/20026 ( 46%)], Train Loss: 0.56372\n","Epoch: 00 [ 9191/20026 ( 46%)], Train Loss: 0.56438\n","Epoch: 00 [ 9201/20026 ( 46%)], Train Loss: 0.56412\n","Epoch: 00 [ 9211/20026 ( 46%)], Train Loss: 0.56357\n","Epoch: 00 [ 9221/20026 ( 46%)], Train Loss: 0.56363\n","Epoch: 00 [ 9231/20026 ( 46%)], Train Loss: 0.56320\n","Epoch: 00 [ 9241/20026 ( 46%)], Train Loss: 0.56308\n","Epoch: 00 [ 9251/20026 ( 46%)], Train Loss: 0.56266\n","Epoch: 00 [ 9261/20026 ( 46%)], Train Loss: 0.56244\n","Epoch: 00 [ 9271/20026 ( 46%)], Train Loss: 0.56245\n","Epoch: 00 [ 9281/20026 ( 46%)], Train Loss: 0.56202\n","Epoch: 00 [ 9291/20026 ( 46%)], Train Loss: 0.56176\n","Epoch: 00 [ 9301/20026 ( 46%)], Train Loss: 0.56151\n","Epoch: 00 [ 9311/20026 ( 46%)], Train Loss: 0.56118\n","Epoch: 00 [ 9321/20026 ( 47%)], Train Loss: 0.56169\n","Epoch: 00 [ 9331/20026 ( 47%)], Train Loss: 0.56112\n","Epoch: 00 [ 9341/20026 ( 47%)], Train Loss: 0.56124\n","Epoch: 00 [ 9351/20026 ( 47%)], Train Loss: 0.56098\n","Epoch: 00 [ 9361/20026 ( 47%)], Train Loss: 0.56078\n","Epoch: 00 [ 9371/20026 ( 47%)], Train Loss: 0.56053\n","Epoch: 00 [ 9381/20026 ( 47%)], Train Loss: 0.56080\n","Epoch: 00 [ 9391/20026 ( 47%)], Train Loss: 0.56042\n","Epoch: 00 [ 9401/20026 ( 47%)], Train Loss: 0.56011\n","Epoch: 00 [ 9411/20026 ( 47%)], Train Loss: 0.56006\n","Epoch: 00 [ 9421/20026 ( 47%)], Train Loss: 0.56026\n","Epoch: 00 [ 9431/20026 ( 47%)], Train Loss: 0.56001\n","Epoch: 00 [ 9441/20026 ( 47%)], Train Loss: 0.55986\n","Epoch: 00 [ 9451/20026 ( 47%)], Train Loss: 0.55965\n","Epoch: 00 [ 9461/20026 ( 47%)], Train Loss: 0.55960\n","Epoch: 00 [ 9471/20026 ( 47%)], Train Loss: 0.55905\n","Epoch: 00 [ 9481/20026 ( 47%)], Train Loss: 0.55905\n","Epoch: 00 [ 9491/20026 ( 47%)], Train Loss: 0.55922\n","Epoch: 00 [ 9501/20026 ( 47%)], Train Loss: 0.55911\n","Epoch: 00 [ 9511/20026 ( 47%)], Train Loss: 0.55873\n","Epoch: 00 [ 9521/20026 ( 48%)], Train Loss: 0.55840\n","Epoch: 00 [ 9531/20026 ( 48%)], Train Loss: 0.55838\n","Epoch: 00 [ 9541/20026 ( 48%)], Train Loss: 0.55866\n","Epoch: 00 [ 9551/20026 ( 48%)], Train Loss: 0.55853\n","Epoch: 00 [ 9561/20026 ( 48%)], Train Loss: 0.55821\n","Epoch: 00 [ 9571/20026 ( 48%)], Train Loss: 0.55820\n","Epoch: 00 [ 9581/20026 ( 48%)], Train Loss: 0.55781\n","Epoch: 00 [ 9591/20026 ( 48%)], Train Loss: 0.55777\n","Epoch: 00 [ 9601/20026 ( 48%)], Train Loss: 0.55741\n","Epoch: 00 [ 9611/20026 ( 48%)], Train Loss: 0.55746\n","Epoch: 00 [ 9621/20026 ( 48%)], Train Loss: 0.55700\n","Epoch: 00 [ 9631/20026 ( 48%)], Train Loss: 0.55656\n","Epoch: 00 [ 9641/20026 ( 48%)], Train Loss: 0.55626\n","Epoch: 00 [ 9651/20026 ( 48%)], Train Loss: 0.55651\n","Epoch: 00 [ 9661/20026 ( 48%)], Train Loss: 0.55649\n","Epoch: 00 [ 9671/20026 ( 48%)], Train Loss: 0.55620\n","Epoch: 00 [ 9681/20026 ( 48%)], Train Loss: 0.55593\n","Epoch: 00 [ 9691/20026 ( 48%)], Train Loss: 0.55643\n","Epoch: 00 [ 9701/20026 ( 48%)], Train Loss: 0.55596\n","Epoch: 00 [ 9711/20026 ( 48%)], Train Loss: 0.55592\n","Epoch: 00 [ 9721/20026 ( 49%)], Train Loss: 0.55585\n","Epoch: 00 [ 9731/20026 ( 49%)], Train Loss: 0.55553\n","Epoch: 00 [ 9741/20026 ( 49%)], Train Loss: 0.55544\n","Epoch: 00 [ 9751/20026 ( 49%)], Train Loss: 0.55546\n","Epoch: 00 [ 9761/20026 ( 49%)], Train Loss: 0.55530\n","Epoch: 00 [ 9771/20026 ( 49%)], Train Loss: 0.55527\n","Epoch: 00 [ 9781/20026 ( 49%)], Train Loss: 0.55520\n","Epoch: 00 [ 9791/20026 ( 49%)], Train Loss: 0.55591\n","Epoch: 00 [ 9801/20026 ( 49%)], Train Loss: 0.55560\n","Epoch: 00 [ 9811/20026 ( 49%)], Train Loss: 0.55528\n","Epoch: 00 [ 9821/20026 ( 49%)], Train Loss: 0.55522\n","Epoch: 00 [ 9831/20026 ( 49%)], Train Loss: 0.55473\n","Epoch: 00 [ 9841/20026 ( 49%)], Train Loss: 0.55468\n","Epoch: 00 [ 9851/20026 ( 49%)], Train Loss: 0.55437\n","Epoch: 00 [ 9861/20026 ( 49%)], Train Loss: 0.55416\n","Epoch: 00 [ 9871/20026 ( 49%)], Train Loss: 0.55404\n","Epoch: 00 [ 9881/20026 ( 49%)], Train Loss: 0.55363\n","Epoch: 00 [ 9891/20026 ( 49%)], Train Loss: 0.55353\n","Epoch: 00 [ 9901/20026 ( 49%)], Train Loss: 0.55341\n","Epoch: 00 [ 9911/20026 ( 49%)], Train Loss: 0.55310\n","Epoch: 00 [ 9921/20026 ( 50%)], Train Loss: 0.55300\n","Epoch: 00 [ 9931/20026 ( 50%)], Train Loss: 0.55288\n","Epoch: 00 [ 9941/20026 ( 50%)], Train Loss: 0.55295\n","Epoch: 00 [ 9951/20026 ( 50%)], Train Loss: 0.55345\n","Epoch: 00 [ 9961/20026 ( 50%)], Train Loss: 0.55328\n","Epoch: 00 [ 9971/20026 ( 50%)], Train Loss: 0.55341\n","Epoch: 00 [ 9981/20026 ( 50%)], Train Loss: 0.55301\n","Epoch: 00 [ 9991/20026 ( 50%)], Train Loss: 0.55289\n","Epoch: 00 [10001/20026 ( 50%)], Train Loss: 0.55272\n","Epoch: 00 [10011/20026 ( 50%)], Train Loss: 0.55244\n","Epoch: 00 [10021/20026 ( 50%)], Train Loss: 0.55246\n","Epoch: 00 [10031/20026 ( 50%)], Train Loss: 0.55220\n","Epoch: 00 [10041/20026 ( 50%)], Train Loss: 0.55204\n","Epoch: 00 [10051/20026 ( 50%)], Train Loss: 0.55161\n","Epoch: 00 [10061/20026 ( 50%)], Train Loss: 0.55169\n","Epoch: 00 [10071/20026 ( 50%)], Train Loss: 0.55139\n","Epoch: 00 [10081/20026 ( 50%)], Train Loss: 0.55113\n","Epoch: 00 [10091/20026 ( 50%)], Train Loss: 0.55119\n","Epoch: 00 [10101/20026 ( 50%)], Train Loss: 0.55138\n","Epoch: 00 [10111/20026 ( 50%)], Train Loss: 0.55098\n","Epoch: 00 [10121/20026 ( 51%)], Train Loss: 0.55090\n","Epoch: 00 [10131/20026 ( 51%)], Train Loss: 0.55130\n","Epoch: 00 [10141/20026 ( 51%)], Train Loss: 0.55140\n","Epoch: 00 [10151/20026 ( 51%)], Train Loss: 0.55133\n","Epoch: 00 [10161/20026 ( 51%)], Train Loss: 0.55117\n","Epoch: 00 [10171/20026 ( 51%)], Train Loss: 0.55088\n","Epoch: 00 [10181/20026 ( 51%)], Train Loss: 0.55150\n","Epoch: 00 [10191/20026 ( 51%)], Train Loss: 0.55166\n","Epoch: 00 [10201/20026 ( 51%)], Train Loss: 0.55139\n","Epoch: 00 [10211/20026 ( 51%)], Train Loss: 0.55132\n","Epoch: 00 [10221/20026 ( 51%)], Train Loss: 0.55134\n","Epoch: 00 [10231/20026 ( 51%)], Train Loss: 0.55119\n","Epoch: 00 [10241/20026 ( 51%)], Train Loss: 0.55090\n","Epoch: 00 [10251/20026 ( 51%)], Train Loss: 0.55041\n","Epoch: 00 [10261/20026 ( 51%)], Train Loss: 0.55042\n","Epoch: 00 [10271/20026 ( 51%)], Train Loss: 0.55026\n","Epoch: 00 [10281/20026 ( 51%)], Train Loss: 0.55019\n","Epoch: 00 [10291/20026 ( 51%)], Train Loss: 0.55038\n","Epoch: 00 [10301/20026 ( 51%)], Train Loss: 0.55033\n","Epoch: 00 [10311/20026 ( 51%)], Train Loss: 0.54993\n","Epoch: 00 [10321/20026 ( 52%)], Train Loss: 0.54972\n","Epoch: 00 [10331/20026 ( 52%)], Train Loss: 0.54944\n","Epoch: 00 [10341/20026 ( 52%)], Train Loss: 0.54921\n","Epoch: 00 [10351/20026 ( 52%)], Train Loss: 0.54879\n","Epoch: 00 [10361/20026 ( 52%)], Train Loss: 0.54883\n","Epoch: 00 [10371/20026 ( 52%)], Train Loss: 0.54972\n","Epoch: 00 [10381/20026 ( 52%)], Train Loss: 0.54949\n","Epoch: 00 [10391/20026 ( 52%)], Train Loss: 0.54956\n","Epoch: 00 [10401/20026 ( 52%)], Train Loss: 0.54928\n","Epoch: 00 [10411/20026 ( 52%)], Train Loss: 0.54925\n","Epoch: 00 [10421/20026 ( 52%)], Train Loss: 0.54904\n","Epoch: 00 [10431/20026 ( 52%)], Train Loss: 0.54892\n","Epoch: 00 [10441/20026 ( 52%)], Train Loss: 0.54894\n","Epoch: 00 [10451/20026 ( 52%)], Train Loss: 0.54852\n","Epoch: 00 [10461/20026 ( 52%)], Train Loss: 0.54850\n","Epoch: 00 [10471/20026 ( 52%)], Train Loss: 0.54832\n","Epoch: 00 [10481/20026 ( 52%)], Train Loss: 0.54833\n","Epoch: 00 [10491/20026 ( 52%)], Train Loss: 0.54795\n","Epoch: 00 [10501/20026 ( 52%)], Train Loss: 0.54772\n","Epoch: 00 [10511/20026 ( 52%)], Train Loss: 0.54750\n","Epoch: 00 [10521/20026 ( 53%)], Train Loss: 0.54751\n","Epoch: 00 [10531/20026 ( 53%)], Train Loss: 0.54718\n","Epoch: 00 [10541/20026 ( 53%)], Train Loss: 0.54702\n","Epoch: 00 [10551/20026 ( 53%)], Train Loss: 0.54677\n","Epoch: 00 [10561/20026 ( 53%)], Train Loss: 0.54661\n","Epoch: 00 [10571/20026 ( 53%)], Train Loss: 0.54668\n","Epoch: 00 [10581/20026 ( 53%)], Train Loss: 0.54667\n","Epoch: 00 [10591/20026 ( 53%)], Train Loss: 0.54668\n","Epoch: 00 [10601/20026 ( 53%)], Train Loss: 0.54660\n","Epoch: 00 [10611/20026 ( 53%)], Train Loss: 0.54664\n","Epoch: 00 [10621/20026 ( 53%)], Train Loss: 0.54640\n","Epoch: 00 [10631/20026 ( 53%)], Train Loss: 0.54625\n","Epoch: 00 [10641/20026 ( 53%)], Train Loss: 0.54602\n","Epoch: 00 [10651/20026 ( 53%)], Train Loss: 0.54598\n","Epoch: 00 [10661/20026 ( 53%)], Train Loss: 0.54565\n","Epoch: 00 [10671/20026 ( 53%)], Train Loss: 0.54547\n","Epoch: 00 [10681/20026 ( 53%)], Train Loss: 0.54535\n","Epoch: 00 [10691/20026 ( 53%)], Train Loss: 0.54551\n","Epoch: 00 [10701/20026 ( 53%)], Train Loss: 0.54526\n","Epoch: 00 [10711/20026 ( 53%)], Train Loss: 0.54507\n","Epoch: 00 [10721/20026 ( 54%)], Train Loss: 0.54500\n","Epoch: 00 [10731/20026 ( 54%)], Train Loss: 0.54482\n","Epoch: 00 [10741/20026 ( 54%)], Train Loss: 0.54441\n","Epoch: 00 [10751/20026 ( 54%)], Train Loss: 0.54474\n","Epoch: 00 [10761/20026 ( 54%)], Train Loss: 0.54473\n","Epoch: 00 [10771/20026 ( 54%)], Train Loss: 0.54454\n","Epoch: 00 [10781/20026 ( 54%)], Train Loss: 0.54430\n","Epoch: 00 [10791/20026 ( 54%)], Train Loss: 0.54437\n","Epoch: 00 [10801/20026 ( 54%)], Train Loss: 0.54419\n","Epoch: 00 [10811/20026 ( 54%)], Train Loss: 0.54384\n","Epoch: 00 [10821/20026 ( 54%)], Train Loss: 0.54366\n","Epoch: 00 [10831/20026 ( 54%)], Train Loss: 0.54367\n","Epoch: 00 [10841/20026 ( 54%)], Train Loss: 0.54365\n","Epoch: 00 [10851/20026 ( 54%)], Train Loss: 0.54394\n","Epoch: 00 [10861/20026 ( 54%)], Train Loss: 0.54352\n","Epoch: 00 [10871/20026 ( 54%)], Train Loss: 0.54361\n","Epoch: 00 [10881/20026 ( 54%)], Train Loss: 0.54348\n","Epoch: 00 [10891/20026 ( 54%)], Train Loss: 0.54382\n","Epoch: 00 [10901/20026 ( 54%)], Train Loss: 0.54390\n","Epoch: 00 [10911/20026 ( 54%)], Train Loss: 0.54359\n","Epoch: 00 [10921/20026 ( 55%)], Train Loss: 0.54355\n","Epoch: 00 [10931/20026 ( 55%)], Train Loss: 0.54318\n","Epoch: 00 [10941/20026 ( 55%)], Train Loss: 0.54303\n","Epoch: 00 [10951/20026 ( 55%)], Train Loss: 0.54267\n","Epoch: 00 [10961/20026 ( 55%)], Train Loss: 0.54253\n","Epoch: 00 [10971/20026 ( 55%)], Train Loss: 0.54223\n","Epoch: 00 [10981/20026 ( 55%)], Train Loss: 0.54277\n","Epoch: 00 [10991/20026 ( 55%)], Train Loss: 0.54236\n","Epoch: 00 [11001/20026 ( 55%)], Train Loss: 0.54218\n","Epoch: 00 [11011/20026 ( 55%)], Train Loss: 0.54180\n","Epoch: 00 [11021/20026 ( 55%)], Train Loss: 0.54164\n","Epoch: 00 [11031/20026 ( 55%)], Train Loss: 0.54200\n","Epoch: 00 [11041/20026 ( 55%)], Train Loss: 0.54186\n","Epoch: 00 [11051/20026 ( 55%)], Train Loss: 0.54160\n","Epoch: 00 [11061/20026 ( 55%)], Train Loss: 0.54180\n","Epoch: 00 [11071/20026 ( 55%)], Train Loss: 0.54175\n","Epoch: 00 [11081/20026 ( 55%)], Train Loss: 0.54155\n","Epoch: 00 [11091/20026 ( 55%)], Train Loss: 0.54130\n","Epoch: 00 [11101/20026 ( 55%)], Train Loss: 0.54103\n","Epoch: 00 [11111/20026 ( 55%)], Train Loss: 0.54067\n","Epoch: 00 [11121/20026 ( 56%)], Train Loss: 0.54030\n","Epoch: 00 [11131/20026 ( 56%)], Train Loss: 0.54003\n","Epoch: 00 [11141/20026 ( 56%)], Train Loss: 0.53998\n","Epoch: 00 [11151/20026 ( 56%)], Train Loss: 0.53988\n","Epoch: 00 [11161/20026 ( 56%)], Train Loss: 0.53981\n","Epoch: 00 [11171/20026 ( 56%)], Train Loss: 0.53966\n","Epoch: 00 [11181/20026 ( 56%)], Train Loss: 0.53978\n","Epoch: 00 [11191/20026 ( 56%)], Train Loss: 0.53963\n","Epoch: 00 [11201/20026 ( 56%)], Train Loss: 0.53974\n","Epoch: 00 [11211/20026 ( 56%)], Train Loss: 0.53985\n","Epoch: 00 [11221/20026 ( 56%)], Train Loss: 0.53970\n","Epoch: 00 [11231/20026 ( 56%)], Train Loss: 0.54028\n","Epoch: 00 [11241/20026 ( 56%)], Train Loss: 0.54018\n","Epoch: 00 [11251/20026 ( 56%)], Train Loss: 0.53986\n","Epoch: 00 [11261/20026 ( 56%)], Train Loss: 0.53964\n","Epoch: 00 [11271/20026 ( 56%)], Train Loss: 0.53943\n","Epoch: 00 [11281/20026 ( 56%)], Train Loss: 0.53957\n","Epoch: 00 [11291/20026 ( 56%)], Train Loss: 0.53937\n","Epoch: 00 [11301/20026 ( 56%)], Train Loss: 0.53965\n","Epoch: 00 [11311/20026 ( 56%)], Train Loss: 0.53939\n","Epoch: 00 [11321/20026 ( 57%)], Train Loss: 0.53938\n","Epoch: 00 [11331/20026 ( 57%)], Train Loss: 0.53926\n","Epoch: 00 [11341/20026 ( 57%)], Train Loss: 0.53927\n","Epoch: 00 [11351/20026 ( 57%)], Train Loss: 0.53908\n","Epoch: 00 [11361/20026 ( 57%)], Train Loss: 0.53901\n","Epoch: 00 [11371/20026 ( 57%)], Train Loss: 0.53901\n","Epoch: 00 [11381/20026 ( 57%)], Train Loss: 0.53925\n","Epoch: 00 [11391/20026 ( 57%)], Train Loss: 0.53953\n","Epoch: 00 [11401/20026 ( 57%)], Train Loss: 0.53944\n","Epoch: 00 [11411/20026 ( 57%)], Train Loss: 0.53959\n","Epoch: 00 [11421/20026 ( 57%)], Train Loss: 0.53936\n","Epoch: 00 [11431/20026 ( 57%)], Train Loss: 0.53925\n","Epoch: 00 [11441/20026 ( 57%)], Train Loss: 0.53913\n","Epoch: 00 [11451/20026 ( 57%)], Train Loss: 0.53906\n","Epoch: 00 [11461/20026 ( 57%)], Train Loss: 0.53908\n","Epoch: 00 [11471/20026 ( 57%)], Train Loss: 0.53887\n","Epoch: 00 [11481/20026 ( 57%)], Train Loss: 0.53887\n","Epoch: 00 [11491/20026 ( 57%)], Train Loss: 0.53891\n","Epoch: 00 [11501/20026 ( 57%)], Train Loss: 0.53846\n","Epoch: 00 [11511/20026 ( 57%)], Train Loss: 0.53837\n","Epoch: 00 [11521/20026 ( 58%)], Train Loss: 0.53861\n","Epoch: 00 [11531/20026 ( 58%)], Train Loss: 0.53835\n","Epoch: 00 [11541/20026 ( 58%)], Train Loss: 0.53832\n","Epoch: 00 [11551/20026 ( 58%)], Train Loss: 0.53815\n","Epoch: 00 [11561/20026 ( 58%)], Train Loss: 0.53865\n","Epoch: 00 [11571/20026 ( 58%)], Train Loss: 0.53837\n","Epoch: 00 [11581/20026 ( 58%)], Train Loss: 0.53815\n","Epoch: 00 [11591/20026 ( 58%)], Train Loss: 0.53780\n","Epoch: 00 [11601/20026 ( 58%)], Train Loss: 0.53749\n","Epoch: 00 [11611/20026 ( 58%)], Train Loss: 0.53740\n","Epoch: 00 [11621/20026 ( 58%)], Train Loss: 0.53736\n","Epoch: 00 [11631/20026 ( 58%)], Train Loss: 0.53735\n","Epoch: 00 [11641/20026 ( 58%)], Train Loss: 0.53702\n","Epoch: 00 [11651/20026 ( 58%)], Train Loss: 0.53681\n","Epoch: 00 [11661/20026 ( 58%)], Train Loss: 0.53658\n","Epoch: 00 [11671/20026 ( 58%)], Train Loss: 0.53644\n","Epoch: 00 [11681/20026 ( 58%)], Train Loss: 0.53618\n","Epoch: 00 [11691/20026 ( 58%)], Train Loss: 0.53619\n","Epoch: 00 [11701/20026 ( 58%)], Train Loss: 0.53644\n","Epoch: 00 [11711/20026 ( 58%)], Train Loss: 0.53630\n","Epoch: 00 [11721/20026 ( 59%)], Train Loss: 0.53620\n","Epoch: 00 [11731/20026 ( 59%)], Train Loss: 0.53584\n","Epoch: 00 [11741/20026 ( 59%)], Train Loss: 0.53566\n","Epoch: 00 [11751/20026 ( 59%)], Train Loss: 0.53567\n","Epoch: 00 [11761/20026 ( 59%)], Train Loss: 0.53535\n","Epoch: 00 [11771/20026 ( 59%)], Train Loss: 0.53518\n","Epoch: 00 [11781/20026 ( 59%)], Train Loss: 0.53479\n","Epoch: 00 [11791/20026 ( 59%)], Train Loss: 0.53478\n","Epoch: 00 [11801/20026 ( 59%)], Train Loss: 0.53461\n","Epoch: 00 [11811/20026 ( 59%)], Train Loss: 0.53462\n","Epoch: 00 [11821/20026 ( 59%)], Train Loss: 0.53417\n","Epoch: 00 [11831/20026 ( 59%)], Train Loss: 0.53437\n","Epoch: 00 [11841/20026 ( 59%)], Train Loss: 0.53413\n","Epoch: 00 [11851/20026 ( 59%)], Train Loss: 0.53423\n","Epoch: 00 [11861/20026 ( 59%)], Train Loss: 0.53407\n","Epoch: 00 [11871/20026 ( 59%)], Train Loss: 0.53375\n","Epoch: 00 [11881/20026 ( 59%)], Train Loss: 0.53336\n","Epoch: 00 [11891/20026 ( 59%)], Train Loss: 0.53322\n","Epoch: 00 [11901/20026 ( 59%)], Train Loss: 0.53326\n","Epoch: 00 [11911/20026 ( 59%)], Train Loss: 0.53302\n","Epoch: 00 [11921/20026 ( 60%)], Train Loss: 0.53284\n","Epoch: 00 [11931/20026 ( 60%)], Train Loss: 0.53303\n","Epoch: 00 [11941/20026 ( 60%)], Train Loss: 0.53289\n","Epoch: 00 [11951/20026 ( 60%)], Train Loss: 0.53266\n","Epoch: 00 [11961/20026 ( 60%)], Train Loss: 0.53251\n","Epoch: 00 [11971/20026 ( 60%)], Train Loss: 0.53237\n","Epoch: 00 [11981/20026 ( 60%)], Train Loss: 0.53243\n","Epoch: 00 [11991/20026 ( 60%)], Train Loss: 0.53239\n","Epoch: 00 [12001/20026 ( 60%)], Train Loss: 0.53231\n","Epoch: 00 [12011/20026 ( 60%)], Train Loss: 0.53207\n","Epoch: 00 [12021/20026 ( 60%)], Train Loss: 0.53185\n","Epoch: 00 [12031/20026 ( 60%)], Train Loss: 0.53158\n","Epoch: 00 [12041/20026 ( 60%)], Train Loss: 0.53182\n","Epoch: 00 [12051/20026 ( 60%)], Train Loss: 0.53139\n","Epoch: 00 [12061/20026 ( 60%)], Train Loss: 0.53138\n","Epoch: 00 [12071/20026 ( 60%)], Train Loss: 0.53111\n","Epoch: 00 [12081/20026 ( 60%)], Train Loss: 0.53079\n","Epoch: 00 [12091/20026 ( 60%)], Train Loss: 0.53077\n","Epoch: 00 [12101/20026 ( 60%)], Train Loss: 0.53055\n","Epoch: 00 [12111/20026 ( 60%)], Train Loss: 0.53080\n","Epoch: 00 [12121/20026 ( 61%)], Train Loss: 0.53059\n","Epoch: 00 [12131/20026 ( 61%)], Train Loss: 0.53047\n","Epoch: 00 [12141/20026 ( 61%)], Train Loss: 0.53034\n","Epoch: 00 [12151/20026 ( 61%)], Train Loss: 0.53028\n","Epoch: 00 [12161/20026 ( 61%)], Train Loss: 0.53005\n","Epoch: 00 [12171/20026 ( 61%)], Train Loss: 0.53019\n","Epoch: 00 [12181/20026 ( 61%)], Train Loss: 0.52978\n","Epoch: 00 [12191/20026 ( 61%)], Train Loss: 0.52964\n","Epoch: 00 [12201/20026 ( 61%)], Train Loss: 0.52971\n","Epoch: 00 [12211/20026 ( 61%)], Train Loss: 0.52942\n","Epoch: 00 [12221/20026 ( 61%)], Train Loss: 0.52927\n","Epoch: 00 [12231/20026 ( 61%)], Train Loss: 0.52921\n","Epoch: 00 [12241/20026 ( 61%)], Train Loss: 0.52893\n","Epoch: 00 [12251/20026 ( 61%)], Train Loss: 0.52884\n","Epoch: 00 [12261/20026 ( 61%)], Train Loss: 0.52869\n","Epoch: 00 [12271/20026 ( 61%)], Train Loss: 0.52855\n","Epoch: 00 [12281/20026 ( 61%)], Train Loss: 0.52834\n","Epoch: 00 [12291/20026 ( 61%)], Train Loss: 0.52816\n","Epoch: 00 [12301/20026 ( 61%)], Train Loss: 0.52808\n","Epoch: 00 [12311/20026 ( 61%)], Train Loss: 0.52811\n","Epoch: 00 [12321/20026 ( 62%)], Train Loss: 0.52801\n","Epoch: 00 [12331/20026 ( 62%)], Train Loss: 0.52800\n","Epoch: 00 [12341/20026 ( 62%)], Train Loss: 0.52824\n","Epoch: 00 [12351/20026 ( 62%)], Train Loss: 0.52799\n","Epoch: 00 [12361/20026 ( 62%)], Train Loss: 0.52770\n","Epoch: 00 [12371/20026 ( 62%)], Train Loss: 0.52782\n","Epoch: 00 [12381/20026 ( 62%)], Train Loss: 0.52758\n","Epoch: 00 [12391/20026 ( 62%)], Train Loss: 0.52730\n","Epoch: 00 [12401/20026 ( 62%)], Train Loss: 0.52733\n","Epoch: 00 [12411/20026 ( 62%)], Train Loss: 0.52736\n","Epoch: 00 [12421/20026 ( 62%)], Train Loss: 0.52714\n","Epoch: 00 [12431/20026 ( 62%)], Train Loss: 0.52695\n","Epoch: 00 [12441/20026 ( 62%)], Train Loss: 0.52668\n","Epoch: 00 [12451/20026 ( 62%)], Train Loss: 0.52674\n","Epoch: 00 [12461/20026 ( 62%)], Train Loss: 0.52692\n","Epoch: 00 [12471/20026 ( 62%)], Train Loss: 0.52673\n","Epoch: 00 [12481/20026 ( 62%)], Train Loss: 0.52657\n","Epoch: 00 [12491/20026 ( 62%)], Train Loss: 0.52636\n","Epoch: 00 [12501/20026 ( 62%)], Train Loss: 0.52620\n","Epoch: 00 [12511/20026 ( 62%)], Train Loss: 0.52605\n","Epoch: 00 [12521/20026 ( 63%)], Train Loss: 0.52609\n","Epoch: 00 [12531/20026 ( 63%)], Train Loss: 0.52639\n","Epoch: 00 [12541/20026 ( 63%)], Train Loss: 0.52625\n","Epoch: 00 [12551/20026 ( 63%)], Train Loss: 0.52640\n","Epoch: 00 [12561/20026 ( 63%)], Train Loss: 0.52608\n","Epoch: 00 [12571/20026 ( 63%)], Train Loss: 0.52616\n","Epoch: 00 [12581/20026 ( 63%)], Train Loss: 0.52609\n","Epoch: 00 [12591/20026 ( 63%)], Train Loss: 0.52586\n","Epoch: 00 [12601/20026 ( 63%)], Train Loss: 0.52559\n","Epoch: 00 [12611/20026 ( 63%)], Train Loss: 0.52550\n","Epoch: 00 [12621/20026 ( 63%)], Train Loss: 0.52534\n","Epoch: 00 [12631/20026 ( 63%)], Train Loss: 0.52501\n","Epoch: 00 [12641/20026 ( 63%)], Train Loss: 0.52488\n","Epoch: 00 [12651/20026 ( 63%)], Train Loss: 0.52450\n","Epoch: 00 [12661/20026 ( 63%)], Train Loss: 0.52425\n","Epoch: 00 [12671/20026 ( 63%)], Train Loss: 0.52395\n","Epoch: 00 [12681/20026 ( 63%)], Train Loss: 0.52401\n","Epoch: 00 [12691/20026 ( 63%)], Train Loss: 0.52383\n","Epoch: 00 [12701/20026 ( 63%)], Train Loss: 0.52379\n","Epoch: 00 [12711/20026 ( 63%)], Train Loss: 0.52410\n","Epoch: 00 [12721/20026 ( 64%)], Train Loss: 0.52411\n","Epoch: 00 [12731/20026 ( 64%)], Train Loss: 0.52398\n","Epoch: 00 [12741/20026 ( 64%)], Train Loss: 0.52365\n","Epoch: 00 [12751/20026 ( 64%)], Train Loss: 0.52356\n","Epoch: 00 [12761/20026 ( 64%)], Train Loss: 0.52377\n","Epoch: 00 [12771/20026 ( 64%)], Train Loss: 0.52362\n","Epoch: 00 [12781/20026 ( 64%)], Train Loss: 0.52368\n","Epoch: 00 [12791/20026 ( 64%)], Train Loss: 0.52345\n","Epoch: 00 [12801/20026 ( 64%)], Train Loss: 0.52329\n","Epoch: 00 [12811/20026 ( 64%)], Train Loss: 0.52310\n","Epoch: 00 [12821/20026 ( 64%)], Train Loss: 0.52306\n","Epoch: 00 [12831/20026 ( 64%)], Train Loss: 0.52269\n","Epoch: 00 [12841/20026 ( 64%)], Train Loss: 0.52277\n","Epoch: 00 [12851/20026 ( 64%)], Train Loss: 0.52264\n","Epoch: 00 [12861/20026 ( 64%)], Train Loss: 0.52270\n","Epoch: 00 [12871/20026 ( 64%)], Train Loss: 0.52268\n","Epoch: 00 [12881/20026 ( 64%)], Train Loss: 0.52239\n","Epoch: 00 [12891/20026 ( 64%)], Train Loss: 0.52202\n","Epoch: 00 [12901/20026 ( 64%)], Train Loss: 0.52189\n","Epoch: 00 [12911/20026 ( 64%)], Train Loss: 0.52157\n","Epoch: 00 [12921/20026 ( 65%)], Train Loss: 0.52134\n","Epoch: 00 [12931/20026 ( 65%)], Train Loss: 0.52120\n","Epoch: 00 [12941/20026 ( 65%)], Train Loss: 0.52101\n","Epoch: 00 [12951/20026 ( 65%)], Train Loss: 0.52104\n","Epoch: 00 [12961/20026 ( 65%)], Train Loss: 0.52124\n","Epoch: 00 [12971/20026 ( 65%)], Train Loss: 0.52122\n","Epoch: 00 [12981/20026 ( 65%)], Train Loss: 0.52125\n","Epoch: 00 [12991/20026 ( 65%)], Train Loss: 0.52109\n","Epoch: 00 [13001/20026 ( 65%)], Train Loss: 0.52084\n","Epoch: 00 [13011/20026 ( 65%)], Train Loss: 0.52067\n","Epoch: 00 [13021/20026 ( 65%)], Train Loss: 0.52074\n","Epoch: 00 [13031/20026 ( 65%)], Train Loss: 0.52049\n","Epoch: 00 [13041/20026 ( 65%)], Train Loss: 0.52052\n","Epoch: 00 [13051/20026 ( 65%)], Train Loss: 0.52023\n","Epoch: 00 [13061/20026 ( 65%)], Train Loss: 0.52003\n","Epoch: 00 [13071/20026 ( 65%)], Train Loss: 0.52031\n","Epoch: 00 [13081/20026 ( 65%)], Train Loss: 0.52001\n","Epoch: 00 [13091/20026 ( 65%)], Train Loss: 0.51968\n","Epoch: 00 [13101/20026 ( 65%)], Train Loss: 0.51940\n","Epoch: 00 [13111/20026 ( 65%)], Train Loss: 0.51932\n","Epoch: 00 [13121/20026 ( 66%)], Train Loss: 0.51934\n","Epoch: 00 [13131/20026 ( 66%)], Train Loss: 0.51953\n","Epoch: 00 [13141/20026 ( 66%)], Train Loss: 0.51942\n","Epoch: 00 [13151/20026 ( 66%)], Train Loss: 0.51923\n","Epoch: 00 [13161/20026 ( 66%)], Train Loss: 0.51917\n","Epoch: 00 [13171/20026 ( 66%)], Train Loss: 0.51918\n","Epoch: 00 [13181/20026 ( 66%)], Train Loss: 0.51915\n","Epoch: 00 [13191/20026 ( 66%)], Train Loss: 0.51934\n","Epoch: 00 [13201/20026 ( 66%)], Train Loss: 0.51956\n","Epoch: 00 [13211/20026 ( 66%)], Train Loss: 0.51990\n","Epoch: 00 [13221/20026 ( 66%)], Train Loss: 0.51966\n","Epoch: 00 [13231/20026 ( 66%)], Train Loss: 0.51976\n","Epoch: 00 [13241/20026 ( 66%)], Train Loss: 0.51974\n","Epoch: 00 [13251/20026 ( 66%)], Train Loss: 0.51948\n","Epoch: 00 [13261/20026 ( 66%)], Train Loss: 0.51942\n","Epoch: 00 [13271/20026 ( 66%)], Train Loss: 0.51935\n","Epoch: 00 [13281/20026 ( 66%)], Train Loss: 0.51923\n","Epoch: 00 [13291/20026 ( 66%)], Train Loss: 0.51939\n","Epoch: 00 [13301/20026 ( 66%)], Train Loss: 0.51913\n","Epoch: 00 [13311/20026 ( 66%)], Train Loss: 0.51925\n","Epoch: 00 [13321/20026 ( 67%)], Train Loss: 0.51906\n","Epoch: 00 [13331/20026 ( 67%)], Train Loss: 0.51898\n","Epoch: 00 [13341/20026 ( 67%)], Train Loss: 0.51892\n","Epoch: 00 [13351/20026 ( 67%)], Train Loss: 0.51871\n","Epoch: 00 [13361/20026 ( 67%)], Train Loss: 0.51877\n","Epoch: 00 [13371/20026 ( 67%)], Train Loss: 0.51863\n","Epoch: 00 [13381/20026 ( 67%)], Train Loss: 0.51864\n","Epoch: 00 [13391/20026 ( 67%)], Train Loss: 0.51834\n","Epoch: 00 [13401/20026 ( 67%)], Train Loss: 0.51801\n","Epoch: 00 [13411/20026 ( 67%)], Train Loss: 0.51793\n","Epoch: 00 [13421/20026 ( 67%)], Train Loss: 0.51765\n","Epoch: 00 [13431/20026 ( 67%)], Train Loss: 0.51751\n","Epoch: 00 [13441/20026 ( 67%)], Train Loss: 0.51743\n","Epoch: 00 [13451/20026 ( 67%)], Train Loss: 0.51719\n","Epoch: 00 [13461/20026 ( 67%)], Train Loss: 0.51701\n","Epoch: 00 [13471/20026 ( 67%)], Train Loss: 0.51668\n","Epoch: 00 [13481/20026 ( 67%)], Train Loss: 0.51664\n","Epoch: 00 [13491/20026 ( 67%)], Train Loss: 0.51677\n","Epoch: 00 [13501/20026 ( 67%)], Train Loss: 0.51697\n","Epoch: 00 [13511/20026 ( 67%)], Train Loss: 0.51671\n","Epoch: 00 [13521/20026 ( 68%)], Train Loss: 0.51651\n","Epoch: 00 [13531/20026 ( 68%)], Train Loss: 0.51636\n","Epoch: 00 [13541/20026 ( 68%)], Train Loss: 0.51629\n","Epoch: 00 [13551/20026 ( 68%)], Train Loss: 0.51605\n","Epoch: 00 [13561/20026 ( 68%)], Train Loss: 0.51592\n","Epoch: 00 [13571/20026 ( 68%)], Train Loss: 0.51565\n","Epoch: 00 [13581/20026 ( 68%)], Train Loss: 0.51550\n","Epoch: 00 [13591/20026 ( 68%)], Train Loss: 0.51513\n","Epoch: 00 [13601/20026 ( 68%)], Train Loss: 0.51502\n","Epoch: 00 [13611/20026 ( 68%)], Train Loss: 0.51501\n","Epoch: 00 [13621/20026 ( 68%)], Train Loss: 0.51469\n","Epoch: 00 [13631/20026 ( 68%)], Train Loss: 0.51457\n","Epoch: 00 [13641/20026 ( 68%)], Train Loss: 0.51444\n","Epoch: 00 [13651/20026 ( 68%)], Train Loss: 0.51440\n","Epoch: 00 [13661/20026 ( 68%)], Train Loss: 0.51490\n","Epoch: 00 [13671/20026 ( 68%)], Train Loss: 0.51475\n","Epoch: 00 [13681/20026 ( 68%)], Train Loss: 0.51469\n","Epoch: 00 [13691/20026 ( 68%)], Train Loss: 0.51472\n","Epoch: 00 [13701/20026 ( 68%)], Train Loss: 0.51470\n","Epoch: 00 [13711/20026 ( 68%)], Train Loss: 0.51440\n","Epoch: 00 [13721/20026 ( 69%)], Train Loss: 0.51432\n","Epoch: 00 [13731/20026 ( 69%)], Train Loss: 0.51428\n","Epoch: 00 [13741/20026 ( 69%)], Train Loss: 0.51435\n","Epoch: 00 [13751/20026 ( 69%)], Train Loss: 0.51414\n","Epoch: 00 [13761/20026 ( 69%)], Train Loss: 0.51409\n","Epoch: 00 [13771/20026 ( 69%)], Train Loss: 0.51390\n","Epoch: 00 [13781/20026 ( 69%)], Train Loss: 0.51396\n","Epoch: 00 [13791/20026 ( 69%)], Train Loss: 0.51371\n","Epoch: 00 [13801/20026 ( 69%)], Train Loss: 0.51341\n","Epoch: 00 [13811/20026 ( 69%)], Train Loss: 0.51335\n","Epoch: 00 [13821/20026 ( 69%)], Train Loss: 0.51327\n","Epoch: 00 [13831/20026 ( 69%)], Train Loss: 0.51338\n","Epoch: 00 [13841/20026 ( 69%)], Train Loss: 0.51340\n","Epoch: 00 [13851/20026 ( 69%)], Train Loss: 0.51369\n","Epoch: 00 [13861/20026 ( 69%)], Train Loss: 0.51348\n","Epoch: 00 [13871/20026 ( 69%)], Train Loss: 0.51353\n","Epoch: 00 [13881/20026 ( 69%)], Train Loss: 0.51337\n","Epoch: 00 [13891/20026 ( 69%)], Train Loss: 0.51355\n","Epoch: 00 [13901/20026 ( 69%)], Train Loss: 0.51355\n","Epoch: 00 [13911/20026 ( 69%)], Train Loss: 0.51370\n","Epoch: 00 [13921/20026 ( 70%)], Train Loss: 0.51368\n","Epoch: 00 [13931/20026 ( 70%)], Train Loss: 0.51361\n","Epoch: 00 [13941/20026 ( 70%)], Train Loss: 0.51348\n","Epoch: 00 [13951/20026 ( 70%)], Train Loss: 0.51334\n","Epoch: 00 [13961/20026 ( 70%)], Train Loss: 0.51321\n","Epoch: 00 [13971/20026 ( 70%)], Train Loss: 0.51305\n","Epoch: 00 [13981/20026 ( 70%)], Train Loss: 0.51269\n","Epoch: 00 [13991/20026 ( 70%)], Train Loss: 0.51258\n","Epoch: 00 [14001/20026 ( 70%)], Train Loss: 0.51228\n","Epoch: 00 [14011/20026 ( 70%)], Train Loss: 0.51238\n","Epoch: 00 [14021/20026 ( 70%)], Train Loss: 0.51248\n","Epoch: 00 [14031/20026 ( 70%)], Train Loss: 0.51233\n","Epoch: 00 [14041/20026 ( 70%)], Train Loss: 0.51247\n","Epoch: 00 [14051/20026 ( 70%)], Train Loss: 0.51241\n","Epoch: 00 [14061/20026 ( 70%)], Train Loss: 0.51282\n","Epoch: 00 [14071/20026 ( 70%)], Train Loss: 0.51272\n","Epoch: 00 [14081/20026 ( 70%)], Train Loss: 0.51237\n","Epoch: 00 [14091/20026 ( 70%)], Train Loss: 0.51218\n","Epoch: 00 [14101/20026 ( 70%)], Train Loss: 0.51186\n","Epoch: 00 [14111/20026 ( 70%)], Train Loss: 0.51159\n","Epoch: 00 [14121/20026 ( 71%)], Train Loss: 0.51153\n","Epoch: 00 [14131/20026 ( 71%)], Train Loss: 0.51143\n","Epoch: 00 [14141/20026 ( 71%)], Train Loss: 0.51134\n","Epoch: 00 [14151/20026 ( 71%)], Train Loss: 0.51108\n","Epoch: 00 [14161/20026 ( 71%)], Train Loss: 0.51119\n","Epoch: 00 [14171/20026 ( 71%)], Train Loss: 0.51150\n","Epoch: 00 [14181/20026 ( 71%)], Train Loss: 0.51130\n","Epoch: 00 [14191/20026 ( 71%)], Train Loss: 0.51114\n","Epoch: 00 [14201/20026 ( 71%)], Train Loss: 0.51084\n","Epoch: 00 [14211/20026 ( 71%)], Train Loss: 0.51076\n","Epoch: 00 [14221/20026 ( 71%)], Train Loss: 0.51060\n","Epoch: 00 [14231/20026 ( 71%)], Train Loss: 0.51070\n","Epoch: 00 [14241/20026 ( 71%)], Train Loss: 0.51045\n","Epoch: 00 [14251/20026 ( 71%)], Train Loss: 0.51015\n","Epoch: 00 [14261/20026 ( 71%)], Train Loss: 0.51029\n","Epoch: 00 [14271/20026 ( 71%)], Train Loss: 0.51002\n","Epoch: 00 [14281/20026 ( 71%)], Train Loss: 0.50982\n","Epoch: 00 [14291/20026 ( 71%)], Train Loss: 0.50970\n","Epoch: 00 [14301/20026 ( 71%)], Train Loss: 0.50965\n","Epoch: 00 [14311/20026 ( 71%)], Train Loss: 0.50937\n","Epoch: 00 [14321/20026 ( 72%)], Train Loss: 0.50945\n","Epoch: 00 [14331/20026 ( 72%)], Train Loss: 0.50934\n","Epoch: 00 [14341/20026 ( 72%)], Train Loss: 0.50914\n","Epoch: 00 [14351/20026 ( 72%)], Train Loss: 0.50909\n","Epoch: 00 [14361/20026 ( 72%)], Train Loss: 0.50920\n","Epoch: 00 [14371/20026 ( 72%)], Train Loss: 0.50900\n","Epoch: 00 [14381/20026 ( 72%)], Train Loss: 0.50879\n","Epoch: 00 [14391/20026 ( 72%)], Train Loss: 0.50877\n","Epoch: 00 [14401/20026 ( 72%)], Train Loss: 0.50860\n","Epoch: 00 [14411/20026 ( 72%)], Train Loss: 0.50836\n","Epoch: 00 [14421/20026 ( 72%)], Train Loss: 0.50838\n","Epoch: 00 [14431/20026 ( 72%)], Train Loss: 0.50806\n","Epoch: 00 [14441/20026 ( 72%)], Train Loss: 0.50775\n","Epoch: 00 [14451/20026 ( 72%)], Train Loss: 0.50781\n","Epoch: 00 [14461/20026 ( 72%)], Train Loss: 0.50772\n","Epoch: 00 [14471/20026 ( 72%)], Train Loss: 0.50741\n","Epoch: 00 [14481/20026 ( 72%)], Train Loss: 0.50731\n","Epoch: 00 [14491/20026 ( 72%)], Train Loss: 0.50736\n","Epoch: 00 [14501/20026 ( 72%)], Train Loss: 0.50711\n","Epoch: 00 [14511/20026 ( 72%)], Train Loss: 0.50695\n","Epoch: 00 [14521/20026 ( 73%)], Train Loss: 0.50689\n","Epoch: 00 [14531/20026 ( 73%)], Train Loss: 0.50662\n","Epoch: 00 [14541/20026 ( 73%)], Train Loss: 0.50642\n","Epoch: 00 [14551/20026 ( 73%)], Train Loss: 0.50659\n","Epoch: 00 [14561/20026 ( 73%)], Train Loss: 0.50640\n","Epoch: 00 [14571/20026 ( 73%)], Train Loss: 0.50629\n","Epoch: 00 [14581/20026 ( 73%)], Train Loss: 0.50612\n","Epoch: 00 [14591/20026 ( 73%)], Train Loss: 0.50600\n","Epoch: 00 [14601/20026 ( 73%)], Train Loss: 0.50593\n","Epoch: 00 [14611/20026 ( 73%)], Train Loss: 0.50597\n","Epoch: 00 [14621/20026 ( 73%)], Train Loss: 0.50576\n","Epoch: 00 [14631/20026 ( 73%)], Train Loss: 0.50605\n","Epoch: 00 [14641/20026 ( 73%)], Train Loss: 0.50607\n","Epoch: 00 [14651/20026 ( 73%)], Train Loss: 0.50590\n","Epoch: 00 [14661/20026 ( 73%)], Train Loss: 0.50567\n","Epoch: 00 [14671/20026 ( 73%)], Train Loss: 0.50561\n","Epoch: 00 [14681/20026 ( 73%)], Train Loss: 0.50548\n","Epoch: 00 [14691/20026 ( 73%)], Train Loss: 0.50531\n","Epoch: 00 [14701/20026 ( 73%)], Train Loss: 0.50515\n","Epoch: 00 [14711/20026 ( 73%)], Train Loss: 0.50509\n","Epoch: 00 [14721/20026 ( 74%)], Train Loss: 0.50507\n","Epoch: 00 [14731/20026 ( 74%)], Train Loss: 0.50494\n","Epoch: 00 [14741/20026 ( 74%)], Train Loss: 0.50482\n","Epoch: 00 [14751/20026 ( 74%)], Train Loss: 0.50452\n","Epoch: 00 [14761/20026 ( 74%)], Train Loss: 0.50426\n","Epoch: 00 [14771/20026 ( 74%)], Train Loss: 0.50446\n","Epoch: 00 [14781/20026 ( 74%)], Train Loss: 0.50413\n","Epoch: 00 [14791/20026 ( 74%)], Train Loss: 0.50432\n","Epoch: 00 [14801/20026 ( 74%)], Train Loss: 0.50427\n","Epoch: 00 [14811/20026 ( 74%)], Train Loss: 0.50434\n","Epoch: 00 [14821/20026 ( 74%)], Train Loss: 0.50427\n","Epoch: 00 [14831/20026 ( 74%)], Train Loss: 0.50419\n","Epoch: 00 [14841/20026 ( 74%)], Train Loss: 0.50418\n","Epoch: 00 [14851/20026 ( 74%)], Train Loss: 0.50410\n","Epoch: 00 [14861/20026 ( 74%)], Train Loss: 0.50406\n","Epoch: 00 [14871/20026 ( 74%)], Train Loss: 0.50437\n","Epoch: 00 [14881/20026 ( 74%)], Train Loss: 0.50417\n","Epoch: 00 [14891/20026 ( 74%)], Train Loss: 0.50425\n","Epoch: 00 [14901/20026 ( 74%)], Train Loss: 0.50406\n","Epoch: 00 [14911/20026 ( 74%)], Train Loss: 0.50391\n","Epoch: 00 [14921/20026 ( 75%)], Train Loss: 0.50387\n","Epoch: 00 [14931/20026 ( 75%)], Train Loss: 0.50364\n","Epoch: 00 [14941/20026 ( 75%)], Train Loss: 0.50347\n","Epoch: 00 [14951/20026 ( 75%)], Train Loss: 0.50340\n","Epoch: 00 [14961/20026 ( 75%)], Train Loss: 0.50343\n","Epoch: 00 [14971/20026 ( 75%)], Train Loss: 0.50350\n","Epoch: 00 [14981/20026 ( 75%)], Train Loss: 0.50360\n","Epoch: 00 [14991/20026 ( 75%)], Train Loss: 0.50346\n","Epoch: 00 [15001/20026 ( 75%)], Train Loss: 0.50335\n","Epoch: 00 [15011/20026 ( 75%)], Train Loss: 0.50306\n","Epoch: 00 [15021/20026 ( 75%)], Train Loss: 0.50303\n","Epoch: 00 [15031/20026 ( 75%)], Train Loss: 0.50295\n","Epoch: 00 [15041/20026 ( 75%)], Train Loss: 0.50278\n","Epoch: 00 [15051/20026 ( 75%)], Train Loss: 0.50252\n","Epoch: 00 [15061/20026 ( 75%)], Train Loss: 0.50224\n","Epoch: 00 [15071/20026 ( 75%)], Train Loss: 0.50198\n","Epoch: 00 [15081/20026 ( 75%)], Train Loss: 0.50208\n","Epoch: 00 [15091/20026 ( 75%)], Train Loss: 0.50175\n","Epoch: 00 [15101/20026 ( 75%)], Train Loss: 0.50166\n","Epoch: 00 [15111/20026 ( 75%)], Train Loss: 0.50155\n","Epoch: 00 [15121/20026 ( 76%)], Train Loss: 0.50143\n","Epoch: 00 [15131/20026 ( 76%)], Train Loss: 0.50114\n","Epoch: 00 [15141/20026 ( 76%)], Train Loss: 0.50124\n","Epoch: 00 [15151/20026 ( 76%)], Train Loss: 0.50099\n","Epoch: 00 [15161/20026 ( 76%)], Train Loss: 0.50105\n","Epoch: 00 [15171/20026 ( 76%)], Train Loss: 0.50099\n","Epoch: 00 [15181/20026 ( 76%)], Train Loss: 0.50070\n","Epoch: 00 [15191/20026 ( 76%)], Train Loss: 0.50045\n","Epoch: 00 [15201/20026 ( 76%)], Train Loss: 0.50036\n","Epoch: 00 [15211/20026 ( 76%)], Train Loss: 0.50024\n","Epoch: 00 [15221/20026 ( 76%)], Train Loss: 0.50018\n","Epoch: 00 [15231/20026 ( 76%)], Train Loss: 0.49996\n","Epoch: 00 [15241/20026 ( 76%)], Train Loss: 0.50003\n","Epoch: 00 [15251/20026 ( 76%)], Train Loss: 0.50001\n","Epoch: 00 [15261/20026 ( 76%)], Train Loss: 0.49980\n","Epoch: 00 [15271/20026 ( 76%)], Train Loss: 0.49989\n","Epoch: 00 [15281/20026 ( 76%)], Train Loss: 0.49961\n","Epoch: 00 [15291/20026 ( 76%)], Train Loss: 0.49934\n","Epoch: 00 [15301/20026 ( 76%)], Train Loss: 0.49943\n","Epoch: 00 [15311/20026 ( 76%)], Train Loss: 0.49939\n","Epoch: 00 [15321/20026 ( 77%)], Train Loss: 0.49942\n","Epoch: 00 [15331/20026 ( 77%)], Train Loss: 0.49932\n","Epoch: 00 [15341/20026 ( 77%)], Train Loss: 0.49928\n","Epoch: 00 [15351/20026 ( 77%)], Train Loss: 0.49909\n","Epoch: 00 [15361/20026 ( 77%)], Train Loss: 0.49889\n","Epoch: 00 [15371/20026 ( 77%)], Train Loss: 0.49879\n","Epoch: 00 [15381/20026 ( 77%)], Train Loss: 0.49881\n","Epoch: 00 [15391/20026 ( 77%)], Train Loss: 0.49898\n","Epoch: 00 [15401/20026 ( 77%)], Train Loss: 0.49900\n","Epoch: 00 [15411/20026 ( 77%)], Train Loss: 0.49898\n","Epoch: 00 [15421/20026 ( 77%)], Train Loss: 0.49879\n","Epoch: 00 [15431/20026 ( 77%)], Train Loss: 0.49869\n","Epoch: 00 [15441/20026 ( 77%)], Train Loss: 0.49861\n","Epoch: 00 [15451/20026 ( 77%)], Train Loss: 0.49845\n","Epoch: 00 [15461/20026 ( 77%)], Train Loss: 0.49850\n","Epoch: 00 [15471/20026 ( 77%)], Train Loss: 0.49862\n","Epoch: 00 [15481/20026 ( 77%)], Train Loss: 0.49847\n","Epoch: 00 [15491/20026 ( 77%)], Train Loss: 0.49846\n","Epoch: 00 [15501/20026 ( 77%)], Train Loss: 0.49833\n","Epoch: 00 [15511/20026 ( 77%)], Train Loss: 0.49805\n","Epoch: 00 [15521/20026 ( 78%)], Train Loss: 0.49789\n","Epoch: 00 [15531/20026 ( 78%)], Train Loss: 0.49777\n","Epoch: 00 [15541/20026 ( 78%)], Train Loss: 0.49780\n","Epoch: 00 [15551/20026 ( 78%)], Train Loss: 0.49763\n","Epoch: 00 [15561/20026 ( 78%)], Train Loss: 0.49760\n","Epoch: 00 [15571/20026 ( 78%)], Train Loss: 0.49740\n","Epoch: 00 [15581/20026 ( 78%)], Train Loss: 0.49732\n","Epoch: 00 [15591/20026 ( 78%)], Train Loss: 0.49733\n","Epoch: 00 [15601/20026 ( 78%)], Train Loss: 0.49733\n","Epoch: 00 [15611/20026 ( 78%)], Train Loss: 0.49708\n","Epoch: 00 [15621/20026 ( 78%)], Train Loss: 0.49708\n","Epoch: 00 [15631/20026 ( 78%)], Train Loss: 0.49690\n","Epoch: 00 [15641/20026 ( 78%)], Train Loss: 0.49693\n","Epoch: 00 [15651/20026 ( 78%)], Train Loss: 0.49686\n","Epoch: 00 [15661/20026 ( 78%)], Train Loss: 0.49684\n","Epoch: 00 [15671/20026 ( 78%)], Train Loss: 0.49693\n","Epoch: 00 [15681/20026 ( 78%)], Train Loss: 0.49683\n","Epoch: 00 [15691/20026 ( 78%)], Train Loss: 0.49682\n","Epoch: 00 [15701/20026 ( 78%)], Train Loss: 0.49670\n","Epoch: 00 [15711/20026 ( 78%)], Train Loss: 0.49665\n","Epoch: 00 [15721/20026 ( 79%)], Train Loss: 0.49646\n","Epoch: 00 [15731/20026 ( 79%)], Train Loss: 0.49623\n","Epoch: 00 [15741/20026 ( 79%)], Train Loss: 0.49599\n","Epoch: 00 [15751/20026 ( 79%)], Train Loss: 0.49625\n","Epoch: 00 [15761/20026 ( 79%)], Train Loss: 0.49626\n","Epoch: 00 [15771/20026 ( 79%)], Train Loss: 0.49602\n","Epoch: 00 [15781/20026 ( 79%)], Train Loss: 0.49596\n","Epoch: 00 [15791/20026 ( 79%)], Train Loss: 0.49582\n","Epoch: 00 [15801/20026 ( 79%)], Train Loss: 0.49591\n","Epoch: 00 [15811/20026 ( 79%)], Train Loss: 0.49615\n","Epoch: 00 [15821/20026 ( 79%)], Train Loss: 0.49625\n","Epoch: 00 [15831/20026 ( 79%)], Train Loss: 0.49617\n","Epoch: 00 [15841/20026 ( 79%)], Train Loss: 0.49604\n","Epoch: 00 [15851/20026 ( 79%)], Train Loss: 0.49595\n","Epoch: 00 [15861/20026 ( 79%)], Train Loss: 0.49604\n","Epoch: 00 [15871/20026 ( 79%)], Train Loss: 0.49575\n","Epoch: 00 [15881/20026 ( 79%)], Train Loss: 0.49583\n","Epoch: 00 [15891/20026 ( 79%)], Train Loss: 0.49571\n","Epoch: 00 [15901/20026 ( 79%)], Train Loss: 0.49565\n","Epoch: 00 [15911/20026 ( 79%)], Train Loss: 0.49540\n","Epoch: 00 [15921/20026 ( 80%)], Train Loss: 0.49536\n","Epoch: 00 [15931/20026 ( 80%)], Train Loss: 0.49528\n","Epoch: 00 [15941/20026 ( 80%)], Train Loss: 0.49508\n","Epoch: 00 [15951/20026 ( 80%)], Train Loss: 0.49485\n","Epoch: 00 [15961/20026 ( 80%)], Train Loss: 0.49471\n","Epoch: 00 [15971/20026 ( 80%)], Train Loss: 0.49448\n","Epoch: 00 [15981/20026 ( 80%)], Train Loss: 0.49442\n","Epoch: 00 [15991/20026 ( 80%)], Train Loss: 0.49422\n","Epoch: 00 [16001/20026 ( 80%)], Train Loss: 0.49400\n","Epoch: 00 [16011/20026 ( 80%)], Train Loss: 0.49393\n","Epoch: 00 [16021/20026 ( 80%)], Train Loss: 0.49368\n","Epoch: 00 [16031/20026 ( 80%)], Train Loss: 0.49361\n","Epoch: 00 [16041/20026 ( 80%)], Train Loss: 0.49350\n","Epoch: 00 [16051/20026 ( 80%)], Train Loss: 0.49344\n","Epoch: 00 [16061/20026 ( 80%)], Train Loss: 0.49324\n","Epoch: 00 [16071/20026 ( 80%)], Train Loss: 0.49312\n","Epoch: 00 [16081/20026 ( 80%)], Train Loss: 0.49308\n","Epoch: 00 [16091/20026 ( 80%)], Train Loss: 0.49305\n","Epoch: 00 [16101/20026 ( 80%)], Train Loss: 0.49289\n","Epoch: 00 [16111/20026 ( 80%)], Train Loss: 0.49291\n","Epoch: 00 [16121/20026 ( 81%)], Train Loss: 0.49273\n","Epoch: 00 [16131/20026 ( 81%)], Train Loss: 0.49281\n","Epoch: 00 [16141/20026 ( 81%)], Train Loss: 0.49251\n","Epoch: 00 [16151/20026 ( 81%)], Train Loss: 0.49226\n","Epoch: 00 [16161/20026 ( 81%)], Train Loss: 0.49231\n","Epoch: 00 [16171/20026 ( 81%)], Train Loss: 0.49240\n","Epoch: 00 [16181/20026 ( 81%)], Train Loss: 0.49241\n","Epoch: 00 [16191/20026 ( 81%)], Train Loss: 0.49232\n","Epoch: 00 [16201/20026 ( 81%)], Train Loss: 0.49253\n","Epoch: 00 [16211/20026 ( 81%)], Train Loss: 0.49245\n","Epoch: 00 [16221/20026 ( 81%)], Train Loss: 0.49230\n","Epoch: 00 [16231/20026 ( 81%)], Train Loss: 0.49220\n","Epoch: 00 [16241/20026 ( 81%)], Train Loss: 0.49228\n","Epoch: 00 [16251/20026 ( 81%)], Train Loss: 0.49206\n","Epoch: 00 [16261/20026 ( 81%)], Train Loss: 0.49190\n","Epoch: 00 [16271/20026 ( 81%)], Train Loss: 0.49168\n","Epoch: 00 [16281/20026 ( 81%)], Train Loss: 0.49168\n","Epoch: 00 [16291/20026 ( 81%)], Train Loss: 0.49180\n","Epoch: 00 [16301/20026 ( 81%)], Train Loss: 0.49177\n","Epoch: 00 [16311/20026 ( 81%)], Train Loss: 0.49177\n","Epoch: 00 [16321/20026 ( 81%)], Train Loss: 0.49164\n","Epoch: 00 [16331/20026 ( 82%)], Train Loss: 0.49187\n","Epoch: 00 [16341/20026 ( 82%)], Train Loss: 0.49161\n","Epoch: 00 [16351/20026 ( 82%)], Train Loss: 0.49146\n","Epoch: 00 [16361/20026 ( 82%)], Train Loss: 0.49147\n","Epoch: 00 [16371/20026 ( 82%)], Train Loss: 0.49173\n","Epoch: 00 [16381/20026 ( 82%)], Train Loss: 0.49154\n","Epoch: 00 [16391/20026 ( 82%)], Train Loss: 0.49142\n","Epoch: 00 [16401/20026 ( 82%)], Train Loss: 0.49126\n","Epoch: 00 [16411/20026 ( 82%)], Train Loss: 0.49124\n","Epoch: 00 [16421/20026 ( 82%)], Train Loss: 0.49136\n","Epoch: 00 [16431/20026 ( 82%)], Train Loss: 0.49130\n","Epoch: 00 [16441/20026 ( 82%)], Train Loss: 0.49109\n","Epoch: 00 [16451/20026 ( 82%)], Train Loss: 0.49102\n","Epoch: 00 [16461/20026 ( 82%)], Train Loss: 0.49109\n","Epoch: 00 [16471/20026 ( 82%)], Train Loss: 0.49093\n","Epoch: 00 [16481/20026 ( 82%)], Train Loss: 0.49090\n","Epoch: 00 [16491/20026 ( 82%)], Train Loss: 0.49072\n","Epoch: 00 [16501/20026 ( 82%)], Train Loss: 0.49065\n","Epoch: 00 [16511/20026 ( 82%)], Train Loss: 0.49047\n","Epoch: 00 [16521/20026 ( 82%)], Train Loss: 0.49041\n","Epoch: 00 [16531/20026 ( 83%)], Train Loss: 0.49033\n","Epoch: 00 [16541/20026 ( 83%)], Train Loss: 0.49008\n","Epoch: 00 [16551/20026 ( 83%)], Train Loss: 0.49023\n","Epoch: 00 [16561/20026 ( 83%)], Train Loss: 0.49012\n","Epoch: 00 [16571/20026 ( 83%)], Train Loss: 0.49024\n","Epoch: 00 [16581/20026 ( 83%)], Train Loss: 0.49016\n","Epoch: 00 [16591/20026 ( 83%)], Train Loss: 0.48994\n","Epoch: 00 [16601/20026 ( 83%)], Train Loss: 0.48971\n","Epoch: 00 [16611/20026 ( 83%)], Train Loss: 0.48955\n","Epoch: 00 [16621/20026 ( 83%)], Train Loss: 0.48932\n","Epoch: 00 [16631/20026 ( 83%)], Train Loss: 0.48946\n","Epoch: 00 [16641/20026 ( 83%)], Train Loss: 0.48937\n","Epoch: 00 [16651/20026 ( 83%)], Train Loss: 0.48928\n","Epoch: 00 [16661/20026 ( 83%)], Train Loss: 0.48930\n","Epoch: 00 [16671/20026 ( 83%)], Train Loss: 0.48935\n","Epoch: 00 [16681/20026 ( 83%)], Train Loss: 0.48918\n","Epoch: 00 [16691/20026 ( 83%)], Train Loss: 0.48900\n","Epoch: 00 [16701/20026 ( 83%)], Train Loss: 0.48883\n","Epoch: 00 [16711/20026 ( 83%)], Train Loss: 0.48873\n","Epoch: 00 [16721/20026 ( 83%)], Train Loss: 0.48867\n","Epoch: 00 [16731/20026 ( 84%)], Train Loss: 0.48863\n","Epoch: 00 [16741/20026 ( 84%)], Train Loss: 0.48868\n","Epoch: 00 [16751/20026 ( 84%)], Train Loss: 0.48854\n","Epoch: 00 [16761/20026 ( 84%)], Train Loss: 0.48835\n","Epoch: 00 [16771/20026 ( 84%)], Train Loss: 0.48816\n","Epoch: 00 [16781/20026 ( 84%)], Train Loss: 0.48823\n","Epoch: 00 [16791/20026 ( 84%)], Train Loss: 0.48797\n","Epoch: 00 [16801/20026 ( 84%)], Train Loss: 0.48775\n","Epoch: 00 [16811/20026 ( 84%)], Train Loss: 0.48758\n","Epoch: 00 [16821/20026 ( 84%)], Train Loss: 0.48746\n","Epoch: 00 [16831/20026 ( 84%)], Train Loss: 0.48726\n","Epoch: 00 [16841/20026 ( 84%)], Train Loss: 0.48723\n","Epoch: 00 [16851/20026 ( 84%)], Train Loss: 0.48731\n","Epoch: 00 [16861/20026 ( 84%)], Train Loss: 0.48743\n","Epoch: 00 [16871/20026 ( 84%)], Train Loss: 0.48731\n","Epoch: 00 [16881/20026 ( 84%)], Train Loss: 0.48748\n","Epoch: 00 [16891/20026 ( 84%)], Train Loss: 0.48735\n","Epoch: 00 [16901/20026 ( 84%)], Train Loss: 0.48733\n","Epoch: 00 [16911/20026 ( 84%)], Train Loss: 0.48712\n","Epoch: 00 [16921/20026 ( 84%)], Train Loss: 0.48737\n","Epoch: 00 [16931/20026 ( 85%)], Train Loss: 0.48723\n","Epoch: 00 [16941/20026 ( 85%)], Train Loss: 0.48704\n","Epoch: 00 [16951/20026 ( 85%)], Train Loss: 0.48703\n","Epoch: 00 [16961/20026 ( 85%)], Train Loss: 0.48707\n","Epoch: 00 [16971/20026 ( 85%)], Train Loss: 0.48706\n","Epoch: 00 [16981/20026 ( 85%)], Train Loss: 0.48685\n","Epoch: 00 [16991/20026 ( 85%)], Train Loss: 0.48666\n","Epoch: 00 [17001/20026 ( 85%)], Train Loss: 0.48668\n","Epoch: 00 [17011/20026 ( 85%)], Train Loss: 0.48657\n","Epoch: 00 [17021/20026 ( 85%)], Train Loss: 0.48646\n","Epoch: 00 [17031/20026 ( 85%)], Train Loss: 0.48653\n","Epoch: 00 [17041/20026 ( 85%)], Train Loss: 0.48644\n","Epoch: 00 [17051/20026 ( 85%)], Train Loss: 0.48625\n","Epoch: 00 [17061/20026 ( 85%)], Train Loss: 0.48630\n","Epoch: 00 [17071/20026 ( 85%)], Train Loss: 0.48629\n","Epoch: 00 [17081/20026 ( 85%)], Train Loss: 0.48620\n","Epoch: 00 [17091/20026 ( 85%)], Train Loss: 0.48611\n","Epoch: 00 [17101/20026 ( 85%)], Train Loss: 0.48600\n","Epoch: 00 [17111/20026 ( 85%)], Train Loss: 0.48586\n","Epoch: 00 [17121/20026 ( 85%)], Train Loss: 0.48579\n","Epoch: 00 [17131/20026 ( 86%)], Train Loss: 0.48568\n","Epoch: 00 [17141/20026 ( 86%)], Train Loss: 0.48558\n","Epoch: 00 [17151/20026 ( 86%)], Train Loss: 0.48539\n","Epoch: 00 [17161/20026 ( 86%)], Train Loss: 0.48538\n","Epoch: 00 [17171/20026 ( 86%)], Train Loss: 0.48536\n","Epoch: 00 [17181/20026 ( 86%)], Train Loss: 0.48523\n","Epoch: 00 [17191/20026 ( 86%)], Train Loss: 0.48507\n","Epoch: 00 [17201/20026 ( 86%)], Train Loss: 0.48492\n","Epoch: 00 [17211/20026 ( 86%)], Train Loss: 0.48504\n","Epoch: 00 [17221/20026 ( 86%)], Train Loss: 0.48511\n","Epoch: 00 [17231/20026 ( 86%)], Train Loss: 0.48493\n","Epoch: 00 [17241/20026 ( 86%)], Train Loss: 0.48474\n","Epoch: 00 [17251/20026 ( 86%)], Train Loss: 0.48475\n","Epoch: 00 [17261/20026 ( 86%)], Train Loss: 0.48456\n","Epoch: 00 [17271/20026 ( 86%)], Train Loss: 0.48456\n","Epoch: 00 [17281/20026 ( 86%)], Train Loss: 0.48474\n","Epoch: 00 [17291/20026 ( 86%)], Train Loss: 0.48458\n","Epoch: 00 [17301/20026 ( 86%)], Train Loss: 0.48457\n","Epoch: 00 [17311/20026 ( 86%)], Train Loss: 0.48435\n","Epoch: 00 [17321/20026 ( 86%)], Train Loss: 0.48449\n","Epoch: 00 [17331/20026 ( 87%)], Train Loss: 0.48451\n","Epoch: 00 [17341/20026 ( 87%)], Train Loss: 0.48429\n","Epoch: 00 [17351/20026 ( 87%)], Train Loss: 0.48406\n","Epoch: 00 [17361/20026 ( 87%)], Train Loss: 0.48401\n","Epoch: 00 [17371/20026 ( 87%)], Train Loss: 0.48391\n","Epoch: 00 [17381/20026 ( 87%)], Train Loss: 0.48398\n","Epoch: 00 [17391/20026 ( 87%)], Train Loss: 0.48387\n","Epoch: 00 [17401/20026 ( 87%)], Train Loss: 0.48374\n","Epoch: 00 [17411/20026 ( 87%)], Train Loss: 0.48374\n","Epoch: 00 [17421/20026 ( 87%)], Train Loss: 0.48364\n","Epoch: 00 [17431/20026 ( 87%)], Train Loss: 0.48361\n","Epoch: 00 [17441/20026 ( 87%)], Train Loss: 0.48354\n","Epoch: 00 [17451/20026 ( 87%)], Train Loss: 0.48332\n","Epoch: 00 [17461/20026 ( 87%)], Train Loss: 0.48327\n","Epoch: 00 [17471/20026 ( 87%)], Train Loss: 0.48316\n","Epoch: 00 [17481/20026 ( 87%)], Train Loss: 0.48337\n","Epoch: 00 [17491/20026 ( 87%)], Train Loss: 0.48329\n","Epoch: 00 [17501/20026 ( 87%)], Train Loss: 0.48331\n","Epoch: 00 [17511/20026 ( 87%)], Train Loss: 0.48321\n","Epoch: 00 [17521/20026 ( 87%)], Train Loss: 0.48305\n","Epoch: 00 [17531/20026 ( 88%)], Train Loss: 0.48287\n","Epoch: 00 [17541/20026 ( 88%)], Train Loss: 0.48284\n","Epoch: 00 [17551/20026 ( 88%)], Train Loss: 0.48270\n","Epoch: 00 [17561/20026 ( 88%)], Train Loss: 0.48268\n","Epoch: 00 [17571/20026 ( 88%)], Train Loss: 0.48252\n","Epoch: 00 [17581/20026 ( 88%)], Train Loss: 0.48240\n","Epoch: 00 [17591/20026 ( 88%)], Train Loss: 0.48218\n","Epoch: 00 [17601/20026 ( 88%)], Train Loss: 0.48213\n","Epoch: 00 [17611/20026 ( 88%)], Train Loss: 0.48202\n","Epoch: 00 [17621/20026 ( 88%)], Train Loss: 0.48193\n","Epoch: 00 [17631/20026 ( 88%)], Train Loss: 0.48208\n","Epoch: 00 [17641/20026 ( 88%)], Train Loss: 0.48183\n","Epoch: 00 [17651/20026 ( 88%)], Train Loss: 0.48188\n","Epoch: 00 [17661/20026 ( 88%)], Train Loss: 0.48184\n","Epoch: 00 [17671/20026 ( 88%)], Train Loss: 0.48169\n","Epoch: 00 [17681/20026 ( 88%)], Train Loss: 0.48149\n","Epoch: 00 [17691/20026 ( 88%)], Train Loss: 0.48131\n","Epoch: 00 [17701/20026 ( 88%)], Train Loss: 0.48143\n","Epoch: 00 [17711/20026 ( 88%)], Train Loss: 0.48146\n","Epoch: 00 [17721/20026 ( 88%)], Train Loss: 0.48135\n","Epoch: 00 [17731/20026 ( 89%)], Train Loss: 0.48128\n","Epoch: 00 [17741/20026 ( 89%)], Train Loss: 0.48121\n","Epoch: 00 [17751/20026 ( 89%)], Train Loss: 0.48126\n","Epoch: 00 [17761/20026 ( 89%)], Train Loss: 0.48118\n","Epoch: 00 [17771/20026 ( 89%)], Train Loss: 0.48098\n","Epoch: 00 [17781/20026 ( 89%)], Train Loss: 0.48113\n","Epoch: 00 [17791/20026 ( 89%)], Train Loss: 0.48091\n","Epoch: 00 [17801/20026 ( 89%)], Train Loss: 0.48078\n","Epoch: 00 [17811/20026 ( 89%)], Train Loss: 0.48063\n","Epoch: 00 [17821/20026 ( 89%)], Train Loss: 0.48079\n","Epoch: 00 [17831/20026 ( 89%)], Train Loss: 0.48069\n","Epoch: 00 [17841/20026 ( 89%)], Train Loss: 0.48046\n","Epoch: 00 [17851/20026 ( 89%)], Train Loss: 0.48045\n","Epoch: 00 [17861/20026 ( 89%)], Train Loss: 0.48042\n","Epoch: 00 [17871/20026 ( 89%)], Train Loss: 0.48022\n","Epoch: 00 [17881/20026 ( 89%)], Train Loss: 0.48013\n","Epoch: 00 [17891/20026 ( 89%)], Train Loss: 0.48015\n","Epoch: 00 [17901/20026 ( 89%)], Train Loss: 0.48004\n","Epoch: 00 [17911/20026 ( 89%)], Train Loss: 0.47985\n","Epoch: 00 [17921/20026 ( 89%)], Train Loss: 0.47976\n","Epoch: 00 [17931/20026 ( 90%)], Train Loss: 0.47978\n","Epoch: 00 [17941/20026 ( 90%)], Train Loss: 0.47999\n","Epoch: 00 [17951/20026 ( 90%)], Train Loss: 0.47982\n","Epoch: 00 [17961/20026 ( 90%)], Train Loss: 0.47992\n","Epoch: 00 [17971/20026 ( 90%)], Train Loss: 0.47989\n","Epoch: 00 [17981/20026 ( 90%)], Train Loss: 0.47999\n","Epoch: 00 [17991/20026 ( 90%)], Train Loss: 0.48005\n","Epoch: 00 [18001/20026 ( 90%)], Train Loss: 0.48020\n","Epoch: 00 [18011/20026 ( 90%)], Train Loss: 0.48005\n","Epoch: 00 [18021/20026 ( 90%)], Train Loss: 0.47995\n","Epoch: 00 [18031/20026 ( 90%)], Train Loss: 0.47978\n","Epoch: 00 [18041/20026 ( 90%)], Train Loss: 0.47984\n","Epoch: 00 [18051/20026 ( 90%)], Train Loss: 0.47977\n","Epoch: 00 [18061/20026 ( 90%)], Train Loss: 0.47962\n","Epoch: 00 [18071/20026 ( 90%)], Train Loss: 0.47955\n","Epoch: 00 [18081/20026 ( 90%)], Train Loss: 0.47941\n","Epoch: 00 [18091/20026 ( 90%)], Train Loss: 0.47956\n","Epoch: 00 [18101/20026 ( 90%)], Train Loss: 0.47956\n","Epoch: 00 [18111/20026 ( 90%)], Train Loss: 0.47936\n","Epoch: 00 [18121/20026 ( 90%)], Train Loss: 0.47947\n","Epoch: 00 [18131/20026 ( 91%)], Train Loss: 0.47935\n","Epoch: 00 [18141/20026 ( 91%)], Train Loss: 0.47911\n","Epoch: 00 [18151/20026 ( 91%)], Train Loss: 0.47900\n","Epoch: 00 [18161/20026 ( 91%)], Train Loss: 0.47889\n","Epoch: 00 [18171/20026 ( 91%)], Train Loss: 0.47879\n","Epoch: 00 [18181/20026 ( 91%)], Train Loss: 0.47871\n","Epoch: 00 [18191/20026 ( 91%)], Train Loss: 0.47852\n","Epoch: 00 [18201/20026 ( 91%)], Train Loss: 0.47840\n","Epoch: 00 [18211/20026 ( 91%)], Train Loss: 0.47823\n","Epoch: 00 [18221/20026 ( 91%)], Train Loss: 0.47811\n","Epoch: 00 [18231/20026 ( 91%)], Train Loss: 0.47811\n","Epoch: 00 [18241/20026 ( 91%)], Train Loss: 0.47787\n","Epoch: 00 [18251/20026 ( 91%)], Train Loss: 0.47786\n","Epoch: 00 [18261/20026 ( 91%)], Train Loss: 0.47776\n","Epoch: 00 [18271/20026 ( 91%)], Train Loss: 0.47766\n","Epoch: 00 [18281/20026 ( 91%)], Train Loss: 0.47741\n","Epoch: 00 [18291/20026 ( 91%)], Train Loss: 0.47733\n","Epoch: 00 [18301/20026 ( 91%)], Train Loss: 0.47711\n","Epoch: 00 [18311/20026 ( 91%)], Train Loss: 0.47694\n","Epoch: 00 [18321/20026 ( 91%)], Train Loss: 0.47688\n","Epoch: 00 [18331/20026 ( 92%)], Train Loss: 0.47678\n","Epoch: 00 [18341/20026 ( 92%)], Train Loss: 0.47667\n","Epoch: 00 [18351/20026 ( 92%)], Train Loss: 0.47653\n","Epoch: 00 [18361/20026 ( 92%)], Train Loss: 0.47661\n","Epoch: 00 [18371/20026 ( 92%)], Train Loss: 0.47659\n","Epoch: 00 [18381/20026 ( 92%)], Train Loss: 0.47655\n","Epoch: 00 [18391/20026 ( 92%)], Train Loss: 0.47653\n","Epoch: 00 [18401/20026 ( 92%)], Train Loss: 0.47638\n","Epoch: 00 [18411/20026 ( 92%)], Train Loss: 0.47616\n","Epoch: 00 [18421/20026 ( 92%)], Train Loss: 0.47600\n","Epoch: 00 [18431/20026 ( 92%)], Train Loss: 0.47597\n","Epoch: 00 [18441/20026 ( 92%)], Train Loss: 0.47593\n","Epoch: 00 [18451/20026 ( 92%)], Train Loss: 0.47571\n","Epoch: 00 [18461/20026 ( 92%)], Train Loss: 0.47549\n","Epoch: 00 [18471/20026 ( 92%)], Train Loss: 0.47533\n","Epoch: 00 [18481/20026 ( 92%)], Train Loss: 0.47512\n","Epoch: 00 [18491/20026 ( 92%)], Train Loss: 0.47523\n","Epoch: 00 [18501/20026 ( 92%)], Train Loss: 0.47502\n","Epoch: 00 [18511/20026 ( 92%)], Train Loss: 0.47494\n","Epoch: 00 [18521/20026 ( 92%)], Train Loss: 0.47483\n","Epoch: 00 [18531/20026 ( 93%)], Train Loss: 0.47486\n","Epoch: 00 [18541/20026 ( 93%)], Train Loss: 0.47477\n","Epoch: 00 [18551/20026 ( 93%)], Train Loss: 0.47470\n","Epoch: 00 [18561/20026 ( 93%)], Train Loss: 0.47501\n","Epoch: 00 [18571/20026 ( 93%)], Train Loss: 0.47494\n","Epoch: 00 [18581/20026 ( 93%)], Train Loss: 0.47474\n","Epoch: 00 [18591/20026 ( 93%)], Train Loss: 0.47470\n","Epoch: 00 [18601/20026 ( 93%)], Train Loss: 0.47480\n","Epoch: 00 [18611/20026 ( 93%)], Train Loss: 0.47459\n","Epoch: 00 [18621/20026 ( 93%)], Train Loss: 0.47463\n","Epoch: 00 [18631/20026 ( 93%)], Train Loss: 0.47460\n","Epoch: 00 [18641/20026 ( 93%)], Train Loss: 0.47441\n","Epoch: 00 [18651/20026 ( 93%)], Train Loss: 0.47416\n","Epoch: 00 [18661/20026 ( 93%)], Train Loss: 0.47428\n","Epoch: 00 [18671/20026 ( 93%)], Train Loss: 0.47409\n","Epoch: 00 [18681/20026 ( 93%)], Train Loss: 0.47390\n","Epoch: 00 [18691/20026 ( 93%)], Train Loss: 0.47385\n","Epoch: 00 [18701/20026 ( 93%)], Train Loss: 0.47366\n","Epoch: 00 [18711/20026 ( 93%)], Train Loss: 0.47361\n","Epoch: 00 [18721/20026 ( 93%)], Train Loss: 0.47351\n","Epoch: 00 [18731/20026 ( 94%)], Train Loss: 0.47349\n","Epoch: 00 [18741/20026 ( 94%)], Train Loss: 0.47341\n","Epoch: 00 [18751/20026 ( 94%)], Train Loss: 0.47332\n","Epoch: 00 [18761/20026 ( 94%)], Train Loss: 0.47329\n","Epoch: 00 [18771/20026 ( 94%)], Train Loss: 0.47320\n","Epoch: 00 [18781/20026 ( 94%)], Train Loss: 0.47305\n","Epoch: 00 [18791/20026 ( 94%)], Train Loss: 0.47300\n","Epoch: 00 [18801/20026 ( 94%)], Train Loss: 0.47303\n","Epoch: 00 [18811/20026 ( 94%)], Train Loss: 0.47290\n","Epoch: 00 [18821/20026 ( 94%)], Train Loss: 0.47301\n","Epoch: 00 [18831/20026 ( 94%)], Train Loss: 0.47283\n","Epoch: 00 [18841/20026 ( 94%)], Train Loss: 0.47275\n","Epoch: 00 [18851/20026 ( 94%)], Train Loss: 0.47258\n","Epoch: 00 [18861/20026 ( 94%)], Train Loss: 0.47254\n","Epoch: 00 [18871/20026 ( 94%)], Train Loss: 0.47253\n","Epoch: 00 [18881/20026 ( 94%)], Train Loss: 0.47286\n","Epoch: 00 [18891/20026 ( 94%)], Train Loss: 0.47310\n","Epoch: 00 [18901/20026 ( 94%)], Train Loss: 0.47292\n","Epoch: 00 [18911/20026 ( 94%)], Train Loss: 0.47302\n","Epoch: 00 [18921/20026 ( 94%)], Train Loss: 0.47285\n","Epoch: 00 [18931/20026 ( 95%)], Train Loss: 0.47275\n","Epoch: 00 [18941/20026 ( 95%)], Train Loss: 0.47266\n","Epoch: 00 [18951/20026 ( 95%)], Train Loss: 0.47255\n","Epoch: 00 [18961/20026 ( 95%)], Train Loss: 0.47239\n","Epoch: 00 [18971/20026 ( 95%)], Train Loss: 0.47234\n","Epoch: 00 [18981/20026 ( 95%)], Train Loss: 0.47221\n","Epoch: 00 [18991/20026 ( 95%)], Train Loss: 0.47208\n","Epoch: 00 [19001/20026 ( 95%)], Train Loss: 0.47201\n","Epoch: 00 [19011/20026 ( 95%)], Train Loss: 0.47187\n","Epoch: 00 [19021/20026 ( 95%)], Train Loss: 0.47168\n","Epoch: 00 [19031/20026 ( 95%)], Train Loss: 0.47145\n","Epoch: 00 [19041/20026 ( 95%)], Train Loss: 0.47149\n","Epoch: 00 [19051/20026 ( 95%)], Train Loss: 0.47142\n","Epoch: 00 [19061/20026 ( 95%)], Train Loss: 0.47166\n","Epoch: 00 [19071/20026 ( 95%)], Train Loss: 0.47176\n","Epoch: 00 [19081/20026 ( 95%)], Train Loss: 0.47155\n","Epoch: 00 [19091/20026 ( 95%)], Train Loss: 0.47141\n","Epoch: 00 [19101/20026 ( 95%)], Train Loss: 0.47142\n","Epoch: 00 [19111/20026 ( 95%)], Train Loss: 0.47152\n","Epoch: 00 [19121/20026 ( 95%)], Train Loss: 0.47138\n","Epoch: 00 [19131/20026 ( 96%)], Train Loss: 0.47130\n","Epoch: 00 [19141/20026 ( 96%)], Train Loss: 0.47117\n","Epoch: 00 [19151/20026 ( 96%)], Train Loss: 0.47111\n","Epoch: 00 [19161/20026 ( 96%)], Train Loss: 0.47115\n","Epoch: 00 [19171/20026 ( 96%)], Train Loss: 0.47119\n","Epoch: 00 [19181/20026 ( 96%)], Train Loss: 0.47105\n","Epoch: 00 [19191/20026 ( 96%)], Train Loss: 0.47101\n","Epoch: 00 [19201/20026 ( 96%)], Train Loss: 0.47085\n","Epoch: 00 [19211/20026 ( 96%)], Train Loss: 0.47062\n","Epoch: 00 [19221/20026 ( 96%)], Train Loss: 0.47051\n","Epoch: 00 [19231/20026 ( 96%)], Train Loss: 0.47050\n","Epoch: 00 [19241/20026 ( 96%)], Train Loss: 0.47034\n","Epoch: 00 [19251/20026 ( 96%)], Train Loss: 0.47018\n","Epoch: 00 [19261/20026 ( 96%)], Train Loss: 0.47012\n","Epoch: 00 [19271/20026 ( 96%)], Train Loss: 0.46995\n","Epoch: 00 [19281/20026 ( 96%)], Train Loss: 0.46979\n","Epoch: 00 [19291/20026 ( 96%)], Train Loss: 0.46970\n","Epoch: 00 [19301/20026 ( 96%)], Train Loss: 0.46955\n","Epoch: 00 [19311/20026 ( 96%)], Train Loss: 0.46948\n","Epoch: 00 [19321/20026 ( 96%)], Train Loss: 0.46933\n","Epoch: 00 [19331/20026 ( 97%)], Train Loss: 0.46912\n","Epoch: 00 [19341/20026 ( 97%)], Train Loss: 0.46920\n","Epoch: 00 [19351/20026 ( 97%)], Train Loss: 0.46909\n","Epoch: 00 [19361/20026 ( 97%)], Train Loss: 0.46926\n","Epoch: 00 [19371/20026 ( 97%)], Train Loss: 0.46916\n","Epoch: 00 [19381/20026 ( 97%)], Train Loss: 0.46911\n","Epoch: 00 [19391/20026 ( 97%)], Train Loss: 0.46939\n","Epoch: 00 [19401/20026 ( 97%)], Train Loss: 0.46926\n","Epoch: 00 [19411/20026 ( 97%)], Train Loss: 0.46944\n","Epoch: 00 [19421/20026 ( 97%)], Train Loss: 0.46932\n","Epoch: 00 [19431/20026 ( 97%)], Train Loss: 0.46952\n","Epoch: 00 [19441/20026 ( 97%)], Train Loss: 0.46944\n","Epoch: 00 [19451/20026 ( 97%)], Train Loss: 0.46929\n","Epoch: 00 [19461/20026 ( 97%)], Train Loss: 0.46930\n","Epoch: 00 [19471/20026 ( 97%)], Train Loss: 0.46914\n","Epoch: 00 [19481/20026 ( 97%)], Train Loss: 0.46922\n","Epoch: 00 [19491/20026 ( 97%)], Train Loss: 0.46950\n","Epoch: 00 [19501/20026 ( 97%)], Train Loss: 0.46927\n","Epoch: 00 [19511/20026 ( 97%)], Train Loss: 0.46917\n","Epoch: 00 [19521/20026 ( 97%)], Train Loss: 0.46918\n","Epoch: 00 [19531/20026 ( 98%)], Train Loss: 0.46896\n","Epoch: 00 [19541/20026 ( 98%)], Train Loss: 0.46881\n","Epoch: 00 [19551/20026 ( 98%)], Train Loss: 0.46870\n","Epoch: 00 [19561/20026 ( 98%)], Train Loss: 0.46866\n","Epoch: 00 [19571/20026 ( 98%)], Train Loss: 0.46865\n","Epoch: 00 [19581/20026 ( 98%)], Train Loss: 0.46856\n","Epoch: 00 [19591/20026 ( 98%)], Train Loss: 0.46835\n","Epoch: 00 [19601/20026 ( 98%)], Train Loss: 0.46820\n","Epoch: 00 [19611/20026 ( 98%)], Train Loss: 0.46803\n","Epoch: 00 [19621/20026 ( 98%)], Train Loss: 0.46803\n","Epoch: 00 [19631/20026 ( 98%)], Train Loss: 0.46804\n","Epoch: 00 [19641/20026 ( 98%)], Train Loss: 0.46784\n","Epoch: 00 [19651/20026 ( 98%)], Train Loss: 0.46762\n","Epoch: 00 [19661/20026 ( 98%)], Train Loss: 0.46752\n","Epoch: 00 [19671/20026 ( 98%)], Train Loss: 0.46745\n","Epoch: 00 [19681/20026 ( 98%)], Train Loss: 0.46728\n","Epoch: 00 [19691/20026 ( 98%)], Train Loss: 0.46736\n","Epoch: 00 [19701/20026 ( 98%)], Train Loss: 0.46722\n","Epoch: 00 [19711/20026 ( 98%)], Train Loss: 0.46738\n","Epoch: 00 [19721/20026 ( 98%)], Train Loss: 0.46719\n","Epoch: 00 [19731/20026 ( 99%)], Train Loss: 0.46700\n","Epoch: 00 [19741/20026 ( 99%)], Train Loss: 0.46707\n","Epoch: 00 [19751/20026 ( 99%)], Train Loss: 0.46733\n","Epoch: 00 [19761/20026 ( 99%)], Train Loss: 0.46714\n","Epoch: 00 [19771/20026 ( 99%)], Train Loss: 0.46705\n","Epoch: 00 [19781/20026 ( 99%)], Train Loss: 0.46700\n","Epoch: 00 [19791/20026 ( 99%)], Train Loss: 0.46690\n","Epoch: 00 [19801/20026 ( 99%)], Train Loss: 0.46685\n","Epoch: 00 [19811/20026 ( 99%)], Train Loss: 0.46670\n","Epoch: 00 [19821/20026 ( 99%)], Train Loss: 0.46667\n","Epoch: 00 [19831/20026 ( 99%)], Train Loss: 0.46648\n","Epoch: 00 [19841/20026 ( 99%)], Train Loss: 0.46626\n","Epoch: 00 [19851/20026 ( 99%)], Train Loss: 0.46636\n","Epoch: 00 [19861/20026 ( 99%)], Train Loss: 0.46620\n","Epoch: 00 [19871/20026 ( 99%)], Train Loss: 0.46600\n","Epoch: 00 [19881/20026 ( 99%)], Train Loss: 0.46582\n","Epoch: 00 [19891/20026 ( 99%)], Train Loss: 0.46563\n","Epoch: 00 [19901/20026 ( 99%)], Train Loss: 0.46544\n","Epoch: 00 [19911/20026 ( 99%)], Train Loss: 0.46533\n","Epoch: 00 [19921/20026 ( 99%)], Train Loss: 0.46553\n","Epoch: 00 [19931/20026 (100%)], Train Loss: 0.46538\n","Epoch: 00 [19941/20026 (100%)], Train Loss: 0.46520\n","Epoch: 00 [19951/20026 (100%)], Train Loss: 0.46500\n","Epoch: 00 [19961/20026 (100%)], Train Loss: 0.46491\n","Epoch: 00 [19971/20026 (100%)], Train Loss: 0.46481\n","Epoch: 00 [19981/20026 (100%)], Train Loss: 0.46480\n","Epoch: 00 [19991/20026 (100%)], Train Loss: 0.46464\n","Epoch: 00 [20001/20026 (100%)], Train Loss: 0.46469\n","Epoch: 00 [20011/20026 (100%)], Train Loss: 0.46453\n","Epoch: 00 [20021/20026 (100%)], Train Loss: 0.46436\n","Epoch: 00 [20026/20026 (100%)], Train Loss: 0.46426\n","----Validation Results Summary----\n","Epoch: [0] Valid Loss: 0.20288\n","0 Epoch, Best epoch was updated! Valid Loss: 0.20288\n","Saving model checkpoint to output/checkpoint-fold-0.\n","\n","Total Training Time: 14044.199845552444secs, Average Training Time per Epoch: 14044.199845552444secs.\n","Total Validation Time: 591.2403721809387secs, Average Validation Time per Epoch: 591.2403721809387secs.\n"]}]},{"cell_type":"code","metadata":{"id":"o6-F5rzbHSK5"},"source":["! cp -r /content/output/checkpoint-fold-0 /content/drive/Shareddrives/NLP/Dataset/murli"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LdC5X_O7HFVz"},"source":[""],"execution_count":null,"outputs":[]}]}
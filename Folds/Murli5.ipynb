{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Murli5.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l_gRgN5qTmKM","executionInfo":{"status":"ok","timestamp":1635297194656,"user_tz":-330,"elapsed":27210,"user":{"displayName":"2020 11004","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06803819529238251334"}},"outputId":"0b1550ff-202a-4450-83b0-cb28a865f10c"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"faIyP9zZEd0G","executionInfo":{"status":"ok","timestamp":1635297203499,"user_tz":-330,"elapsed":8853,"user":{"displayName":"2020 11004","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06803819529238251334"}},"outputId":"081bdaf9-971a-4d20-a1d0-148c95395dc2"},"source":["! pip install transformers sentencepiece "],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 5.3 MB/s \n","\u001b[?25hCollecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 39.2 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 42.5 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 46.4 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting huggingface-hub>=0.0.17\n","  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n","\u001b[K     |████████████████████████████████| 56 kB 4.2 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 43.8 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.19 pyyaml-6.0 sacremoses-0.0.46 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.11.3\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jDu4BWB1EJJ3","executionInfo":{"status":"ok","timestamp":1635297231657,"user_tz":-330,"elapsed":28168,"user":{"displayName":"2020 11004","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06803819529238251334"}},"outputId":"83aa89b5-21c1-49b4-b53a-d80c0bb55d7f"},"source":["import os\n","import gc\n","gc.enable()\n","import math\n","import json\n","import time\n","import random\n","import multiprocessing\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm, trange\n","from sklearn import model_selection\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import Parameter\n","import torch.optim as optim\n","from torch.utils.data import (\n","    Dataset, DataLoader,\n","    SequentialSampler, RandomSampler\n",")\n","from torch.utils.data.distributed import DistributedSampler\n","\n","try:\n","    from apex import amp\n","    APEX_INSTALLED = True\n","except ImportError:\n","    APEX_INSTALLED = False\n","\n","import transformers\n","from transformers import (\n","    WEIGHTS_NAME,\n","    AdamW,\n","    AutoConfig,\n","    AutoModel,\n","    AutoTokenizer,\n","    get_cosine_schedule_with_warmup,\n","    get_linear_schedule_with_warmup,\n","    logging,\n","    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n",")\n","logging.set_verbosity_warning()\n","logging.set_verbosity_error()\n","\n","def fix_all_seeds(seed):\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","def optimal_num_of_loader_workers():\n","    num_cpus = multiprocessing.cpu_count()\n","    num_gpus = torch.cuda.device_count()\n","    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n","    return optimal_value\n","\n","print(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\n","MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n","MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Apex AMP Installed :: False\n"]}]},{"cell_type":"code","metadata":{"id":"t8HfXz6SHhE7","executionInfo":{"status":"ok","timestamp":1635297231660,"user_tz":-330,"elapsed":20,"user":{"displayName":"2020 11004","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06803819529238251334"}}},"source":["class Config:\n","    # model\n","    model_type = 'bert'\n","    model_name_or_path = \"/content/drive/Shareddrives/NLP/Tarang/xlm-roberta-large-squad2/model\"\n","    config_name = \"/content/drive/Shareddrives/NLP/Tarang/xlm-roberta-large-squad2/model\"\n","    fp16 = True if APEX_INSTALLED else False\n","    fp16_opt_level = \"O1\"\n","    gradient_accumulation_steps = 2\n","\n","    # tokenizer\n","    tokenizer_name = \"/content/drive/Shareddrives/NLP/Tarang/xlm-roberta-large-squad2/tok\"\n","    max_seq_length = 384\n","    doc_stride = 128\n","\n","    # train\n","    epochs = 1\n","    train_batch_size = 1\n","    eval_batch_size = 1\n","\n","    # optimizer\n","    optimizer_type = 'AdamW'\n","    learning_rate = 1.5e-5\n","    weight_decay = 1e-2\n","    epsilon = 1e-8\n","    max_grad_norm = 1.0\n","\n","    # scheduler\n","    decay_name = 'linear-warmup'\n","    warmup_ratio = 0.1\n","\n","    # logging\n","    logging_steps = 10\n","\n","    # evaluate\n","    output_dir = 'output'\n","    seed = 42"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jzv3krJSETEs","executionInfo":{"status":"ok","timestamp":1635297235435,"user_tz":-330,"elapsed":3080,"user":{"displayName":"2020 11004","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06803819529238251334"}}},"source":["train = pd.read_csv('/content/drive/Shareddrives/NLP/Dataset/train.csv')\n","test = pd.read_csv('/content/drive/Shareddrives/NLP/Dataset/test.csv')\n","external_mlqa = pd.read_csv('/content/drive/Shareddrives/NLP/Dataset/mlqa_hindi.csv')\n","external_xquad = pd.read_csv('/content/drive/Shareddrives/NLP/Dataset/xquad.csv')\n","external_train = pd.concat([external_mlqa, external_xquad])"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"wNv3o7qeFmaB","executionInfo":{"status":"ok","timestamp":1635297235437,"user_tz":-330,"elapsed":17,"user":{"displayName":"2020 11004","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06803819529238251334"}}},"source":["def create_folds(data, num_splits):\n","    data[\"kfold\"] = -1\n","    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=69)\n","    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n","        data.loc[v_, 'kfold'] = f\n","    return data\n","\n","train = create_folds(train, num_splits=5)\n","external_train[\"kfold\"] = -1\n","external_train['id'] = list(np.arange(1, len(external_train)+1))\n","train = pd.concat([train, external_train]).reset_index(drop=True)\n","\n","def convert_answers(row):\n","    return {'answer_start': [row[0]], 'text': [row[1]]}\n","\n","train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"dVmPKiJZGKGc","executionInfo":{"status":"ok","timestamp":1635297235438,"user_tz":-330,"elapsed":15,"user":{"displayName":"2020 11004","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06803819529238251334"}}},"source":["def prepare_train_features(args, example, tokenizer):\n","    example[\"question\"] = example[\"question\"].lstrip()\n","    tokenized_example = tokenizer(\n","        example[\"question\"],\n","        example[\"context\"],\n","        truncation=\"only_second\",\n","        max_length=args.max_seq_length,\n","        stride=args.doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n","    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n","\n","    features = []\n","    for i, offsets in enumerate(offset_mapping):\n","        feature = {}\n","\n","        input_ids = tokenized_example[\"input_ids\"][i]\n","        attention_mask = tokenized_example[\"attention_mask\"][i]\n","\n","        feature['input_ids'] = input_ids\n","        feature['attention_mask'] = attention_mask\n","        feature['offset_mapping'] = offsets\n","\n","        cls_index = input_ids.index(tokenizer.cls_token_id)\n","        sequence_ids = tokenized_example.sequence_ids(i)\n","\n","        sample_index = sample_mapping[i]\n","        answers = example[\"answers\"]\n","\n","        if len(answers[\"answer_start\"]) == 0:\n","            feature[\"start_position\"] = cls_index\n","            feature[\"end_position\"] = cls_index\n","        else:\n","            start_char = answers[\"answer_start\"][0]\n","            end_char = start_char + len(answers[\"text\"][0])\n","\n","            token_start_index = 0\n","            while sequence_ids[token_start_index] != 1:\n","                token_start_index += 1\n","\n","            token_end_index = len(input_ids) - 1\n","            while sequence_ids[token_end_index] != 1:\n","                token_end_index -= 1\n","\n","            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n","                feature[\"start_position\"] = cls_index\n","                feature[\"end_position\"] = cls_index\n","            else:\n","                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n","                    token_start_index += 1\n","                feature[\"start_position\"] = token_start_index - 1\n","                while offsets[token_end_index][1] >= end_char:\n","                    token_end_index -= 1\n","                feature[\"end_position\"] = token_end_index + 1\n","\n","        features.append(feature)\n","    return features"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"1X5iXyuyGRuH","executionInfo":{"status":"ok","timestamp":1635297235440,"user_tz":-330,"elapsed":14,"user":{"displayName":"2020 11004","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06803819529238251334"}}},"source":["class DatasetRetriever(Dataset):\n","    def __init__(self, features, mode='train'):\n","        super(DatasetRetriever, self).__init__()\n","        self.features = features\n","        self.mode = mode\n","        \n","    def __len__(self):\n","        return len(self.features)\n","    \n","    def __getitem__(self, item):   \n","        feature = self.features[item]\n","        if self.mode == 'train':\n","            return {\n","                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n","                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n","                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n","                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n","                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n","            }\n","        else:\n","            return {\n","                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n","                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n","                'offset_mapping':feature['offset_mapping'],\n","                'sequence_ids':feature['sequence_ids'],\n","                'id':feature['example_id'],\n","                'context': feature['context'],\n","                'question': feature['question']\n","            }"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"c83EghN4GXie","executionInfo":{"status":"ok","timestamp":1635297236088,"user_tz":-330,"elapsed":13,"user":{"displayName":"2020 11004","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06803819529238251334"}}},"source":["class Model(nn.Module):\n","    def __init__(self, modelname_or_path, config):\n","        super(Model, self).__init__()\n","        self.config = config\n","        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n","        self.linear_layer = nn.Linear(config.hidden_size, 64)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.qa_outputs = nn.Linear(64, 2)\n","        self._init_weights(self.qa_outputs)\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","\n","    def forward(\n","        self, \n","        input_ids, \n","        attention_mask=None, \n","    ):\n","        outputs = self.xlm_roberta(input_ids,attention_mask=attention_mask)\n","\n","        sequence_output = outputs[0]\n","        pooled_output = outputs[1]\n","        \n","        linear_output = self.linear_layer(sequence_output)\n","        linear_output = self.dropout(linear_output)\n","        qa_logits = self.qa_outputs(linear_output)\n","        \n","        start_logits, end_logits = qa_logits.split(1, dim=-1)\n","        start_logits = start_logits.squeeze(-1)\n","        end_logits = end_logits.squeeze(-1)\n","    \n","        return start_logits, end_logits"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"gKn4pHnvGc4H","executionInfo":{"status":"ok","timestamp":1635297236089,"user_tz":-330,"elapsed":13,"user":{"displayName":"2020 11004","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06803819529238251334"}}},"source":["def loss_fn(preds, labels):\n","    start_preds, end_preds = preds\n","    start_labels, end_labels = labels\n","    \n","    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n","    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n","    total_loss = (start_loss + end_loss) / 2\n","    return total_loss"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"qoV8_pn6GgdW","executionInfo":{"status":"ok","timestamp":1635297236089,"user_tz":-330,"elapsed":12,"user":{"displayName":"2020 11004","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06803819529238251334"}}},"source":["def get_optimizer_grouped_parameters(args, model):\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n","    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n","    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n","    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': args.weight_decay},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': args.weight_decay, 'lr': args.learning_rate/2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': args.weight_decay, 'lr': args.learning_rate},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': args.weight_decay, 'lr': args.learning_rate*2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': args.learning_rate/2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': args.learning_rate},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': args.learning_rate*2.6},\n","        {'params': [p for n, p in model.named_parameters() if args.model_type not in n], 'lr':args.learning_rate*20, \"weight_decay\": 0.0},\n","    ]\n","    return optimizer_grouped_parameters"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"xp_uEUijGrCv","executionInfo":{"status":"ok","timestamp":1635297236090,"user_tz":-330,"elapsed":13,"user":{"displayName":"2020 11004","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06803819529238251334"}}},"source":["class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","        self.max = 0\n","        self.min = 1e5\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","        if val > self.max:\n","            self.max = val\n","        if val < self.min:\n","            self.min = val"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"RKcK3xENGvQ4","executionInfo":{"status":"ok","timestamp":1635297236090,"user_tz":-330,"elapsed":12,"user":{"displayName":"2020 11004","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06803819529238251334"}}},"source":["def make_model(args):\n","    config = AutoConfig.from_pretrained(args.config_name)\n","    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n","    model = Model(args.model_name_or_path, config=config)\n","    return config, tokenizer, model\n","\n","def make_optimizer(args, model):\n","    optimizer_grouped_parameters = get_optimizer_grouped_parameters(args, model)\n","#     no_decay = [\"bias\", \"LayerNorm.weight\"]\n","#     optimizer_grouped_parameters = [\n","#         {\n","#             \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","#             \"weight_decay\": args.weight_decay,\n","#         },\n","#         {\n","#             \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","#             \"weight_decay\": 0.0,\n","#         },\n","#     ]\n","    if args.optimizer_type == \"AdamW\":\n","        optimizer = AdamW(\n","            optimizer_grouped_parameters,\n","            lr=args.learning_rate,\n","            eps=args.epsilon,\n","            correct_bias=True\n","        )\n","        return optimizer\n","\n","def make_scheduler(\n","    args, optimizer, \n","    num_warmup_steps, \n","    num_training_steps\n","):\n","    if args.decay_name == \"cosine-warmup\":\n","        scheduler = get_cosine_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps\n","        )\n","    else:\n","        scheduler = get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps\n","        )\n","    return scheduler    \n","\n","def make_loader(\n","    args, data, \n","    tokenizer, fold\n","):\n","    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n","    \n","    train_features, valid_features = [[] for _ in range(2)]\n","    for i, row in train_set.iterrows():\n","        train_features += prepare_train_features(args, row, tokenizer)\n","    for i, row in valid_set.iterrows():\n","        valid_features += prepare_train_features(args, row, tokenizer)\n","\n","    train_dataset = DatasetRetriever(train_features)\n","    valid_dataset = DatasetRetriever(valid_features)\n","    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n","    \n","    train_sampler = RandomSampler(train_dataset)\n","    valid_sampler = SequentialSampler(valid_dataset)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=args.train_batch_size,\n","        sampler=train_sampler,\n","        num_workers=optimal_num_of_loader_workers(),\n","        pin_memory=True,\n","        drop_last=False \n","    )\n","\n","    valid_dataloader = DataLoader(\n","        valid_dataset,\n","        batch_size=args.eval_batch_size, \n","        sampler=valid_sampler,\n","        num_workers=optimal_num_of_loader_workers(),\n","        pin_memory=True, \n","        drop_last=False\n","    )\n","\n","    return train_dataloader, valid_dataloader"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"YYXpJ2UaG4ft","executionInfo":{"status":"ok","timestamp":1635297236090,"user_tz":-330,"elapsed":11,"user":{"displayName":"2020 11004","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06803819529238251334"}}},"source":["class Trainer:\n","    def __init__(\n","        self, model, tokenizer, \n","        optimizer, scheduler\n","    ):\n","        self.model = model\n","        self.tokenizer = tokenizer\n","        self.optimizer = optimizer\n","        self.scheduler = scheduler\n","\n","    def train(\n","        self, args, \n","        train_dataloader, \n","        epoch, result_dict\n","    ):\n","        count = 0\n","        losses = AverageMeter()\n","        \n","        self.model.zero_grad()\n","        self.model.train()\n","        \n","        fix_all_seeds(args.seed)\n","        \n","        for batch_idx, batch_data in enumerate(train_dataloader):\n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                batch_data['input_ids'], batch_data['attention_mask'], \\\n","                    batch_data['start_position'], batch_data['end_position']\n","            \n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n","\n","            outputs_start, outputs_end = self.model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","            )\n","            \n","            loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n","            loss = loss / args.gradient_accumulation_steps\n","\n","            if args.fp16:\n","                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","            else:\n","                loss.backward()\n","\n","            count += input_ids.size(0)\n","            losses.update(loss.item(), input_ids.size(0))\n","\n","            # if args.fp16:\n","            #     torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), args.max_grad_norm)\n","            # else:\n","            #     torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.max_grad_norm)\n","\n","            if batch_idx % args.gradient_accumulation_steps == 0 or batch_idx == len(train_dataloader) - 1:\n","                self.optimizer.step()\n","                self.scheduler.step()\n","                self.optimizer.zero_grad()\n","\n","            if (batch_idx % args.logging_steps == 0) or (batch_idx+1)==len(train_dataloader):\n","                _s = str(len(str(len(train_dataloader.sampler))))\n","                ret = [\n","                    ('Epoch: {:0>2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_dataloader.sampler), 100 * count / len(train_dataloader.sampler)),\n","                    'Train Loss: {: >4.5f}'.format(losses.avg),\n","                ]\n","                print(', '.join(ret))\n","\n","        result_dict['train_loss'].append(losses.avg)\n","        return result_dict"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"c7lo_72cG88t","executionInfo":{"status":"ok","timestamp":1635297236091,"user_tz":-330,"elapsed":12,"user":{"displayName":"2020 11004","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06803819529238251334"}}},"source":["class Evaluator:\n","    def __init__(self, model):\n","        self.model = model\n","    \n","    def save(self, result, output_dir):\n","        with open(f'{output_dir}/result_dict.json', 'w') as f:\n","            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n","\n","    def evaluate(self, valid_dataloader, epoch, result_dict):\n","        losses = AverageMeter()\n","        for batch_idx, batch_data in enumerate(valid_dataloader):\n","            self.model = self.model.eval()\n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                batch_data['input_ids'], batch_data['attention_mask'], \\\n","                    batch_data['start_position'], batch_data['end_position']\n","            \n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n","            \n","            with torch.no_grad():            \n","                outputs_start, outputs_end = self.model(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                )\n","                \n","                loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n","                losses.update(loss.item(), input_ids.size(0))\n","                \n","        print('----Validation Results Summary----')\n","        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n","        result_dict['val_loss'].append(losses.avg)        \n","        return result_dict"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wv14GLumHBTO","executionInfo":{"status":"ok","timestamp":1635297236091,"user_tz":-330,"elapsed":11,"user":{"displayName":"2020 11004","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06803819529238251334"}}},"source":["def init_training(args, data, fold):\n","    fix_all_seeds(args.seed)\n","    \n","    if not os.path.exists(args.output_dir):\n","        os.makedirs(args.output_dir)\n","    \n","    # model\n","    model_config, tokenizer, model = make_model(args)\n","    if torch.cuda.device_count() >= 1:\n","        print('Model pushed to {} GPU(s), type {}.'.format(\n","            torch.cuda.device_count(), \n","            torch.cuda.get_device_name(0))\n","        )\n","        model = model.cuda() \n","    else:\n","        raise ValueError('CPU training is not supported')\n","    \n","    # data loaders\n","    train_dataloader, valid_dataloader = make_loader(args, data, tokenizer, fold)\n","\n","    # optimizer\n","    optimizer = make_optimizer(args, model)\n","\n","    # scheduler\n","    num_training_steps = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps) * args.epochs\n","    if args.warmup_ratio > 0:\n","        num_warmup_steps = int(args.warmup_ratio * num_training_steps)\n","    else:\n","        num_warmup_steps = 0\n","    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n","    scheduler = make_scheduler(args, optimizer, num_warmup_steps, num_training_steps)\n","\n","    # mixed precision training with NVIDIA Apex\n","    if args.fp16:\n","        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n","    \n","    result_dict = {\n","        'epoch':[], \n","        'train_loss': [], \n","        'val_loss' : [], \n","        'best_val_loss': np.inf\n","    }\n","\n","    return (\n","        model, model_config, tokenizer, optimizer, scheduler, \n","        train_dataloader, valid_dataloader, result_dict\n","    )"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"XUjSnwFpHKAi","executionInfo":{"status":"ok","timestamp":1635297236092,"user_tz":-330,"elapsed":11,"user":{"displayName":"2020 11004","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06803819529238251334"}}},"source":["def run(data, fold):\n","    args = Config()\n","    model, model_config, tokenizer, optimizer, scheduler, train_dataloader, \\\n","        valid_dataloader, result_dict = init_training(args, data, fold)\n","    \n","    trainer = Trainer(model, tokenizer, optimizer, scheduler)\n","    evaluator = Evaluator(model)\n","\n","    train_time_list = []\n","    valid_time_list = []\n","\n","    for epoch in range(args.epochs):\n","        result_dict['epoch'].append(epoch)\n","\n","        # Train\n","        torch.cuda.synchronize()\n","        tic1 = time.time()\n","        result_dict = trainer.train(\n","            args, train_dataloader, \n","            epoch, result_dict\n","        )\n","        torch.cuda.synchronize()\n","        tic2 = time.time() \n","        train_time_list.append(tic2 - tic1)\n","        \n","        # Evaluate\n","        torch.cuda.synchronize()\n","        tic3 = time.time()\n","        result_dict = evaluator.evaluate(\n","            valid_dataloader, epoch, result_dict\n","        )\n","        torch.cuda.synchronize()\n","        tic4 = time.time() \n","        valid_time_list.append(tic4 - tic3)\n","            \n","        output_dir = os.path.join(args.output_dir, f\"checkpoint-fold-{fold}\")\n","        if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n","            print(\"{} Epoch, Best epoch was updated! Valid Loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n","            result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]        \n","            \n","            os.makedirs(output_dir, exist_ok=True)\n","            torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n","            model_config.save_pretrained(output_dir)\n","            tokenizer.save_pretrained(output_dir)\n","            print(f\"Saving model checkpoint to {output_dir}.\")\n","            \n","        print()\n","\n","    evaluator.save(result_dict, output_dir)\n","    \n","    print(f\"Total Training Time: {np.sum(train_time_list)}secs, Average Training Time per Epoch: {np.mean(train_time_list)}secs.\")\n","    print(f\"Total Validation Time: {np.sum(valid_time_list)}secs, Average Validation Time per Epoch: {np.mean(valid_time_list)}secs.\")\n","    \n","    torch.cuda.empty_cache()\n","    del trainer, evaluator\n","    del model, model_config, tokenizer\n","    del optimizer, scheduler\n","    del train_dataloader, valid_dataloader, result_dict\n","    gc.collect()\n"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bduls9AyHO8p","executionInfo":{"status":"ok","timestamp":1635311349376,"user_tz":-330,"elapsed":14113294,"user":{"displayName":"2020 11004","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06803819529238251334"}},"outputId":"06478e01-91e7-4dfa-ac79-a87935564e0d"},"source":["for fold in range(4,5):\n","    print();print()\n","    print('-'*50)\n","    print(f'FOLD: {fold}')\n","    print('-'*50)\n","    run(train, fold)"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","--------------------------------------------------\n","FOLD: 4\n","--------------------------------------------------\n","Model pushed to 1 GPU(s), type Tesla K80.\n","Num examples Train= 20164, Num examples Valid=2905\n","Total Training Steps: 10082, Total Warmup Steps: 1008\n","Epoch: 00 [    1/20164 (  0%)], Train Loss: 2.99914\n","Epoch: 00 [   11/20164 (  0%)], Train Loss: 2.96824\n","Epoch: 00 [   21/20164 (  0%)], Train Loss: 2.96249\n","Epoch: 00 [   31/20164 (  0%)], Train Loss: 2.96202\n","Epoch: 00 [   41/20164 (  0%)], Train Loss: 2.96235\n","Epoch: 00 [   51/20164 (  0%)], Train Loss: 2.95695\n","Epoch: 00 [   61/20164 (  0%)], Train Loss: 2.95150\n","Epoch: 00 [   71/20164 (  0%)], Train Loss: 2.94256\n","Epoch: 00 [   81/20164 (  0%)], Train Loss: 2.93467\n","Epoch: 00 [   91/20164 (  0%)], Train Loss: 2.92718\n","Epoch: 00 [  101/20164 (  1%)], Train Loss: 2.91737\n","Epoch: 00 [  111/20164 (  1%)], Train Loss: 2.90667\n","Epoch: 00 [  121/20164 (  1%)], Train Loss: 2.89622\n","Epoch: 00 [  131/20164 (  1%)], Train Loss: 2.88324\n","Epoch: 00 [  141/20164 (  1%)], Train Loss: 2.86562\n","Epoch: 00 [  151/20164 (  1%)], Train Loss: 2.85356\n","Epoch: 00 [  161/20164 (  1%)], Train Loss: 2.83255\n","Epoch: 00 [  171/20164 (  1%)], Train Loss: 2.81081\n","Epoch: 00 [  181/20164 (  1%)], Train Loss: 2.78737\n","Epoch: 00 [  191/20164 (  1%)], Train Loss: 2.76622\n","Epoch: 00 [  201/20164 (  1%)], Train Loss: 2.74179\n","Epoch: 00 [  211/20164 (  1%)], Train Loss: 2.71498\n","Epoch: 00 [  221/20164 (  1%)], Train Loss: 2.68675\n","Epoch: 00 [  231/20164 (  1%)], Train Loss: 2.65560\n","Epoch: 00 [  241/20164 (  1%)], Train Loss: 2.61647\n","Epoch: 00 [  251/20164 (  1%)], Train Loss: 2.58783\n","Epoch: 00 [  261/20164 (  1%)], Train Loss: 2.54143\n","Epoch: 00 [  271/20164 (  1%)], Train Loss: 2.49110\n","Epoch: 00 [  281/20164 (  1%)], Train Loss: 2.43555\n","Epoch: 00 [  291/20164 (  1%)], Train Loss: 2.38692\n","Epoch: 00 [  301/20164 (  1%)], Train Loss: 2.34060\n","Epoch: 00 [  311/20164 (  2%)], Train Loss: 2.28201\n","Epoch: 00 [  321/20164 (  2%)], Train Loss: 2.23287\n","Epoch: 00 [  331/20164 (  2%)], Train Loss: 2.19633\n","Epoch: 00 [  341/20164 (  2%)], Train Loss: 2.16021\n","Epoch: 00 [  351/20164 (  2%)], Train Loss: 2.10890\n","Epoch: 00 [  361/20164 (  2%)], Train Loss: 2.06960\n","Epoch: 00 [  371/20164 (  2%)], Train Loss: 2.03229\n","Epoch: 00 [  381/20164 (  2%)], Train Loss: 2.00091\n","Epoch: 00 [  391/20164 (  2%)], Train Loss: 1.96227\n","Epoch: 00 [  401/20164 (  2%)], Train Loss: 1.93947\n","Epoch: 00 [  411/20164 (  2%)], Train Loss: 1.90849\n","Epoch: 00 [  421/20164 (  2%)], Train Loss: 1.87464\n","Epoch: 00 [  431/20164 (  2%)], Train Loss: 1.83897\n","Epoch: 00 [  441/20164 (  2%)], Train Loss: 1.80283\n","Epoch: 00 [  451/20164 (  2%)], Train Loss: 1.77521\n","Epoch: 00 [  461/20164 (  2%)], Train Loss: 1.74472\n","Epoch: 00 [  471/20164 (  2%)], Train Loss: 1.71972\n","Epoch: 00 [  481/20164 (  2%)], Train Loss: 1.70356\n","Epoch: 00 [  491/20164 (  2%)], Train Loss: 1.68330\n","Epoch: 00 [  501/20164 (  2%)], Train Loss: 1.66113\n","Epoch: 00 [  511/20164 (  3%)], Train Loss: 1.63552\n","Epoch: 00 [  521/20164 (  3%)], Train Loss: 1.60934\n","Epoch: 00 [  531/20164 (  3%)], Train Loss: 1.58562\n","Epoch: 00 [  541/20164 (  3%)], Train Loss: 1.56972\n","Epoch: 00 [  551/20164 (  3%)], Train Loss: 1.55444\n","Epoch: 00 [  561/20164 (  3%)], Train Loss: 1.53663\n","Epoch: 00 [  571/20164 (  3%)], Train Loss: 1.51859\n","Epoch: 00 [  581/20164 (  3%)], Train Loss: 1.50311\n","Epoch: 00 [  591/20164 (  3%)], Train Loss: 1.47973\n","Epoch: 00 [  601/20164 (  3%)], Train Loss: 1.45893\n","Epoch: 00 [  611/20164 (  3%)], Train Loss: 1.44494\n","Epoch: 00 [  621/20164 (  3%)], Train Loss: 1.44071\n","Epoch: 00 [  631/20164 (  3%)], Train Loss: 1.42056\n","Epoch: 00 [  641/20164 (  3%)], Train Loss: 1.40451\n","Epoch: 00 [  651/20164 (  3%)], Train Loss: 1.39633\n","Epoch: 00 [  661/20164 (  3%)], Train Loss: 1.38262\n","Epoch: 00 [  671/20164 (  3%)], Train Loss: 1.36480\n","Epoch: 00 [  681/20164 (  3%)], Train Loss: 1.35372\n","Epoch: 00 [  691/20164 (  3%)], Train Loss: 1.34143\n","Epoch: 00 [  701/20164 (  3%)], Train Loss: 1.33185\n","Epoch: 00 [  711/20164 (  4%)], Train Loss: 1.31525\n","Epoch: 00 [  721/20164 (  4%)], Train Loss: 1.30346\n","Epoch: 00 [  731/20164 (  4%)], Train Loss: 1.28874\n","Epoch: 00 [  741/20164 (  4%)], Train Loss: 1.27723\n","Epoch: 00 [  751/20164 (  4%)], Train Loss: 1.26572\n","Epoch: 00 [  761/20164 (  4%)], Train Loss: 1.25222\n","Epoch: 00 [  771/20164 (  4%)], Train Loss: 1.24018\n","Epoch: 00 [  781/20164 (  4%)], Train Loss: 1.23517\n","Epoch: 00 [  791/20164 (  4%)], Train Loss: 1.22915\n","Epoch: 00 [  801/20164 (  4%)], Train Loss: 1.21854\n","Epoch: 00 [  811/20164 (  4%)], Train Loss: 1.20506\n","Epoch: 00 [  821/20164 (  4%)], Train Loss: 1.19529\n","Epoch: 00 [  831/20164 (  4%)], Train Loss: 1.19477\n","Epoch: 00 [  841/20164 (  4%)], Train Loss: 1.18343\n","Epoch: 00 [  851/20164 (  4%)], Train Loss: 1.17578\n","Epoch: 00 [  861/20164 (  4%)], Train Loss: 1.16795\n","Epoch: 00 [  871/20164 (  4%)], Train Loss: 1.15963\n","Epoch: 00 [  881/20164 (  4%)], Train Loss: 1.15017\n","Epoch: 00 [  891/20164 (  4%)], Train Loss: 1.14215\n","Epoch: 00 [  901/20164 (  4%)], Train Loss: 1.13278\n","Epoch: 00 [  911/20164 (  5%)], Train Loss: 1.12628\n","Epoch: 00 [  921/20164 (  5%)], Train Loss: 1.11662\n","Epoch: 00 [  931/20164 (  5%)], Train Loss: 1.11244\n","Epoch: 00 [  941/20164 (  5%)], Train Loss: 1.10699\n","Epoch: 00 [  951/20164 (  5%)], Train Loss: 1.10149\n","Epoch: 00 [  961/20164 (  5%)], Train Loss: 1.09069\n","Epoch: 00 [  971/20164 (  5%)], Train Loss: 1.08200\n","Epoch: 00 [  981/20164 (  5%)], Train Loss: 1.07454\n","Epoch: 00 [  991/20164 (  5%)], Train Loss: 1.06853\n","Epoch: 00 [ 1001/20164 (  5%)], Train Loss: 1.05818\n","Epoch: 00 [ 1011/20164 (  5%)], Train Loss: 1.05300\n","Epoch: 00 [ 1021/20164 (  5%)], Train Loss: 1.05280\n","Epoch: 00 [ 1031/20164 (  5%)], Train Loss: 1.04716\n","Epoch: 00 [ 1041/20164 (  5%)], Train Loss: 1.04450\n","Epoch: 00 [ 1051/20164 (  5%)], Train Loss: 1.04136\n","Epoch: 00 [ 1061/20164 (  5%)], Train Loss: 1.04177\n","Epoch: 00 [ 1071/20164 (  5%)], Train Loss: 1.03485\n","Epoch: 00 [ 1081/20164 (  5%)], Train Loss: 1.02705\n","Epoch: 00 [ 1091/20164 (  5%)], Train Loss: 1.02103\n","Epoch: 00 [ 1101/20164 (  5%)], Train Loss: 1.01853\n","Epoch: 00 [ 1111/20164 (  6%)], Train Loss: 1.01431\n","Epoch: 00 [ 1121/20164 (  6%)], Train Loss: 1.01001\n","Epoch: 00 [ 1131/20164 (  6%)], Train Loss: 1.00849\n","Epoch: 00 [ 1141/20164 (  6%)], Train Loss: 1.00603\n","Epoch: 00 [ 1151/20164 (  6%)], Train Loss: 1.00265\n","Epoch: 00 [ 1161/20164 (  6%)], Train Loss: 0.99792\n","Epoch: 00 [ 1171/20164 (  6%)], Train Loss: 0.99126\n","Epoch: 00 [ 1181/20164 (  6%)], Train Loss: 0.99415\n","Epoch: 00 [ 1191/20164 (  6%)], Train Loss: 0.99201\n","Epoch: 00 [ 1201/20164 (  6%)], Train Loss: 0.98906\n","Epoch: 00 [ 1211/20164 (  6%)], Train Loss: 0.98726\n","Epoch: 00 [ 1221/20164 (  6%)], Train Loss: 0.98080\n","Epoch: 00 [ 1231/20164 (  6%)], Train Loss: 0.97415\n","Epoch: 00 [ 1241/20164 (  6%)], Train Loss: 0.96794\n","Epoch: 00 [ 1251/20164 (  6%)], Train Loss: 0.96178\n","Epoch: 00 [ 1261/20164 (  6%)], Train Loss: 0.95553\n","Epoch: 00 [ 1271/20164 (  6%)], Train Loss: 0.94877\n","Epoch: 00 [ 1281/20164 (  6%)], Train Loss: 0.94405\n","Epoch: 00 [ 1291/20164 (  6%)], Train Loss: 0.94001\n","Epoch: 00 [ 1301/20164 (  6%)], Train Loss: 0.93513\n","Epoch: 00 [ 1311/20164 (  7%)], Train Loss: 0.93328\n","Epoch: 00 [ 1321/20164 (  7%)], Train Loss: 0.93119\n","Epoch: 00 [ 1331/20164 (  7%)], Train Loss: 0.92666\n","Epoch: 00 [ 1341/20164 (  7%)], Train Loss: 0.92511\n","Epoch: 00 [ 1351/20164 (  7%)], Train Loss: 0.92351\n","Epoch: 00 [ 1361/20164 (  7%)], Train Loss: 0.92007\n","Epoch: 00 [ 1371/20164 (  7%)], Train Loss: 0.91493\n","Epoch: 00 [ 1381/20164 (  7%)], Train Loss: 0.91454\n","Epoch: 00 [ 1391/20164 (  7%)], Train Loss: 0.91136\n","Epoch: 00 [ 1401/20164 (  7%)], Train Loss: 0.91027\n","Epoch: 00 [ 1411/20164 (  7%)], Train Loss: 0.90798\n","Epoch: 00 [ 1421/20164 (  7%)], Train Loss: 0.90896\n","Epoch: 00 [ 1431/20164 (  7%)], Train Loss: 0.90480\n","Epoch: 00 [ 1441/20164 (  7%)], Train Loss: 0.90112\n","Epoch: 00 [ 1451/20164 (  7%)], Train Loss: 0.90038\n","Epoch: 00 [ 1461/20164 (  7%)], Train Loss: 0.89555\n","Epoch: 00 [ 1471/20164 (  7%)], Train Loss: 0.89313\n","Epoch: 00 [ 1481/20164 (  7%)], Train Loss: 0.89093\n","Epoch: 00 [ 1491/20164 (  7%)], Train Loss: 0.88819\n","Epoch: 00 [ 1501/20164 (  7%)], Train Loss: 0.88526\n","Epoch: 00 [ 1511/20164 (  7%)], Train Loss: 0.88119\n","Epoch: 00 [ 1521/20164 (  8%)], Train Loss: 0.87667\n","Epoch: 00 [ 1531/20164 (  8%)], Train Loss: 0.87100\n","Epoch: 00 [ 1541/20164 (  8%)], Train Loss: 0.86745\n","Epoch: 00 [ 1551/20164 (  8%)], Train Loss: 0.86240\n","Epoch: 00 [ 1561/20164 (  8%)], Train Loss: 0.86285\n","Epoch: 00 [ 1571/20164 (  8%)], Train Loss: 0.85806\n","Epoch: 00 [ 1581/20164 (  8%)], Train Loss: 0.85669\n","Epoch: 00 [ 1591/20164 (  8%)], Train Loss: 0.85380\n","Epoch: 00 [ 1601/20164 (  8%)], Train Loss: 0.84871\n","Epoch: 00 [ 1611/20164 (  8%)], Train Loss: 0.84899\n","Epoch: 00 [ 1621/20164 (  8%)], Train Loss: 0.84600\n","Epoch: 00 [ 1631/20164 (  8%)], Train Loss: 0.84388\n","Epoch: 00 [ 1641/20164 (  8%)], Train Loss: 0.84515\n","Epoch: 00 [ 1651/20164 (  8%)], Train Loss: 0.84245\n","Epoch: 00 [ 1661/20164 (  8%)], Train Loss: 0.84084\n","Epoch: 00 [ 1671/20164 (  8%)], Train Loss: 0.84079\n","Epoch: 00 [ 1681/20164 (  8%)], Train Loss: 0.83931\n","Epoch: 00 [ 1691/20164 (  8%)], Train Loss: 0.83521\n","Epoch: 00 [ 1701/20164 (  8%)], Train Loss: 0.83449\n","Epoch: 00 [ 1711/20164 (  8%)], Train Loss: 0.83341\n","Epoch: 00 [ 1721/20164 (  9%)], Train Loss: 0.83064\n","Epoch: 00 [ 1731/20164 (  9%)], Train Loss: 0.82754\n","Epoch: 00 [ 1741/20164 (  9%)], Train Loss: 0.82656\n","Epoch: 00 [ 1751/20164 (  9%)], Train Loss: 0.82658\n","Epoch: 00 [ 1761/20164 (  9%)], Train Loss: 0.82502\n","Epoch: 00 [ 1771/20164 (  9%)], Train Loss: 0.82236\n","Epoch: 00 [ 1781/20164 (  9%)], Train Loss: 0.82020\n","Epoch: 00 [ 1791/20164 (  9%)], Train Loss: 0.81863\n","Epoch: 00 [ 1801/20164 (  9%)], Train Loss: 0.81696\n","Epoch: 00 [ 1811/20164 (  9%)], Train Loss: 0.81471\n","Epoch: 00 [ 1821/20164 (  9%)], Train Loss: 0.81635\n","Epoch: 00 [ 1831/20164 (  9%)], Train Loss: 0.81584\n","Epoch: 00 [ 1841/20164 (  9%)], Train Loss: 0.81638\n","Epoch: 00 [ 1851/20164 (  9%)], Train Loss: 0.81327\n","Epoch: 00 [ 1861/20164 (  9%)], Train Loss: 0.81247\n","Epoch: 00 [ 1871/20164 (  9%)], Train Loss: 0.81107\n","Epoch: 00 [ 1881/20164 (  9%)], Train Loss: 0.80902\n","Epoch: 00 [ 1891/20164 (  9%)], Train Loss: 0.80634\n","Epoch: 00 [ 1901/20164 (  9%)], Train Loss: 0.80391\n","Epoch: 00 [ 1911/20164 (  9%)], Train Loss: 0.80081\n","Epoch: 00 [ 1921/20164 ( 10%)], Train Loss: 0.80191\n","Epoch: 00 [ 1931/20164 ( 10%)], Train Loss: 0.79913\n","Epoch: 00 [ 1941/20164 ( 10%)], Train Loss: 0.79994\n","Epoch: 00 [ 1951/20164 ( 10%)], Train Loss: 0.79704\n","Epoch: 00 [ 1961/20164 ( 10%)], Train Loss: 0.79657\n","Epoch: 00 [ 1971/20164 ( 10%)], Train Loss: 0.79377\n","Epoch: 00 [ 1981/20164 ( 10%)], Train Loss: 0.79467\n","Epoch: 00 [ 1991/20164 ( 10%)], Train Loss: 0.79107\n","Epoch: 00 [ 2001/20164 ( 10%)], Train Loss: 0.78883\n","Epoch: 00 [ 2011/20164 ( 10%)], Train Loss: 0.79156\n","Epoch: 00 [ 2021/20164 ( 10%)], Train Loss: 0.79030\n","Epoch: 00 [ 2031/20164 ( 10%)], Train Loss: 0.79092\n","Epoch: 00 [ 2041/20164 ( 10%)], Train Loss: 0.79030\n","Epoch: 00 [ 2051/20164 ( 10%)], Train Loss: 0.78852\n","Epoch: 00 [ 2061/20164 ( 10%)], Train Loss: 0.78678\n","Epoch: 00 [ 2071/20164 ( 10%)], Train Loss: 0.78505\n","Epoch: 00 [ 2081/20164 ( 10%)], Train Loss: 0.78328\n","Epoch: 00 [ 2091/20164 ( 10%)], Train Loss: 0.78097\n","Epoch: 00 [ 2101/20164 ( 10%)], Train Loss: 0.77788\n","Epoch: 00 [ 2111/20164 ( 10%)], Train Loss: 0.77617\n","Epoch: 00 [ 2121/20164 ( 11%)], Train Loss: 0.77336\n","Epoch: 00 [ 2131/20164 ( 11%)], Train Loss: 0.77578\n","Epoch: 00 [ 2141/20164 ( 11%)], Train Loss: 0.77631\n","Epoch: 00 [ 2151/20164 ( 11%)], Train Loss: 0.77520\n","Epoch: 00 [ 2161/20164 ( 11%)], Train Loss: 0.77239\n","Epoch: 00 [ 2171/20164 ( 11%)], Train Loss: 0.77242\n","Epoch: 00 [ 2181/20164 ( 11%)], Train Loss: 0.77281\n","Epoch: 00 [ 2191/20164 ( 11%)], Train Loss: 0.77275\n","Epoch: 00 [ 2201/20164 ( 11%)], Train Loss: 0.77241\n","Epoch: 00 [ 2211/20164 ( 11%)], Train Loss: 0.77216\n","Epoch: 00 [ 2221/20164 ( 11%)], Train Loss: 0.77224\n","Epoch: 00 [ 2231/20164 ( 11%)], Train Loss: 0.77050\n","Epoch: 00 [ 2241/20164 ( 11%)], Train Loss: 0.76886\n","Epoch: 00 [ 2251/20164 ( 11%)], Train Loss: 0.76641\n","Epoch: 00 [ 2261/20164 ( 11%)], Train Loss: 0.76536\n","Epoch: 00 [ 2271/20164 ( 11%)], Train Loss: 0.76593\n","Epoch: 00 [ 2281/20164 ( 11%)], Train Loss: 0.76571\n","Epoch: 00 [ 2291/20164 ( 11%)], Train Loss: 0.76390\n","Epoch: 00 [ 2301/20164 ( 11%)], Train Loss: 0.76472\n","Epoch: 00 [ 2311/20164 ( 11%)], Train Loss: 0.76483\n","Epoch: 00 [ 2321/20164 ( 12%)], Train Loss: 0.76301\n","Epoch: 00 [ 2331/20164 ( 12%)], Train Loss: 0.76371\n","Epoch: 00 [ 2341/20164 ( 12%)], Train Loss: 0.76261\n","Epoch: 00 [ 2351/20164 ( 12%)], Train Loss: 0.76067\n","Epoch: 00 [ 2361/20164 ( 12%)], Train Loss: 0.75879\n","Epoch: 00 [ 2371/20164 ( 12%)], Train Loss: 0.75678\n","Epoch: 00 [ 2381/20164 ( 12%)], Train Loss: 0.75609\n","Epoch: 00 [ 2391/20164 ( 12%)], Train Loss: 0.75448\n","Epoch: 00 [ 2401/20164 ( 12%)], Train Loss: 0.75439\n","Epoch: 00 [ 2411/20164 ( 12%)], Train Loss: 0.75342\n","Epoch: 00 [ 2421/20164 ( 12%)], Train Loss: 0.75230\n","Epoch: 00 [ 2431/20164 ( 12%)], Train Loss: 0.75142\n","Epoch: 00 [ 2441/20164 ( 12%)], Train Loss: 0.75014\n","Epoch: 00 [ 2451/20164 ( 12%)], Train Loss: 0.74973\n","Epoch: 00 [ 2461/20164 ( 12%)], Train Loss: 0.74723\n","Epoch: 00 [ 2471/20164 ( 12%)], Train Loss: 0.74490\n","Epoch: 00 [ 2481/20164 ( 12%)], Train Loss: 0.74370\n","Epoch: 00 [ 2491/20164 ( 12%)], Train Loss: 0.74401\n","Epoch: 00 [ 2501/20164 ( 12%)], Train Loss: 0.74338\n","Epoch: 00 [ 2511/20164 ( 12%)], Train Loss: 0.74251\n","Epoch: 00 [ 2521/20164 ( 13%)], Train Loss: 0.73968\n","Epoch: 00 [ 2531/20164 ( 13%)], Train Loss: 0.73961\n","Epoch: 00 [ 2541/20164 ( 13%)], Train Loss: 0.73778\n","Epoch: 00 [ 2551/20164 ( 13%)], Train Loss: 0.73771\n","Epoch: 00 [ 2561/20164 ( 13%)], Train Loss: 0.73533\n","Epoch: 00 [ 2571/20164 ( 13%)], Train Loss: 0.73419\n","Epoch: 00 [ 2581/20164 ( 13%)], Train Loss: 0.73455\n","Epoch: 00 [ 2591/20164 ( 13%)], Train Loss: 0.73322\n","Epoch: 00 [ 2601/20164 ( 13%)], Train Loss: 0.73213\n","Epoch: 00 [ 2611/20164 ( 13%)], Train Loss: 0.73092\n","Epoch: 00 [ 2621/20164 ( 13%)], Train Loss: 0.73072\n","Epoch: 00 [ 2631/20164 ( 13%)], Train Loss: 0.72925\n","Epoch: 00 [ 2641/20164 ( 13%)], Train Loss: 0.72929\n","Epoch: 00 [ 2651/20164 ( 13%)], Train Loss: 0.72921\n","Epoch: 00 [ 2661/20164 ( 13%)], Train Loss: 0.72838\n","Epoch: 00 [ 2671/20164 ( 13%)], Train Loss: 0.72738\n","Epoch: 00 [ 2681/20164 ( 13%)], Train Loss: 0.72528\n","Epoch: 00 [ 2691/20164 ( 13%)], Train Loss: 0.72286\n","Epoch: 00 [ 2701/20164 ( 13%)], Train Loss: 0.72275\n","Epoch: 00 [ 2711/20164 ( 13%)], Train Loss: 0.72110\n","Epoch: 00 [ 2721/20164 ( 13%)], Train Loss: 0.71964\n","Epoch: 00 [ 2731/20164 ( 14%)], Train Loss: 0.71768\n","Epoch: 00 [ 2741/20164 ( 14%)], Train Loss: 0.71778\n","Epoch: 00 [ 2751/20164 ( 14%)], Train Loss: 0.71548\n","Epoch: 00 [ 2761/20164 ( 14%)], Train Loss: 0.71299\n","Epoch: 00 [ 2771/20164 ( 14%)], Train Loss: 0.71388\n","Epoch: 00 [ 2781/20164 ( 14%)], Train Loss: 0.71185\n","Epoch: 00 [ 2791/20164 ( 14%)], Train Loss: 0.71160\n","Epoch: 00 [ 2801/20164 ( 14%)], Train Loss: 0.70984\n","Epoch: 00 [ 2811/20164 ( 14%)], Train Loss: 0.71103\n","Epoch: 00 [ 2821/20164 ( 14%)], Train Loss: 0.71014\n","Epoch: 00 [ 2831/20164 ( 14%)], Train Loss: 0.70816\n","Epoch: 00 [ 2841/20164 ( 14%)], Train Loss: 0.70618\n","Epoch: 00 [ 2851/20164 ( 14%)], Train Loss: 0.70530\n","Epoch: 00 [ 2861/20164 ( 14%)], Train Loss: 0.70766\n","Epoch: 00 [ 2871/20164 ( 14%)], Train Loss: 0.70665\n","Epoch: 00 [ 2881/20164 ( 14%)], Train Loss: 0.70626\n","Epoch: 00 [ 2891/20164 ( 14%)], Train Loss: 0.70528\n","Epoch: 00 [ 2901/20164 ( 14%)], Train Loss: 0.70640\n","Epoch: 00 [ 2911/20164 ( 14%)], Train Loss: 0.70506\n","Epoch: 00 [ 2921/20164 ( 14%)], Train Loss: 0.70449\n","Epoch: 00 [ 2931/20164 ( 15%)], Train Loss: 0.70290\n","Epoch: 00 [ 2941/20164 ( 15%)], Train Loss: 0.70078\n","Epoch: 00 [ 2951/20164 ( 15%)], Train Loss: 0.70260\n","Epoch: 00 [ 2961/20164 ( 15%)], Train Loss: 0.70185\n","Epoch: 00 [ 2971/20164 ( 15%)], Train Loss: 0.70176\n","Epoch: 00 [ 2981/20164 ( 15%)], Train Loss: 0.70028\n","Epoch: 00 [ 2991/20164 ( 15%)], Train Loss: 0.69977\n","Epoch: 00 [ 3001/20164 ( 15%)], Train Loss: 0.69976\n","Epoch: 00 [ 3011/20164 ( 15%)], Train Loss: 0.69906\n","Epoch: 00 [ 3021/20164 ( 15%)], Train Loss: 0.69837\n","Epoch: 00 [ 3031/20164 ( 15%)], Train Loss: 0.69776\n","Epoch: 00 [ 3041/20164 ( 15%)], Train Loss: 0.69828\n","Epoch: 00 [ 3051/20164 ( 15%)], Train Loss: 0.69891\n","Epoch: 00 [ 3061/20164 ( 15%)], Train Loss: 0.69803\n","Epoch: 00 [ 3071/20164 ( 15%)], Train Loss: 0.69646\n","Epoch: 00 [ 3081/20164 ( 15%)], Train Loss: 0.69663\n","Epoch: 00 [ 3091/20164 ( 15%)], Train Loss: 0.69489\n","Epoch: 00 [ 3101/20164 ( 15%)], Train Loss: 0.69388\n","Epoch: 00 [ 3111/20164 ( 15%)], Train Loss: 0.69317\n","Epoch: 00 [ 3121/20164 ( 15%)], Train Loss: 0.69347\n","Epoch: 00 [ 3131/20164 ( 16%)], Train Loss: 0.69234\n","Epoch: 00 [ 3141/20164 ( 16%)], Train Loss: 0.69288\n","Epoch: 00 [ 3151/20164 ( 16%)], Train Loss: 0.69196\n","Epoch: 00 [ 3161/20164 ( 16%)], Train Loss: 0.69062\n","Epoch: 00 [ 3171/20164 ( 16%)], Train Loss: 0.69055\n","Epoch: 00 [ 3181/20164 ( 16%)], Train Loss: 0.69140\n","Epoch: 00 [ 3191/20164 ( 16%)], Train Loss: 0.69088\n","Epoch: 00 [ 3201/20164 ( 16%)], Train Loss: 0.68997\n","Epoch: 00 [ 3211/20164 ( 16%)], Train Loss: 0.68847\n","Epoch: 00 [ 3221/20164 ( 16%)], Train Loss: 0.68757\n","Epoch: 00 [ 3231/20164 ( 16%)], Train Loss: 0.68688\n","Epoch: 00 [ 3241/20164 ( 16%)], Train Loss: 0.68573\n","Epoch: 00 [ 3251/20164 ( 16%)], Train Loss: 0.68534\n","Epoch: 00 [ 3261/20164 ( 16%)], Train Loss: 0.68482\n","Epoch: 00 [ 3271/20164 ( 16%)], Train Loss: 0.68395\n","Epoch: 00 [ 3281/20164 ( 16%)], Train Loss: 0.68347\n","Epoch: 00 [ 3291/20164 ( 16%)], Train Loss: 0.68333\n","Epoch: 00 [ 3301/20164 ( 16%)], Train Loss: 0.68280\n","Epoch: 00 [ 3311/20164 ( 16%)], Train Loss: 0.68215\n","Epoch: 00 [ 3321/20164 ( 16%)], Train Loss: 0.68150\n","Epoch: 00 [ 3331/20164 ( 17%)], Train Loss: 0.68047\n","Epoch: 00 [ 3341/20164 ( 17%)], Train Loss: 0.67928\n","Epoch: 00 [ 3351/20164 ( 17%)], Train Loss: 0.67838\n","Epoch: 00 [ 3361/20164 ( 17%)], Train Loss: 0.67767\n","Epoch: 00 [ 3371/20164 ( 17%)], Train Loss: 0.67673\n","Epoch: 00 [ 3381/20164 ( 17%)], Train Loss: 0.67838\n","Epoch: 00 [ 3391/20164 ( 17%)], Train Loss: 0.67720\n","Epoch: 00 [ 3401/20164 ( 17%)], Train Loss: 0.67648\n","Epoch: 00 [ 3411/20164 ( 17%)], Train Loss: 0.67565\n","Epoch: 00 [ 3421/20164 ( 17%)], Train Loss: 0.67490\n","Epoch: 00 [ 3431/20164 ( 17%)], Train Loss: 0.67406\n","Epoch: 00 [ 3441/20164 ( 17%)], Train Loss: 0.67407\n","Epoch: 00 [ 3451/20164 ( 17%)], Train Loss: 0.67368\n","Epoch: 00 [ 3461/20164 ( 17%)], Train Loss: 0.67294\n","Epoch: 00 [ 3471/20164 ( 17%)], Train Loss: 0.67252\n","Epoch: 00 [ 3481/20164 ( 17%)], Train Loss: 0.67089\n","Epoch: 00 [ 3491/20164 ( 17%)], Train Loss: 0.67138\n","Epoch: 00 [ 3501/20164 ( 17%)], Train Loss: 0.67305\n","Epoch: 00 [ 3511/20164 ( 17%)], Train Loss: 0.67325\n","Epoch: 00 [ 3521/20164 ( 17%)], Train Loss: 0.67303\n","Epoch: 00 [ 3531/20164 ( 18%)], Train Loss: 0.67257\n","Epoch: 00 [ 3541/20164 ( 18%)], Train Loss: 0.67133\n","Epoch: 00 [ 3551/20164 ( 18%)], Train Loss: 0.67138\n","Epoch: 00 [ 3561/20164 ( 18%)], Train Loss: 0.67131\n","Epoch: 00 [ 3571/20164 ( 18%)], Train Loss: 0.67059\n","Epoch: 00 [ 3581/20164 ( 18%)], Train Loss: 0.67074\n","Epoch: 00 [ 3591/20164 ( 18%)], Train Loss: 0.67041\n","Epoch: 00 [ 3601/20164 ( 18%)], Train Loss: 0.66917\n","Epoch: 00 [ 3611/20164 ( 18%)], Train Loss: 0.67017\n","Epoch: 00 [ 3621/20164 ( 18%)], Train Loss: 0.66969\n","Epoch: 00 [ 3631/20164 ( 18%)], Train Loss: 0.66883\n","Epoch: 00 [ 3641/20164 ( 18%)], Train Loss: 0.66786\n","Epoch: 00 [ 3651/20164 ( 18%)], Train Loss: 0.66620\n","Epoch: 00 [ 3661/20164 ( 18%)], Train Loss: 0.66591\n","Epoch: 00 [ 3671/20164 ( 18%)], Train Loss: 0.66559\n","Epoch: 00 [ 3681/20164 ( 18%)], Train Loss: 0.66517\n","Epoch: 00 [ 3691/20164 ( 18%)], Train Loss: 0.66537\n","Epoch: 00 [ 3701/20164 ( 18%)], Train Loss: 0.66566\n","Epoch: 00 [ 3711/20164 ( 18%)], Train Loss: 0.66529\n","Epoch: 00 [ 3721/20164 ( 18%)], Train Loss: 0.66430\n","Epoch: 00 [ 3731/20164 ( 19%)], Train Loss: 0.66355\n","Epoch: 00 [ 3741/20164 ( 19%)], Train Loss: 0.66329\n","Epoch: 00 [ 3751/20164 ( 19%)], Train Loss: 0.66243\n","Epoch: 00 [ 3761/20164 ( 19%)], Train Loss: 0.66105\n","Epoch: 00 [ 3771/20164 ( 19%)], Train Loss: 0.66032\n","Epoch: 00 [ 3781/20164 ( 19%)], Train Loss: 0.66018\n","Epoch: 00 [ 3791/20164 ( 19%)], Train Loss: 0.66050\n","Epoch: 00 [ 3801/20164 ( 19%)], Train Loss: 0.65993\n","Epoch: 00 [ 3811/20164 ( 19%)], Train Loss: 0.65885\n","Epoch: 00 [ 3821/20164 ( 19%)], Train Loss: 0.65795\n","Epoch: 00 [ 3831/20164 ( 19%)], Train Loss: 0.65745\n","Epoch: 00 [ 3841/20164 ( 19%)], Train Loss: 0.65858\n","Epoch: 00 [ 3851/20164 ( 19%)], Train Loss: 0.65868\n","Epoch: 00 [ 3861/20164 ( 19%)], Train Loss: 0.65756\n","Epoch: 00 [ 3871/20164 ( 19%)], Train Loss: 0.65698\n","Epoch: 00 [ 3881/20164 ( 19%)], Train Loss: 0.65641\n","Epoch: 00 [ 3891/20164 ( 19%)], Train Loss: 0.65664\n","Epoch: 00 [ 3901/20164 ( 19%)], Train Loss: 0.65617\n","Epoch: 00 [ 3911/20164 ( 19%)], Train Loss: 0.65516\n","Epoch: 00 [ 3921/20164 ( 19%)], Train Loss: 0.65460\n","Epoch: 00 [ 3931/20164 ( 19%)], Train Loss: 0.65445\n","Epoch: 00 [ 3941/20164 ( 20%)], Train Loss: 0.65457\n","Epoch: 00 [ 3951/20164 ( 20%)], Train Loss: 0.65496\n","Epoch: 00 [ 3961/20164 ( 20%)], Train Loss: 0.65467\n","Epoch: 00 [ 3971/20164 ( 20%)], Train Loss: 0.65442\n","Epoch: 00 [ 3981/20164 ( 20%)], Train Loss: 0.65361\n","Epoch: 00 [ 3991/20164 ( 20%)], Train Loss: 0.65289\n","Epoch: 00 [ 4001/20164 ( 20%)], Train Loss: 0.65147\n","Epoch: 00 [ 4011/20164 ( 20%)], Train Loss: 0.65067\n","Epoch: 00 [ 4021/20164 ( 20%)], Train Loss: 0.64992\n","Epoch: 00 [ 4031/20164 ( 20%)], Train Loss: 0.65127\n","Epoch: 00 [ 4041/20164 ( 20%)], Train Loss: 0.65176\n","Epoch: 00 [ 4051/20164 ( 20%)], Train Loss: 0.65107\n","Epoch: 00 [ 4061/20164 ( 20%)], Train Loss: 0.65159\n","Epoch: 00 [ 4071/20164 ( 20%)], Train Loss: 0.65094\n","Epoch: 00 [ 4081/20164 ( 20%)], Train Loss: 0.64987\n","Epoch: 00 [ 4091/20164 ( 20%)], Train Loss: 0.64984\n","Epoch: 00 [ 4101/20164 ( 20%)], Train Loss: 0.64871\n","Epoch: 00 [ 4111/20164 ( 20%)], Train Loss: 0.65020\n","Epoch: 00 [ 4121/20164 ( 20%)], Train Loss: 0.64952\n","Epoch: 00 [ 4131/20164 ( 20%)], Train Loss: 0.64973\n","Epoch: 00 [ 4141/20164 ( 21%)], Train Loss: 0.64894\n","Epoch: 00 [ 4151/20164 ( 21%)], Train Loss: 0.64778\n","Epoch: 00 [ 4161/20164 ( 21%)], Train Loss: 0.64705\n","Epoch: 00 [ 4171/20164 ( 21%)], Train Loss: 0.64640\n","Epoch: 00 [ 4181/20164 ( 21%)], Train Loss: 0.64531\n","Epoch: 00 [ 4191/20164 ( 21%)], Train Loss: 0.64386\n","Epoch: 00 [ 4201/20164 ( 21%)], Train Loss: 0.64362\n","Epoch: 00 [ 4211/20164 ( 21%)], Train Loss: 0.64290\n","Epoch: 00 [ 4221/20164 ( 21%)], Train Loss: 0.64214\n","Epoch: 00 [ 4231/20164 ( 21%)], Train Loss: 0.64257\n","Epoch: 00 [ 4241/20164 ( 21%)], Train Loss: 0.64159\n","Epoch: 00 [ 4251/20164 ( 21%)], Train Loss: 0.64124\n","Epoch: 00 [ 4261/20164 ( 21%)], Train Loss: 0.64060\n","Epoch: 00 [ 4271/20164 ( 21%)], Train Loss: 0.64244\n","Epoch: 00 [ 4281/20164 ( 21%)], Train Loss: 0.64311\n","Epoch: 00 [ 4291/20164 ( 21%)], Train Loss: 0.64190\n","Epoch: 00 [ 4301/20164 ( 21%)], Train Loss: 0.64153\n","Epoch: 00 [ 4311/20164 ( 21%)], Train Loss: 0.64134\n","Epoch: 00 [ 4321/20164 ( 21%)], Train Loss: 0.64127\n","Epoch: 00 [ 4331/20164 ( 21%)], Train Loss: 0.64181\n","Epoch: 00 [ 4341/20164 ( 22%)], Train Loss: 0.64113\n","Epoch: 00 [ 4351/20164 ( 22%)], Train Loss: 0.64033\n","Epoch: 00 [ 4361/20164 ( 22%)], Train Loss: 0.63974\n","Epoch: 00 [ 4371/20164 ( 22%)], Train Loss: 0.63882\n","Epoch: 00 [ 4381/20164 ( 22%)], Train Loss: 0.63802\n","Epoch: 00 [ 4391/20164 ( 22%)], Train Loss: 0.63709\n","Epoch: 00 [ 4401/20164 ( 22%)], Train Loss: 0.63717\n","Epoch: 00 [ 4411/20164 ( 22%)], Train Loss: 0.63707\n","Epoch: 00 [ 4421/20164 ( 22%)], Train Loss: 0.63709\n","Epoch: 00 [ 4431/20164 ( 22%)], Train Loss: 0.63669\n","Epoch: 00 [ 4441/20164 ( 22%)], Train Loss: 0.63628\n","Epoch: 00 [ 4451/20164 ( 22%)], Train Loss: 0.63564\n","Epoch: 00 [ 4461/20164 ( 22%)], Train Loss: 0.63549\n","Epoch: 00 [ 4471/20164 ( 22%)], Train Loss: 0.63489\n","Epoch: 00 [ 4481/20164 ( 22%)], Train Loss: 0.63572\n","Epoch: 00 [ 4491/20164 ( 22%)], Train Loss: 0.63537\n","Epoch: 00 [ 4501/20164 ( 22%)], Train Loss: 0.63469\n","Epoch: 00 [ 4511/20164 ( 22%)], Train Loss: 0.63464\n","Epoch: 00 [ 4521/20164 ( 22%)], Train Loss: 0.63407\n","Epoch: 00 [ 4531/20164 ( 22%)], Train Loss: 0.63564\n","Epoch: 00 [ 4541/20164 ( 23%)], Train Loss: 0.63508\n","Epoch: 00 [ 4551/20164 ( 23%)], Train Loss: 0.63444\n","Epoch: 00 [ 4561/20164 ( 23%)], Train Loss: 0.63384\n","Epoch: 00 [ 4571/20164 ( 23%)], Train Loss: 0.63365\n","Epoch: 00 [ 4581/20164 ( 23%)], Train Loss: 0.63252\n","Epoch: 00 [ 4591/20164 ( 23%)], Train Loss: 0.63276\n","Epoch: 00 [ 4601/20164 ( 23%)], Train Loss: 0.63274\n","Epoch: 00 [ 4611/20164 ( 23%)], Train Loss: 0.63181\n","Epoch: 00 [ 4621/20164 ( 23%)], Train Loss: 0.63286\n","Epoch: 00 [ 4631/20164 ( 23%)], Train Loss: 0.63266\n","Epoch: 00 [ 4641/20164 ( 23%)], Train Loss: 0.63239\n","Epoch: 00 [ 4651/20164 ( 23%)], Train Loss: 0.63198\n","Epoch: 00 [ 4661/20164 ( 23%)], Train Loss: 0.63122\n","Epoch: 00 [ 4671/20164 ( 23%)], Train Loss: 0.63184\n","Epoch: 00 [ 4681/20164 ( 23%)], Train Loss: 0.63101\n","Epoch: 00 [ 4691/20164 ( 23%)], Train Loss: 0.63126\n","Epoch: 00 [ 4701/20164 ( 23%)], Train Loss: 0.63083\n","Epoch: 00 [ 4711/20164 ( 23%)], Train Loss: 0.63011\n","Epoch: 00 [ 4721/20164 ( 23%)], Train Loss: 0.62897\n","Epoch: 00 [ 4731/20164 ( 23%)], Train Loss: 0.62884\n","Epoch: 00 [ 4741/20164 ( 24%)], Train Loss: 0.62913\n","Epoch: 00 [ 4751/20164 ( 24%)], Train Loss: 0.62891\n","Epoch: 00 [ 4761/20164 ( 24%)], Train Loss: 0.62849\n","Epoch: 00 [ 4771/20164 ( 24%)], Train Loss: 0.62810\n","Epoch: 00 [ 4781/20164 ( 24%)], Train Loss: 0.62712\n","Epoch: 00 [ 4791/20164 ( 24%)], Train Loss: 0.62619\n","Epoch: 00 [ 4801/20164 ( 24%)], Train Loss: 0.62562\n","Epoch: 00 [ 4811/20164 ( 24%)], Train Loss: 0.62565\n","Epoch: 00 [ 4821/20164 ( 24%)], Train Loss: 0.62458\n","Epoch: 00 [ 4831/20164 ( 24%)], Train Loss: 0.62468\n","Epoch: 00 [ 4841/20164 ( 24%)], Train Loss: 0.62423\n","Epoch: 00 [ 4851/20164 ( 24%)], Train Loss: 0.62356\n","Epoch: 00 [ 4861/20164 ( 24%)], Train Loss: 0.62366\n","Epoch: 00 [ 4871/20164 ( 24%)], Train Loss: 0.62335\n","Epoch: 00 [ 4881/20164 ( 24%)], Train Loss: 0.62284\n","Epoch: 00 [ 4891/20164 ( 24%)], Train Loss: 0.62240\n","Epoch: 00 [ 4901/20164 ( 24%)], Train Loss: 0.62309\n","Epoch: 00 [ 4911/20164 ( 24%)], Train Loss: 0.62299\n","Epoch: 00 [ 4921/20164 ( 24%)], Train Loss: 0.62290\n","Epoch: 00 [ 4931/20164 ( 24%)], Train Loss: 0.62225\n","Epoch: 00 [ 4941/20164 ( 25%)], Train Loss: 0.62154\n","Epoch: 00 [ 4951/20164 ( 25%)], Train Loss: 0.62125\n","Epoch: 00 [ 4961/20164 ( 25%)], Train Loss: 0.62093\n","Epoch: 00 [ 4971/20164 ( 25%)], Train Loss: 0.62087\n","Epoch: 00 [ 4981/20164 ( 25%)], Train Loss: 0.62029\n","Epoch: 00 [ 4991/20164 ( 25%)], Train Loss: 0.61965\n","Epoch: 00 [ 5001/20164 ( 25%)], Train Loss: 0.61957\n","Epoch: 00 [ 5011/20164 ( 25%)], Train Loss: 0.61897\n","Epoch: 00 [ 5021/20164 ( 25%)], Train Loss: 0.61821\n","Epoch: 00 [ 5031/20164 ( 25%)], Train Loss: 0.61813\n","Epoch: 00 [ 5041/20164 ( 25%)], Train Loss: 0.61920\n","Epoch: 00 [ 5051/20164 ( 25%)], Train Loss: 0.61848\n","Epoch: 00 [ 5061/20164 ( 25%)], Train Loss: 0.61883\n","Epoch: 00 [ 5071/20164 ( 25%)], Train Loss: 0.61898\n","Epoch: 00 [ 5081/20164 ( 25%)], Train Loss: 0.61919\n","Epoch: 00 [ 5091/20164 ( 25%)], Train Loss: 0.61925\n","Epoch: 00 [ 5101/20164 ( 25%)], Train Loss: 0.61874\n","Epoch: 00 [ 5111/20164 ( 25%)], Train Loss: 0.61970\n","Epoch: 00 [ 5121/20164 ( 25%)], Train Loss: 0.61990\n","Epoch: 00 [ 5131/20164 ( 25%)], Train Loss: 0.61908\n","Epoch: 00 [ 5141/20164 ( 25%)], Train Loss: 0.61801\n","Epoch: 00 [ 5151/20164 ( 26%)], Train Loss: 0.61766\n","Epoch: 00 [ 5161/20164 ( 26%)], Train Loss: 0.61759\n","Epoch: 00 [ 5171/20164 ( 26%)], Train Loss: 0.61751\n","Epoch: 00 [ 5181/20164 ( 26%)], Train Loss: 0.61743\n","Epoch: 00 [ 5191/20164 ( 26%)], Train Loss: 0.61685\n","Epoch: 00 [ 5201/20164 ( 26%)], Train Loss: 0.61593\n","Epoch: 00 [ 5211/20164 ( 26%)], Train Loss: 0.61554\n","Epoch: 00 [ 5221/20164 ( 26%)], Train Loss: 0.61442\n","Epoch: 00 [ 5231/20164 ( 26%)], Train Loss: 0.61501\n","Epoch: 00 [ 5241/20164 ( 26%)], Train Loss: 0.61457\n","Epoch: 00 [ 5251/20164 ( 26%)], Train Loss: 0.61436\n","Epoch: 00 [ 5261/20164 ( 26%)], Train Loss: 0.61361\n","Epoch: 00 [ 5271/20164 ( 26%)], Train Loss: 0.61470\n","Epoch: 00 [ 5281/20164 ( 26%)], Train Loss: 0.61494\n","Epoch: 00 [ 5291/20164 ( 26%)], Train Loss: 0.61478\n","Epoch: 00 [ 5301/20164 ( 26%)], Train Loss: 0.61466\n","Epoch: 00 [ 5311/20164 ( 26%)], Train Loss: 0.61389\n","Epoch: 00 [ 5321/20164 ( 26%)], Train Loss: 0.61394\n","Epoch: 00 [ 5331/20164 ( 26%)], Train Loss: 0.61289\n","Epoch: 00 [ 5341/20164 ( 26%)], Train Loss: 0.61285\n","Epoch: 00 [ 5351/20164 ( 27%)], Train Loss: 0.61323\n","Epoch: 00 [ 5361/20164 ( 27%)], Train Loss: 0.61248\n","Epoch: 00 [ 5371/20164 ( 27%)], Train Loss: 0.61242\n","Epoch: 00 [ 5381/20164 ( 27%)], Train Loss: 0.61216\n","Epoch: 00 [ 5391/20164 ( 27%)], Train Loss: 0.61127\n","Epoch: 00 [ 5401/20164 ( 27%)], Train Loss: 0.61045\n","Epoch: 00 [ 5411/20164 ( 27%)], Train Loss: 0.61030\n","Epoch: 00 [ 5421/20164 ( 27%)], Train Loss: 0.60986\n","Epoch: 00 [ 5431/20164 ( 27%)], Train Loss: 0.60950\n","Epoch: 00 [ 5441/20164 ( 27%)], Train Loss: 0.60954\n","Epoch: 00 [ 5451/20164 ( 27%)], Train Loss: 0.60970\n","Epoch: 00 [ 5461/20164 ( 27%)], Train Loss: 0.60955\n","Epoch: 00 [ 5471/20164 ( 27%)], Train Loss: 0.60964\n","Epoch: 00 [ 5481/20164 ( 27%)], Train Loss: 0.60883\n","Epoch: 00 [ 5491/20164 ( 27%)], Train Loss: 0.60834\n","Epoch: 00 [ 5501/20164 ( 27%)], Train Loss: 0.60829\n","Epoch: 00 [ 5511/20164 ( 27%)], Train Loss: 0.60745\n","Epoch: 00 [ 5521/20164 ( 27%)], Train Loss: 0.60739\n","Epoch: 00 [ 5531/20164 ( 27%)], Train Loss: 0.60731\n","Epoch: 00 [ 5541/20164 ( 27%)], Train Loss: 0.60658\n","Epoch: 00 [ 5551/20164 ( 28%)], Train Loss: 0.60594\n","Epoch: 00 [ 5561/20164 ( 28%)], Train Loss: 0.60621\n","Epoch: 00 [ 5571/20164 ( 28%)], Train Loss: 0.60760\n","Epoch: 00 [ 5581/20164 ( 28%)], Train Loss: 0.60746\n","Epoch: 00 [ 5591/20164 ( 28%)], Train Loss: 0.60694\n","Epoch: 00 [ 5601/20164 ( 28%)], Train Loss: 0.60710\n","Epoch: 00 [ 5611/20164 ( 28%)], Train Loss: 0.60696\n","Epoch: 00 [ 5621/20164 ( 28%)], Train Loss: 0.60638\n","Epoch: 00 [ 5631/20164 ( 28%)], Train Loss: 0.60621\n","Epoch: 00 [ 5641/20164 ( 28%)], Train Loss: 0.60610\n","Epoch: 00 [ 5651/20164 ( 28%)], Train Loss: 0.60645\n","Epoch: 00 [ 5661/20164 ( 28%)], Train Loss: 0.60594\n","Epoch: 00 [ 5671/20164 ( 28%)], Train Loss: 0.60599\n","Epoch: 00 [ 5681/20164 ( 28%)], Train Loss: 0.60569\n","Epoch: 00 [ 5691/20164 ( 28%)], Train Loss: 0.60609\n","Epoch: 00 [ 5701/20164 ( 28%)], Train Loss: 0.60561\n","Epoch: 00 [ 5711/20164 ( 28%)], Train Loss: 0.60502\n","Epoch: 00 [ 5721/20164 ( 28%)], Train Loss: 0.60456\n","Epoch: 00 [ 5731/20164 ( 28%)], Train Loss: 0.60450\n","Epoch: 00 [ 5741/20164 ( 28%)], Train Loss: 0.60650\n","Epoch: 00 [ 5751/20164 ( 29%)], Train Loss: 0.60603\n","Epoch: 00 [ 5761/20164 ( 29%)], Train Loss: 0.60559\n","Epoch: 00 [ 5771/20164 ( 29%)], Train Loss: 0.60556\n","Epoch: 00 [ 5781/20164 ( 29%)], Train Loss: 0.60584\n","Epoch: 00 [ 5791/20164 ( 29%)], Train Loss: 0.60513\n","Epoch: 00 [ 5801/20164 ( 29%)], Train Loss: 0.60474\n","Epoch: 00 [ 5811/20164 ( 29%)], Train Loss: 0.60469\n","Epoch: 00 [ 5821/20164 ( 29%)], Train Loss: 0.60496\n","Epoch: 00 [ 5831/20164 ( 29%)], Train Loss: 0.60423\n","Epoch: 00 [ 5841/20164 ( 29%)], Train Loss: 0.60346\n","Epoch: 00 [ 5851/20164 ( 29%)], Train Loss: 0.60352\n","Epoch: 00 [ 5861/20164 ( 29%)], Train Loss: 0.60315\n","Epoch: 00 [ 5871/20164 ( 29%)], Train Loss: 0.60359\n","Epoch: 00 [ 5881/20164 ( 29%)], Train Loss: 0.60391\n","Epoch: 00 [ 5891/20164 ( 29%)], Train Loss: 0.60340\n","Epoch: 00 [ 5901/20164 ( 29%)], Train Loss: 0.60342\n","Epoch: 00 [ 5911/20164 ( 29%)], Train Loss: 0.60271\n","Epoch: 00 [ 5921/20164 ( 29%)], Train Loss: 0.60221\n","Epoch: 00 [ 5931/20164 ( 29%)], Train Loss: 0.60215\n","Epoch: 00 [ 5941/20164 ( 29%)], Train Loss: 0.60173\n","Epoch: 00 [ 5951/20164 ( 30%)], Train Loss: 0.60216\n","Epoch: 00 [ 5961/20164 ( 30%)], Train Loss: 0.60264\n","Epoch: 00 [ 5971/20164 ( 30%)], Train Loss: 0.60312\n","Epoch: 00 [ 5981/20164 ( 30%)], Train Loss: 0.60314\n","Epoch: 00 [ 5991/20164 ( 30%)], Train Loss: 0.60366\n","Epoch: 00 [ 6001/20164 ( 30%)], Train Loss: 0.60360\n","Epoch: 00 [ 6011/20164 ( 30%)], Train Loss: 0.60352\n","Epoch: 00 [ 6021/20164 ( 30%)], Train Loss: 0.60275\n","Epoch: 00 [ 6031/20164 ( 30%)], Train Loss: 0.60178\n","Epoch: 00 [ 6041/20164 ( 30%)], Train Loss: 0.60141\n","Epoch: 00 [ 6051/20164 ( 30%)], Train Loss: 0.60092\n","Epoch: 00 [ 6061/20164 ( 30%)], Train Loss: 0.60026\n","Epoch: 00 [ 6071/20164 ( 30%)], Train Loss: 0.59989\n","Epoch: 00 [ 6081/20164 ( 30%)], Train Loss: 0.59968\n","Epoch: 00 [ 6091/20164 ( 30%)], Train Loss: 0.59898\n","Epoch: 00 [ 6101/20164 ( 30%)], Train Loss: 0.59875\n","Epoch: 00 [ 6111/20164 ( 30%)], Train Loss: 0.59867\n","Epoch: 00 [ 6121/20164 ( 30%)], Train Loss: 0.59821\n","Epoch: 00 [ 6131/20164 ( 30%)], Train Loss: 0.59801\n","Epoch: 00 [ 6141/20164 ( 30%)], Train Loss: 0.59746\n","Epoch: 00 [ 6151/20164 ( 31%)], Train Loss: 0.59694\n","Epoch: 00 [ 6161/20164 ( 31%)], Train Loss: 0.59610\n","Epoch: 00 [ 6171/20164 ( 31%)], Train Loss: 0.59736\n","Epoch: 00 [ 6181/20164 ( 31%)], Train Loss: 0.59815\n","Epoch: 00 [ 6191/20164 ( 31%)], Train Loss: 0.59818\n","Epoch: 00 [ 6201/20164 ( 31%)], Train Loss: 0.59777\n","Epoch: 00 [ 6211/20164 ( 31%)], Train Loss: 0.59758\n","Epoch: 00 [ 6221/20164 ( 31%)], Train Loss: 0.59745\n","Epoch: 00 [ 6231/20164 ( 31%)], Train Loss: 0.59675\n","Epoch: 00 [ 6241/20164 ( 31%)], Train Loss: 0.59607\n","Epoch: 00 [ 6251/20164 ( 31%)], Train Loss: 0.59625\n","Epoch: 00 [ 6261/20164 ( 31%)], Train Loss: 0.59646\n","Epoch: 00 [ 6271/20164 ( 31%)], Train Loss: 0.59621\n","Epoch: 00 [ 6281/20164 ( 31%)], Train Loss: 0.59578\n","Epoch: 00 [ 6291/20164 ( 31%)], Train Loss: 0.59538\n","Epoch: 00 [ 6301/20164 ( 31%)], Train Loss: 0.59583\n","Epoch: 00 [ 6311/20164 ( 31%)], Train Loss: 0.59554\n","Epoch: 00 [ 6321/20164 ( 31%)], Train Loss: 0.59539\n","Epoch: 00 [ 6331/20164 ( 31%)], Train Loss: 0.59535\n","Epoch: 00 [ 6341/20164 ( 31%)], Train Loss: 0.59497\n","Epoch: 00 [ 6351/20164 ( 31%)], Train Loss: 0.59454\n","Epoch: 00 [ 6361/20164 ( 32%)], Train Loss: 0.59440\n","Epoch: 00 [ 6371/20164 ( 32%)], Train Loss: 0.59367\n","Epoch: 00 [ 6381/20164 ( 32%)], Train Loss: 0.59302\n","Epoch: 00 [ 6391/20164 ( 32%)], Train Loss: 0.59286\n","Epoch: 00 [ 6401/20164 ( 32%)], Train Loss: 0.59225\n","Epoch: 00 [ 6411/20164 ( 32%)], Train Loss: 0.59195\n","Epoch: 00 [ 6421/20164 ( 32%)], Train Loss: 0.59218\n","Epoch: 00 [ 6431/20164 ( 32%)], Train Loss: 0.59210\n","Epoch: 00 [ 6441/20164 ( 32%)], Train Loss: 0.59231\n","Epoch: 00 [ 6451/20164 ( 32%)], Train Loss: 0.59159\n","Epoch: 00 [ 6461/20164 ( 32%)], Train Loss: 0.59174\n","Epoch: 00 [ 6471/20164 ( 32%)], Train Loss: 0.59140\n","Epoch: 00 [ 6481/20164 ( 32%)], Train Loss: 0.59138\n","Epoch: 00 [ 6491/20164 ( 32%)], Train Loss: 0.59134\n","Epoch: 00 [ 6501/20164 ( 32%)], Train Loss: 0.59099\n","Epoch: 00 [ 6511/20164 ( 32%)], Train Loss: 0.59057\n","Epoch: 00 [ 6521/20164 ( 32%)], Train Loss: 0.59027\n","Epoch: 00 [ 6531/20164 ( 32%)], Train Loss: 0.58944\n","Epoch: 00 [ 6541/20164 ( 32%)], Train Loss: 0.58890\n","Epoch: 00 [ 6551/20164 ( 32%)], Train Loss: 0.58997\n","Epoch: 00 [ 6561/20164 ( 33%)], Train Loss: 0.58972\n","Epoch: 00 [ 6571/20164 ( 33%)], Train Loss: 0.58956\n","Epoch: 00 [ 6581/20164 ( 33%)], Train Loss: 0.58957\n","Epoch: 00 [ 6591/20164 ( 33%)], Train Loss: 0.58956\n","Epoch: 00 [ 6601/20164 ( 33%)], Train Loss: 0.58953\n","Epoch: 00 [ 6611/20164 ( 33%)], Train Loss: 0.58885\n","Epoch: 00 [ 6621/20164 ( 33%)], Train Loss: 0.58807\n","Epoch: 00 [ 6631/20164 ( 33%)], Train Loss: 0.58813\n","Epoch: 00 [ 6641/20164 ( 33%)], Train Loss: 0.58792\n","Epoch: 00 [ 6651/20164 ( 33%)], Train Loss: 0.58753\n","Epoch: 00 [ 6661/20164 ( 33%)], Train Loss: 0.58755\n","Epoch: 00 [ 6671/20164 ( 33%)], Train Loss: 0.58689\n","Epoch: 00 [ 6681/20164 ( 33%)], Train Loss: 0.58607\n","Epoch: 00 [ 6691/20164 ( 33%)], Train Loss: 0.58540\n","Epoch: 00 [ 6701/20164 ( 33%)], Train Loss: 0.58509\n","Epoch: 00 [ 6711/20164 ( 33%)], Train Loss: 0.58530\n","Epoch: 00 [ 6721/20164 ( 33%)], Train Loss: 0.58534\n","Epoch: 00 [ 6731/20164 ( 33%)], Train Loss: 0.58525\n","Epoch: 00 [ 6741/20164 ( 33%)], Train Loss: 0.58485\n","Epoch: 00 [ 6751/20164 ( 33%)], Train Loss: 0.58421\n","Epoch: 00 [ 6761/20164 ( 34%)], Train Loss: 0.58503\n","Epoch: 00 [ 6771/20164 ( 34%)], Train Loss: 0.58469\n","Epoch: 00 [ 6781/20164 ( 34%)], Train Loss: 0.58480\n","Epoch: 00 [ 6791/20164 ( 34%)], Train Loss: 0.58458\n","Epoch: 00 [ 6801/20164 ( 34%)], Train Loss: 0.58400\n","Epoch: 00 [ 6811/20164 ( 34%)], Train Loss: 0.58369\n","Epoch: 00 [ 6821/20164 ( 34%)], Train Loss: 0.58345\n","Epoch: 00 [ 6831/20164 ( 34%)], Train Loss: 0.58322\n","Epoch: 00 [ 6841/20164 ( 34%)], Train Loss: 0.58276\n","Epoch: 00 [ 6851/20164 ( 34%)], Train Loss: 0.58260\n","Epoch: 00 [ 6861/20164 ( 34%)], Train Loss: 0.58191\n","Epoch: 00 [ 6871/20164 ( 34%)], Train Loss: 0.58156\n","Epoch: 00 [ 6881/20164 ( 34%)], Train Loss: 0.58213\n","Epoch: 00 [ 6891/20164 ( 34%)], Train Loss: 0.58208\n","Epoch: 00 [ 6901/20164 ( 34%)], Train Loss: 0.58139\n","Epoch: 00 [ 6911/20164 ( 34%)], Train Loss: 0.58203\n","Epoch: 00 [ 6921/20164 ( 34%)], Train Loss: 0.58156\n","Epoch: 00 [ 6931/20164 ( 34%)], Train Loss: 0.58122\n","Epoch: 00 [ 6941/20164 ( 34%)], Train Loss: 0.58138\n","Epoch: 00 [ 6951/20164 ( 34%)], Train Loss: 0.58077\n","Epoch: 00 [ 6961/20164 ( 35%)], Train Loss: 0.58144\n","Epoch: 00 [ 6971/20164 ( 35%)], Train Loss: 0.58073\n","Epoch: 00 [ 6981/20164 ( 35%)], Train Loss: 0.58042\n","Epoch: 00 [ 6991/20164 ( 35%)], Train Loss: 0.58017\n","Epoch: 00 [ 7001/20164 ( 35%)], Train Loss: 0.58004\n","Epoch: 00 [ 7011/20164 ( 35%)], Train Loss: 0.58050\n","Epoch: 00 [ 7021/20164 ( 35%)], Train Loss: 0.58109\n","Epoch: 00 [ 7031/20164 ( 35%)], Train Loss: 0.58082\n","Epoch: 00 [ 7041/20164 ( 35%)], Train Loss: 0.58061\n","Epoch: 00 [ 7051/20164 ( 35%)], Train Loss: 0.58021\n","Epoch: 00 [ 7061/20164 ( 35%)], Train Loss: 0.58028\n","Epoch: 00 [ 7071/20164 ( 35%)], Train Loss: 0.58066\n","Epoch: 00 [ 7081/20164 ( 35%)], Train Loss: 0.58039\n","Epoch: 00 [ 7091/20164 ( 35%)], Train Loss: 0.58023\n","Epoch: 00 [ 7101/20164 ( 35%)], Train Loss: 0.58027\n","Epoch: 00 [ 7111/20164 ( 35%)], Train Loss: 0.57985\n","Epoch: 00 [ 7121/20164 ( 35%)], Train Loss: 0.57931\n","Epoch: 00 [ 7131/20164 ( 35%)], Train Loss: 0.57979\n","Epoch: 00 [ 7141/20164 ( 35%)], Train Loss: 0.57919\n","Epoch: 00 [ 7151/20164 ( 35%)], Train Loss: 0.57911\n","Epoch: 00 [ 7161/20164 ( 36%)], Train Loss: 0.57877\n","Epoch: 00 [ 7171/20164 ( 36%)], Train Loss: 0.57870\n","Epoch: 00 [ 7181/20164 ( 36%)], Train Loss: 0.57869\n","Epoch: 00 [ 7191/20164 ( 36%)], Train Loss: 0.57821\n","Epoch: 00 [ 7201/20164 ( 36%)], Train Loss: 0.57902\n","Epoch: 00 [ 7211/20164 ( 36%)], Train Loss: 0.57890\n","Epoch: 00 [ 7221/20164 ( 36%)], Train Loss: 0.57927\n","Epoch: 00 [ 7231/20164 ( 36%)], Train Loss: 0.57881\n","Epoch: 00 [ 7241/20164 ( 36%)], Train Loss: 0.57826\n","Epoch: 00 [ 7251/20164 ( 36%)], Train Loss: 0.57831\n","Epoch: 00 [ 7261/20164 ( 36%)], Train Loss: 0.57792\n","Epoch: 00 [ 7271/20164 ( 36%)], Train Loss: 0.57804\n","Epoch: 00 [ 7281/20164 ( 36%)], Train Loss: 0.57751\n","Epoch: 00 [ 7291/20164 ( 36%)], Train Loss: 0.57735\n","Epoch: 00 [ 7301/20164 ( 36%)], Train Loss: 0.57719\n","Epoch: 00 [ 7311/20164 ( 36%)], Train Loss: 0.57646\n","Epoch: 00 [ 7321/20164 ( 36%)], Train Loss: 0.57608\n","Epoch: 00 [ 7331/20164 ( 36%)], Train Loss: 0.57567\n","Epoch: 00 [ 7341/20164 ( 36%)], Train Loss: 0.57604\n","Epoch: 00 [ 7351/20164 ( 36%)], Train Loss: 0.57643\n","Epoch: 00 [ 7361/20164 ( 37%)], Train Loss: 0.57585\n","Epoch: 00 [ 7371/20164 ( 37%)], Train Loss: 0.57573\n","Epoch: 00 [ 7381/20164 ( 37%)], Train Loss: 0.57515\n","Epoch: 00 [ 7391/20164 ( 37%)], Train Loss: 0.57501\n","Epoch: 00 [ 7401/20164 ( 37%)], Train Loss: 0.57447\n","Epoch: 00 [ 7411/20164 ( 37%)], Train Loss: 0.57391\n","Epoch: 00 [ 7421/20164 ( 37%)], Train Loss: 0.57387\n","Epoch: 00 [ 7431/20164 ( 37%)], Train Loss: 0.57430\n","Epoch: 00 [ 7441/20164 ( 37%)], Train Loss: 0.57386\n","Epoch: 00 [ 7451/20164 ( 37%)], Train Loss: 0.57409\n","Epoch: 00 [ 7461/20164 ( 37%)], Train Loss: 0.57435\n","Epoch: 00 [ 7471/20164 ( 37%)], Train Loss: 0.57433\n","Epoch: 00 [ 7481/20164 ( 37%)], Train Loss: 0.57398\n","Epoch: 00 [ 7491/20164 ( 37%)], Train Loss: 0.57373\n","Epoch: 00 [ 7501/20164 ( 37%)], Train Loss: 0.57374\n","Epoch: 00 [ 7511/20164 ( 37%)], Train Loss: 0.57363\n","Epoch: 00 [ 7521/20164 ( 37%)], Train Loss: 0.57324\n","Epoch: 00 [ 7531/20164 ( 37%)], Train Loss: 0.57283\n","Epoch: 00 [ 7541/20164 ( 37%)], Train Loss: 0.57212\n","Epoch: 00 [ 7551/20164 ( 37%)], Train Loss: 0.57219\n","Epoch: 00 [ 7561/20164 ( 37%)], Train Loss: 0.57174\n","Epoch: 00 [ 7571/20164 ( 38%)], Train Loss: 0.57203\n","Epoch: 00 [ 7581/20164 ( 38%)], Train Loss: 0.57245\n","Epoch: 00 [ 7591/20164 ( 38%)], Train Loss: 0.57219\n","Epoch: 00 [ 7601/20164 ( 38%)], Train Loss: 0.57212\n","Epoch: 00 [ 7611/20164 ( 38%)], Train Loss: 0.57198\n","Epoch: 00 [ 7621/20164 ( 38%)], Train Loss: 0.57140\n","Epoch: 00 [ 7631/20164 ( 38%)], Train Loss: 0.57127\n","Epoch: 00 [ 7641/20164 ( 38%)], Train Loss: 0.57090\n","Epoch: 00 [ 7651/20164 ( 38%)], Train Loss: 0.57042\n","Epoch: 00 [ 7661/20164 ( 38%)], Train Loss: 0.57026\n","Epoch: 00 [ 7671/20164 ( 38%)], Train Loss: 0.56999\n","Epoch: 00 [ 7681/20164 ( 38%)], Train Loss: 0.56960\n","Epoch: 00 [ 7691/20164 ( 38%)], Train Loss: 0.56940\n","Epoch: 00 [ 7701/20164 ( 38%)], Train Loss: 0.56919\n","Epoch: 00 [ 7711/20164 ( 38%)], Train Loss: 0.56897\n","Epoch: 00 [ 7721/20164 ( 38%)], Train Loss: 0.56961\n","Epoch: 00 [ 7731/20164 ( 38%)], Train Loss: 0.56922\n","Epoch: 00 [ 7741/20164 ( 38%)], Train Loss: 0.56888\n","Epoch: 00 [ 7751/20164 ( 38%)], Train Loss: 0.56835\n","Epoch: 00 [ 7761/20164 ( 38%)], Train Loss: 0.56827\n","Epoch: 00 [ 7771/20164 ( 39%)], Train Loss: 0.56805\n","Epoch: 00 [ 7781/20164 ( 39%)], Train Loss: 0.56769\n","Epoch: 00 [ 7791/20164 ( 39%)], Train Loss: 0.56784\n","Epoch: 00 [ 7801/20164 ( 39%)], Train Loss: 0.56730\n","Epoch: 00 [ 7811/20164 ( 39%)], Train Loss: 0.56698\n","Epoch: 00 [ 7821/20164 ( 39%)], Train Loss: 0.56668\n","Epoch: 00 [ 7831/20164 ( 39%)], Train Loss: 0.56657\n","Epoch: 00 [ 7841/20164 ( 39%)], Train Loss: 0.56701\n","Epoch: 00 [ 7851/20164 ( 39%)], Train Loss: 0.56705\n","Epoch: 00 [ 7861/20164 ( 39%)], Train Loss: 0.56673\n","Epoch: 00 [ 7871/20164 ( 39%)], Train Loss: 0.56718\n","Epoch: 00 [ 7881/20164 ( 39%)], Train Loss: 0.56659\n","Epoch: 00 [ 7891/20164 ( 39%)], Train Loss: 0.56666\n","Epoch: 00 [ 7901/20164 ( 39%)], Train Loss: 0.56676\n","Epoch: 00 [ 7911/20164 ( 39%)], Train Loss: 0.56647\n","Epoch: 00 [ 7921/20164 ( 39%)], Train Loss: 0.56623\n","Epoch: 00 [ 7931/20164 ( 39%)], Train Loss: 0.56664\n","Epoch: 00 [ 7941/20164 ( 39%)], Train Loss: 0.56623\n","Epoch: 00 [ 7951/20164 ( 39%)], Train Loss: 0.56643\n","Epoch: 00 [ 7961/20164 ( 39%)], Train Loss: 0.56624\n","Epoch: 00 [ 7971/20164 ( 40%)], Train Loss: 0.56596\n","Epoch: 00 [ 7981/20164 ( 40%)], Train Loss: 0.56580\n","Epoch: 00 [ 7991/20164 ( 40%)], Train Loss: 0.56534\n","Epoch: 00 [ 8001/20164 ( 40%)], Train Loss: 0.56564\n","Epoch: 00 [ 8011/20164 ( 40%)], Train Loss: 0.56553\n","Epoch: 00 [ 8021/20164 ( 40%)], Train Loss: 0.56504\n","Epoch: 00 [ 8031/20164 ( 40%)], Train Loss: 0.56476\n","Epoch: 00 [ 8041/20164 ( 40%)], Train Loss: 0.56475\n","Epoch: 00 [ 8051/20164 ( 40%)], Train Loss: 0.56434\n","Epoch: 00 [ 8061/20164 ( 40%)], Train Loss: 0.56417\n","Epoch: 00 [ 8071/20164 ( 40%)], Train Loss: 0.56439\n","Epoch: 00 [ 8081/20164 ( 40%)], Train Loss: 0.56442\n","Epoch: 00 [ 8091/20164 ( 40%)], Train Loss: 0.56426\n","Epoch: 00 [ 8101/20164 ( 40%)], Train Loss: 0.56466\n","Epoch: 00 [ 8111/20164 ( 40%)], Train Loss: 0.56442\n","Epoch: 00 [ 8121/20164 ( 40%)], Train Loss: 0.56438\n","Epoch: 00 [ 8131/20164 ( 40%)], Train Loss: 0.56448\n","Epoch: 00 [ 8141/20164 ( 40%)], Train Loss: 0.56448\n","Epoch: 00 [ 8151/20164 ( 40%)], Train Loss: 0.56471\n","Epoch: 00 [ 8161/20164 ( 40%)], Train Loss: 0.56478\n","Epoch: 00 [ 8171/20164 ( 41%)], Train Loss: 0.56446\n","Epoch: 00 [ 8181/20164 ( 41%)], Train Loss: 0.56398\n","Epoch: 00 [ 8191/20164 ( 41%)], Train Loss: 0.56403\n","Epoch: 00 [ 8201/20164 ( 41%)], Train Loss: 0.56389\n","Epoch: 00 [ 8211/20164 ( 41%)], Train Loss: 0.56327\n","Epoch: 00 [ 8221/20164 ( 41%)], Train Loss: 0.56395\n","Epoch: 00 [ 8231/20164 ( 41%)], Train Loss: 0.56349\n","Epoch: 00 [ 8241/20164 ( 41%)], Train Loss: 0.56357\n","Epoch: 00 [ 8251/20164 ( 41%)], Train Loss: 0.56338\n","Epoch: 00 [ 8261/20164 ( 41%)], Train Loss: 0.56334\n","Epoch: 00 [ 8271/20164 ( 41%)], Train Loss: 0.56297\n","Epoch: 00 [ 8281/20164 ( 41%)], Train Loss: 0.56278\n","Epoch: 00 [ 8291/20164 ( 41%)], Train Loss: 0.56340\n","Epoch: 00 [ 8301/20164 ( 41%)], Train Loss: 0.56293\n","Epoch: 00 [ 8311/20164 ( 41%)], Train Loss: 0.56248\n","Epoch: 00 [ 8321/20164 ( 41%)], Train Loss: 0.56203\n","Epoch: 00 [ 8331/20164 ( 41%)], Train Loss: 0.56214\n","Epoch: 00 [ 8341/20164 ( 41%)], Train Loss: 0.56250\n","Epoch: 00 [ 8351/20164 ( 41%)], Train Loss: 0.56188\n","Epoch: 00 [ 8361/20164 ( 41%)], Train Loss: 0.56178\n","Epoch: 00 [ 8371/20164 ( 42%)], Train Loss: 0.56159\n","Epoch: 00 [ 8381/20164 ( 42%)], Train Loss: 0.56108\n","Epoch: 00 [ 8391/20164 ( 42%)], Train Loss: 0.56097\n","Epoch: 00 [ 8401/20164 ( 42%)], Train Loss: 0.56093\n","Epoch: 00 [ 8411/20164 ( 42%)], Train Loss: 0.56055\n","Epoch: 00 [ 8421/20164 ( 42%)], Train Loss: 0.56011\n","Epoch: 00 [ 8431/20164 ( 42%)], Train Loss: 0.55984\n","Epoch: 00 [ 8441/20164 ( 42%)], Train Loss: 0.55952\n","Epoch: 00 [ 8451/20164 ( 42%)], Train Loss: 0.55922\n","Epoch: 00 [ 8461/20164 ( 42%)], Train Loss: 0.55868\n","Epoch: 00 [ 8471/20164 ( 42%)], Train Loss: 0.55835\n","Epoch: 00 [ 8481/20164 ( 42%)], Train Loss: 0.55798\n","Epoch: 00 [ 8491/20164 ( 42%)], Train Loss: 0.55758\n","Epoch: 00 [ 8501/20164 ( 42%)], Train Loss: 0.55708\n","Epoch: 00 [ 8511/20164 ( 42%)], Train Loss: 0.55668\n","Epoch: 00 [ 8521/20164 ( 42%)], Train Loss: 0.55625\n","Epoch: 00 [ 8531/20164 ( 42%)], Train Loss: 0.55587\n","Epoch: 00 [ 8541/20164 ( 42%)], Train Loss: 0.55535\n","Epoch: 00 [ 8551/20164 ( 42%)], Train Loss: 0.55539\n","Epoch: 00 [ 8561/20164 ( 42%)], Train Loss: 0.55592\n","Epoch: 00 [ 8571/20164 ( 43%)], Train Loss: 0.55578\n","Epoch: 00 [ 8581/20164 ( 43%)], Train Loss: 0.55580\n","Epoch: 00 [ 8591/20164 ( 43%)], Train Loss: 0.55562\n","Epoch: 00 [ 8601/20164 ( 43%)], Train Loss: 0.55546\n","Epoch: 00 [ 8611/20164 ( 43%)], Train Loss: 0.55522\n","Epoch: 00 [ 8621/20164 ( 43%)], Train Loss: 0.55498\n","Epoch: 00 [ 8631/20164 ( 43%)], Train Loss: 0.55503\n","Epoch: 00 [ 8641/20164 ( 43%)], Train Loss: 0.55477\n","Epoch: 00 [ 8651/20164 ( 43%)], Train Loss: 0.55474\n","Epoch: 00 [ 8661/20164 ( 43%)], Train Loss: 0.55448\n","Epoch: 00 [ 8671/20164 ( 43%)], Train Loss: 0.55487\n","Epoch: 00 [ 8681/20164 ( 43%)], Train Loss: 0.55448\n","Epoch: 00 [ 8691/20164 ( 43%)], Train Loss: 0.55412\n","Epoch: 00 [ 8701/20164 ( 43%)], Train Loss: 0.55382\n","Epoch: 00 [ 8711/20164 ( 43%)], Train Loss: 0.55364\n","Epoch: 00 [ 8721/20164 ( 43%)], Train Loss: 0.55339\n","Epoch: 00 [ 8731/20164 ( 43%)], Train Loss: 0.55324\n","Epoch: 00 [ 8741/20164 ( 43%)], Train Loss: 0.55292\n","Epoch: 00 [ 8751/20164 ( 43%)], Train Loss: 0.55342\n","Epoch: 00 [ 8761/20164 ( 43%)], Train Loss: 0.55354\n","Epoch: 00 [ 8771/20164 ( 43%)], Train Loss: 0.55343\n","Epoch: 00 [ 8781/20164 ( 44%)], Train Loss: 0.55315\n","Epoch: 00 [ 8791/20164 ( 44%)], Train Loss: 0.55281\n","Epoch: 00 [ 8801/20164 ( 44%)], Train Loss: 0.55239\n","Epoch: 00 [ 8811/20164 ( 44%)], Train Loss: 0.55203\n","Epoch: 00 [ 8821/20164 ( 44%)], Train Loss: 0.55181\n","Epoch: 00 [ 8831/20164 ( 44%)], Train Loss: 0.55135\n","Epoch: 00 [ 8841/20164 ( 44%)], Train Loss: 0.55118\n","Epoch: 00 [ 8851/20164 ( 44%)], Train Loss: 0.55072\n","Epoch: 00 [ 8861/20164 ( 44%)], Train Loss: 0.55087\n","Epoch: 00 [ 8871/20164 ( 44%)], Train Loss: 0.55065\n","Epoch: 00 [ 8881/20164 ( 44%)], Train Loss: 0.55052\n","Epoch: 00 [ 8891/20164 ( 44%)], Train Loss: 0.55053\n","Epoch: 00 [ 8901/20164 ( 44%)], Train Loss: 0.55097\n","Epoch: 00 [ 8911/20164 ( 44%)], Train Loss: 0.55097\n","Epoch: 00 [ 8921/20164 ( 44%)], Train Loss: 0.55072\n","Epoch: 00 [ 8931/20164 ( 44%)], Train Loss: 0.55051\n","Epoch: 00 [ 8941/20164 ( 44%)], Train Loss: 0.55025\n","Epoch: 00 [ 8951/20164 ( 44%)], Train Loss: 0.54971\n","Epoch: 00 [ 8961/20164 ( 44%)], Train Loss: 0.54928\n","Epoch: 00 [ 8971/20164 ( 44%)], Train Loss: 0.54923\n","Epoch: 00 [ 8981/20164 ( 45%)], Train Loss: 0.54911\n","Epoch: 00 [ 8991/20164 ( 45%)], Train Loss: 0.54890\n","Epoch: 00 [ 9001/20164 ( 45%)], Train Loss: 0.54885\n","Epoch: 00 [ 9011/20164 ( 45%)], Train Loss: 0.54844\n","Epoch: 00 [ 9021/20164 ( 45%)], Train Loss: 0.54814\n","Epoch: 00 [ 9031/20164 ( 45%)], Train Loss: 0.54810\n","Epoch: 00 [ 9041/20164 ( 45%)], Train Loss: 0.54776\n","Epoch: 00 [ 9051/20164 ( 45%)], Train Loss: 0.54793\n","Epoch: 00 [ 9061/20164 ( 45%)], Train Loss: 0.54766\n","Epoch: 00 [ 9071/20164 ( 45%)], Train Loss: 0.54734\n","Epoch: 00 [ 9081/20164 ( 45%)], Train Loss: 0.54786\n","Epoch: 00 [ 9091/20164 ( 45%)], Train Loss: 0.54777\n","Epoch: 00 [ 9101/20164 ( 45%)], Train Loss: 0.54751\n","Epoch: 00 [ 9111/20164 ( 45%)], Train Loss: 0.54747\n","Epoch: 00 [ 9121/20164 ( 45%)], Train Loss: 0.54738\n","Epoch: 00 [ 9131/20164 ( 45%)], Train Loss: 0.54713\n","Epoch: 00 [ 9141/20164 ( 45%)], Train Loss: 0.54693\n","Epoch: 00 [ 9151/20164 ( 45%)], Train Loss: 0.54670\n","Epoch: 00 [ 9161/20164 ( 45%)], Train Loss: 0.54655\n","Epoch: 00 [ 9171/20164 ( 45%)], Train Loss: 0.54647\n","Epoch: 00 [ 9181/20164 ( 46%)], Train Loss: 0.54633\n","Epoch: 00 [ 9191/20164 ( 46%)], Train Loss: 0.54624\n","Epoch: 00 [ 9201/20164 ( 46%)], Train Loss: 0.54611\n","Epoch: 00 [ 9211/20164 ( 46%)], Train Loss: 0.54577\n","Epoch: 00 [ 9221/20164 ( 46%)], Train Loss: 0.54534\n","Epoch: 00 [ 9231/20164 ( 46%)], Train Loss: 0.54549\n","Epoch: 00 [ 9241/20164 ( 46%)], Train Loss: 0.54512\n","Epoch: 00 [ 9251/20164 ( 46%)], Train Loss: 0.54476\n","Epoch: 00 [ 9261/20164 ( 46%)], Train Loss: 0.54478\n","Epoch: 00 [ 9271/20164 ( 46%)], Train Loss: 0.54450\n","Epoch: 00 [ 9281/20164 ( 46%)], Train Loss: 0.54398\n","Epoch: 00 [ 9291/20164 ( 46%)], Train Loss: 0.54423\n","Epoch: 00 [ 9301/20164 ( 46%)], Train Loss: 0.54383\n","Epoch: 00 [ 9311/20164 ( 46%)], Train Loss: 0.54364\n","Epoch: 00 [ 9321/20164 ( 46%)], Train Loss: 0.54354\n","Epoch: 00 [ 9331/20164 ( 46%)], Train Loss: 0.54351\n","Epoch: 00 [ 9341/20164 ( 46%)], Train Loss: 0.54306\n","Epoch: 00 [ 9351/20164 ( 46%)], Train Loss: 0.54272\n","Epoch: 00 [ 9361/20164 ( 46%)], Train Loss: 0.54293\n","Epoch: 00 [ 9371/20164 ( 46%)], Train Loss: 0.54312\n","Epoch: 00 [ 9381/20164 ( 47%)], Train Loss: 0.54295\n","Epoch: 00 [ 9391/20164 ( 47%)], Train Loss: 0.54301\n","Epoch: 00 [ 9401/20164 ( 47%)], Train Loss: 0.54304\n","Epoch: 00 [ 9411/20164 ( 47%)], Train Loss: 0.54301\n","Epoch: 00 [ 9421/20164 ( 47%)], Train Loss: 0.54275\n","Epoch: 00 [ 9431/20164 ( 47%)], Train Loss: 0.54259\n","Epoch: 00 [ 9441/20164 ( 47%)], Train Loss: 0.54286\n","Epoch: 00 [ 9451/20164 ( 47%)], Train Loss: 0.54288\n","Epoch: 00 [ 9461/20164 ( 47%)], Train Loss: 0.54281\n","Epoch: 00 [ 9471/20164 ( 47%)], Train Loss: 0.54315\n","Epoch: 00 [ 9481/20164 ( 47%)], Train Loss: 0.54303\n","Epoch: 00 [ 9491/20164 ( 47%)], Train Loss: 0.54280\n","Epoch: 00 [ 9501/20164 ( 47%)], Train Loss: 0.54258\n","Epoch: 00 [ 9511/20164 ( 47%)], Train Loss: 0.54204\n","Epoch: 00 [ 9521/20164 ( 47%)], Train Loss: 0.54246\n","Epoch: 00 [ 9531/20164 ( 47%)], Train Loss: 0.54233\n","Epoch: 00 [ 9541/20164 ( 47%)], Train Loss: 0.54199\n","Epoch: 00 [ 9551/20164 ( 47%)], Train Loss: 0.54174\n","Epoch: 00 [ 9561/20164 ( 47%)], Train Loss: 0.54213\n","Epoch: 00 [ 9571/20164 ( 47%)], Train Loss: 0.54218\n","Epoch: 00 [ 9581/20164 ( 48%)], Train Loss: 0.54202\n","Epoch: 00 [ 9591/20164 ( 48%)], Train Loss: 0.54158\n","Epoch: 00 [ 9601/20164 ( 48%)], Train Loss: 0.54133\n","Epoch: 00 [ 9611/20164 ( 48%)], Train Loss: 0.54121\n","Epoch: 00 [ 9621/20164 ( 48%)], Train Loss: 0.54085\n","Epoch: 00 [ 9631/20164 ( 48%)], Train Loss: 0.54059\n","Epoch: 00 [ 9641/20164 ( 48%)], Train Loss: 0.54105\n","Epoch: 00 [ 9651/20164 ( 48%)], Train Loss: 0.54069\n","Epoch: 00 [ 9661/20164 ( 48%)], Train Loss: 0.54083\n","Epoch: 00 [ 9671/20164 ( 48%)], Train Loss: 0.54114\n","Epoch: 00 [ 9681/20164 ( 48%)], Train Loss: 0.54096\n","Epoch: 00 [ 9691/20164 ( 48%)], Train Loss: 0.54121\n","Epoch: 00 [ 9701/20164 ( 48%)], Train Loss: 0.54108\n","Epoch: 00 [ 9711/20164 ( 48%)], Train Loss: 0.54126\n","Epoch: 00 [ 9721/20164 ( 48%)], Train Loss: 0.54097\n","Epoch: 00 [ 9731/20164 ( 48%)], Train Loss: 0.54145\n","Epoch: 00 [ 9741/20164 ( 48%)], Train Loss: 0.54162\n","Epoch: 00 [ 9751/20164 ( 48%)], Train Loss: 0.54165\n","Epoch: 00 [ 9761/20164 ( 48%)], Train Loss: 0.54178\n","Epoch: 00 [ 9771/20164 ( 48%)], Train Loss: 0.54156\n","Epoch: 00 [ 9781/20164 ( 49%)], Train Loss: 0.54155\n","Epoch: 00 [ 9791/20164 ( 49%)], Train Loss: 0.54148\n","Epoch: 00 [ 9801/20164 ( 49%)], Train Loss: 0.54134\n","Epoch: 00 [ 9811/20164 ( 49%)], Train Loss: 0.54162\n","Epoch: 00 [ 9821/20164 ( 49%)], Train Loss: 0.54202\n","Epoch: 00 [ 9831/20164 ( 49%)], Train Loss: 0.54158\n","Epoch: 00 [ 9841/20164 ( 49%)], Train Loss: 0.54146\n","Epoch: 00 [ 9851/20164 ( 49%)], Train Loss: 0.54108\n","Epoch: 00 [ 9861/20164 ( 49%)], Train Loss: 0.54068\n","Epoch: 00 [ 9871/20164 ( 49%)], Train Loss: 0.54092\n","Epoch: 00 [ 9881/20164 ( 49%)], Train Loss: 0.54083\n","Epoch: 00 [ 9891/20164 ( 49%)], Train Loss: 0.54076\n","Epoch: 00 [ 9901/20164 ( 49%)], Train Loss: 0.54029\n","Epoch: 00 [ 9911/20164 ( 49%)], Train Loss: 0.54012\n","Epoch: 00 [ 9921/20164 ( 49%)], Train Loss: 0.53994\n","Epoch: 00 [ 9931/20164 ( 49%)], Train Loss: 0.53992\n","Epoch: 00 [ 9941/20164 ( 49%)], Train Loss: 0.53949\n","Epoch: 00 [ 9951/20164 ( 49%)], Train Loss: 0.53926\n","Epoch: 00 [ 9961/20164 ( 49%)], Train Loss: 0.53917\n","Epoch: 00 [ 9971/20164 ( 49%)], Train Loss: 0.53871\n","Epoch: 00 [ 9981/20164 ( 49%)], Train Loss: 0.53883\n","Epoch: 00 [ 9991/20164 ( 50%)], Train Loss: 0.53916\n","Epoch: 00 [10001/20164 ( 50%)], Train Loss: 0.53909\n","Epoch: 00 [10011/20164 ( 50%)], Train Loss: 0.53911\n","Epoch: 00 [10021/20164 ( 50%)], Train Loss: 0.53908\n","Epoch: 00 [10031/20164 ( 50%)], Train Loss: 0.53872\n","Epoch: 00 [10041/20164 ( 50%)], Train Loss: 0.53879\n","Epoch: 00 [10051/20164 ( 50%)], Train Loss: 0.53874\n","Epoch: 00 [10061/20164 ( 50%)], Train Loss: 0.53868\n","Epoch: 00 [10071/20164 ( 50%)], Train Loss: 0.53857\n","Epoch: 00 [10081/20164 ( 50%)], Train Loss: 0.53893\n","Epoch: 00 [10091/20164 ( 50%)], Train Loss: 0.53887\n","Epoch: 00 [10101/20164 ( 50%)], Train Loss: 0.53902\n","Epoch: 00 [10111/20164 ( 50%)], Train Loss: 0.53905\n","Epoch: 00 [10121/20164 ( 50%)], Train Loss: 0.53873\n","Epoch: 00 [10131/20164 ( 50%)], Train Loss: 0.53836\n","Epoch: 00 [10141/20164 ( 50%)], Train Loss: 0.53821\n","Epoch: 00 [10151/20164 ( 50%)], Train Loss: 0.53802\n","Epoch: 00 [10161/20164 ( 50%)], Train Loss: 0.53802\n","Epoch: 00 [10171/20164 ( 50%)], Train Loss: 0.53794\n","Epoch: 00 [10181/20164 ( 50%)], Train Loss: 0.53774\n","Epoch: 00 [10191/20164 ( 51%)], Train Loss: 0.53769\n","Epoch: 00 [10201/20164 ( 51%)], Train Loss: 0.53766\n","Epoch: 00 [10211/20164 ( 51%)], Train Loss: 0.53767\n","Epoch: 00 [10221/20164 ( 51%)], Train Loss: 0.53750\n","Epoch: 00 [10231/20164 ( 51%)], Train Loss: 0.53730\n","Epoch: 00 [10241/20164 ( 51%)], Train Loss: 0.53737\n","Epoch: 00 [10251/20164 ( 51%)], Train Loss: 0.53762\n","Epoch: 00 [10261/20164 ( 51%)], Train Loss: 0.53762\n","Epoch: 00 [10271/20164 ( 51%)], Train Loss: 0.53740\n","Epoch: 00 [10281/20164 ( 51%)], Train Loss: 0.53723\n","Epoch: 00 [10291/20164 ( 51%)], Train Loss: 0.53704\n","Epoch: 00 [10301/20164 ( 51%)], Train Loss: 0.53696\n","Epoch: 00 [10311/20164 ( 51%)], Train Loss: 0.53649\n","Epoch: 00 [10321/20164 ( 51%)], Train Loss: 0.53606\n","Epoch: 00 [10331/20164 ( 51%)], Train Loss: 0.53624\n","Epoch: 00 [10341/20164 ( 51%)], Train Loss: 0.53656\n","Epoch: 00 [10351/20164 ( 51%)], Train Loss: 0.53675\n","Epoch: 00 [10361/20164 ( 51%)], Train Loss: 0.53669\n","Epoch: 00 [10371/20164 ( 51%)], Train Loss: 0.53666\n","Epoch: 00 [10381/20164 ( 51%)], Train Loss: 0.53626\n","Epoch: 00 [10391/20164 ( 52%)], Train Loss: 0.53622\n","Epoch: 00 [10401/20164 ( 52%)], Train Loss: 0.53605\n","Epoch: 00 [10411/20164 ( 52%)], Train Loss: 0.53554\n","Epoch: 00 [10421/20164 ( 52%)], Train Loss: 0.53504\n","Epoch: 00 [10431/20164 ( 52%)], Train Loss: 0.53491\n","Epoch: 00 [10441/20164 ( 52%)], Train Loss: 0.53444\n","Epoch: 00 [10451/20164 ( 52%)], Train Loss: 0.53418\n","Epoch: 00 [10461/20164 ( 52%)], Train Loss: 0.53436\n","Epoch: 00 [10471/20164 ( 52%)], Train Loss: 0.53469\n","Epoch: 00 [10481/20164 ( 52%)], Train Loss: 0.53483\n","Epoch: 00 [10491/20164 ( 52%)], Train Loss: 0.53471\n","Epoch: 00 [10501/20164 ( 52%)], Train Loss: 0.53475\n","Epoch: 00 [10511/20164 ( 52%)], Train Loss: 0.53440\n","Epoch: 00 [10521/20164 ( 52%)], Train Loss: 0.53432\n","Epoch: 00 [10531/20164 ( 52%)], Train Loss: 0.53411\n","Epoch: 00 [10541/20164 ( 52%)], Train Loss: 0.53374\n","Epoch: 00 [10551/20164 ( 52%)], Train Loss: 0.53342\n","Epoch: 00 [10561/20164 ( 52%)], Train Loss: 0.53308\n","Epoch: 00 [10571/20164 ( 52%)], Train Loss: 0.53296\n","Epoch: 00 [10581/20164 ( 52%)], Train Loss: 0.53309\n","Epoch: 00 [10591/20164 ( 53%)], Train Loss: 0.53287\n","Epoch: 00 [10601/20164 ( 53%)], Train Loss: 0.53253\n","Epoch: 00 [10611/20164 ( 53%)], Train Loss: 0.53260\n","Epoch: 00 [10621/20164 ( 53%)], Train Loss: 0.53219\n","Epoch: 00 [10631/20164 ( 53%)], Train Loss: 0.53241\n","Epoch: 00 [10641/20164 ( 53%)], Train Loss: 0.53221\n","Epoch: 00 [10651/20164 ( 53%)], Train Loss: 0.53193\n","Epoch: 00 [10661/20164 ( 53%)], Train Loss: 0.53168\n","Epoch: 00 [10671/20164 ( 53%)], Train Loss: 0.53150\n","Epoch: 00 [10681/20164 ( 53%)], Train Loss: 0.53131\n","Epoch: 00 [10691/20164 ( 53%)], Train Loss: 0.53124\n","Epoch: 00 [10701/20164 ( 53%)], Train Loss: 0.53108\n","Epoch: 00 [10711/20164 ( 53%)], Train Loss: 0.53085\n","Epoch: 00 [10721/20164 ( 53%)], Train Loss: 0.53042\n","Epoch: 00 [10731/20164 ( 53%)], Train Loss: 0.53055\n","Epoch: 00 [10741/20164 ( 53%)], Train Loss: 0.53011\n","Epoch: 00 [10751/20164 ( 53%)], Train Loss: 0.52970\n","Epoch: 00 [10761/20164 ( 53%)], Train Loss: 0.52968\n","Epoch: 00 [10771/20164 ( 53%)], Train Loss: 0.53060\n","Epoch: 00 [10781/20164 ( 53%)], Train Loss: 0.53029\n","Epoch: 00 [10791/20164 ( 54%)], Train Loss: 0.53002\n","Epoch: 00 [10801/20164 ( 54%)], Train Loss: 0.52990\n","Epoch: 00 [10811/20164 ( 54%)], Train Loss: 0.52972\n","Epoch: 00 [10821/20164 ( 54%)], Train Loss: 0.52956\n","Epoch: 00 [10831/20164 ( 54%)], Train Loss: 0.52921\n","Epoch: 00 [10841/20164 ( 54%)], Train Loss: 0.52896\n","Epoch: 00 [10851/20164 ( 54%)], Train Loss: 0.52865\n","Epoch: 00 [10861/20164 ( 54%)], Train Loss: 0.52848\n","Epoch: 00 [10871/20164 ( 54%)], Train Loss: 0.52840\n","Epoch: 00 [10881/20164 ( 54%)], Train Loss: 0.52833\n","Epoch: 00 [10891/20164 ( 54%)], Train Loss: 0.52816\n","Epoch: 00 [10901/20164 ( 54%)], Train Loss: 0.52781\n","Epoch: 00 [10911/20164 ( 54%)], Train Loss: 0.52773\n","Epoch: 00 [10921/20164 ( 54%)], Train Loss: 0.52757\n","Epoch: 00 [10931/20164 ( 54%)], Train Loss: 0.52713\n","Epoch: 00 [10941/20164 ( 54%)], Train Loss: 0.52705\n","Epoch: 00 [10951/20164 ( 54%)], Train Loss: 0.52702\n","Epoch: 00 [10961/20164 ( 54%)], Train Loss: 0.52660\n","Epoch: 00 [10971/20164 ( 54%)], Train Loss: 0.52640\n","Epoch: 00 [10981/20164 ( 54%)], Train Loss: 0.52640\n","Epoch: 00 [10991/20164 ( 55%)], Train Loss: 0.52631\n","Epoch: 00 [11001/20164 ( 55%)], Train Loss: 0.52632\n","Epoch: 00 [11011/20164 ( 55%)], Train Loss: 0.52591\n","Epoch: 00 [11021/20164 ( 55%)], Train Loss: 0.52624\n","Epoch: 00 [11031/20164 ( 55%)], Train Loss: 0.52593\n","Epoch: 00 [11041/20164 ( 55%)], Train Loss: 0.52614\n","Epoch: 00 [11051/20164 ( 55%)], Train Loss: 0.52612\n","Epoch: 00 [11061/20164 ( 55%)], Train Loss: 0.52652\n","Epoch: 00 [11071/20164 ( 55%)], Train Loss: 0.52639\n","Epoch: 00 [11081/20164 ( 55%)], Train Loss: 0.52613\n","Epoch: 00 [11091/20164 ( 55%)], Train Loss: 0.52624\n","Epoch: 00 [11101/20164 ( 55%)], Train Loss: 0.52599\n","Epoch: 00 [11111/20164 ( 55%)], Train Loss: 0.52592\n","Epoch: 00 [11121/20164 ( 55%)], Train Loss: 0.52595\n","Epoch: 00 [11131/20164 ( 55%)], Train Loss: 0.52604\n","Epoch: 00 [11141/20164 ( 55%)], Train Loss: 0.52571\n","Epoch: 00 [11151/20164 ( 55%)], Train Loss: 0.52549\n","Epoch: 00 [11161/20164 ( 55%)], Train Loss: 0.52512\n","Epoch: 00 [11171/20164 ( 55%)], Train Loss: 0.52508\n","Epoch: 00 [11181/20164 ( 55%)], Train Loss: 0.52478\n","Epoch: 00 [11191/20164 ( 55%)], Train Loss: 0.52485\n","Epoch: 00 [11201/20164 ( 56%)], Train Loss: 0.52521\n","Epoch: 00 [11211/20164 ( 56%)], Train Loss: 0.52488\n","Epoch: 00 [11221/20164 ( 56%)], Train Loss: 0.52489\n","Epoch: 00 [11231/20164 ( 56%)], Train Loss: 0.52495\n","Epoch: 00 [11241/20164 ( 56%)], Train Loss: 0.52513\n","Epoch: 00 [11251/20164 ( 56%)], Train Loss: 0.52495\n","Epoch: 00 [11261/20164 ( 56%)], Train Loss: 0.52475\n","Epoch: 00 [11271/20164 ( 56%)], Train Loss: 0.52493\n","Epoch: 00 [11281/20164 ( 56%)], Train Loss: 0.52499\n","Epoch: 00 [11291/20164 ( 56%)], Train Loss: 0.52491\n","Epoch: 00 [11301/20164 ( 56%)], Train Loss: 0.52467\n","Epoch: 00 [11311/20164 ( 56%)], Train Loss: 0.52474\n","Epoch: 00 [11321/20164 ( 56%)], Train Loss: 0.52446\n","Epoch: 00 [11331/20164 ( 56%)], Train Loss: 0.52468\n","Epoch: 00 [11341/20164 ( 56%)], Train Loss: 0.52435\n","Epoch: 00 [11351/20164 ( 56%)], Train Loss: 0.52447\n","Epoch: 00 [11361/20164 ( 56%)], Train Loss: 0.52419\n","Epoch: 00 [11371/20164 ( 56%)], Train Loss: 0.52397\n","Epoch: 00 [11381/20164 ( 56%)], Train Loss: 0.52379\n","Epoch: 00 [11391/20164 ( 56%)], Train Loss: 0.52339\n","Epoch: 00 [11401/20164 ( 57%)], Train Loss: 0.52299\n","Epoch: 00 [11411/20164 ( 57%)], Train Loss: 0.52265\n","Epoch: 00 [11421/20164 ( 57%)], Train Loss: 0.52224\n","Epoch: 00 [11431/20164 ( 57%)], Train Loss: 0.52188\n","Epoch: 00 [11441/20164 ( 57%)], Train Loss: 0.52176\n","Epoch: 00 [11451/20164 ( 57%)], Train Loss: 0.52189\n","Epoch: 00 [11461/20164 ( 57%)], Train Loss: 0.52195\n","Epoch: 00 [11471/20164 ( 57%)], Train Loss: 0.52200\n","Epoch: 00 [11481/20164 ( 57%)], Train Loss: 0.52160\n","Epoch: 00 [11491/20164 ( 57%)], Train Loss: 0.52196\n","Epoch: 00 [11501/20164 ( 57%)], Train Loss: 0.52197\n","Epoch: 00 [11511/20164 ( 57%)], Train Loss: 0.52183\n","Epoch: 00 [11521/20164 ( 57%)], Train Loss: 0.52152\n","Epoch: 00 [11531/20164 ( 57%)], Train Loss: 0.52163\n","Epoch: 00 [11541/20164 ( 57%)], Train Loss: 0.52154\n","Epoch: 00 [11551/20164 ( 57%)], Train Loss: 0.52124\n","Epoch: 00 [11561/20164 ( 57%)], Train Loss: 0.52125\n","Epoch: 00 [11571/20164 ( 57%)], Train Loss: 0.52092\n","Epoch: 00 [11581/20164 ( 57%)], Train Loss: 0.52122\n","Epoch: 00 [11591/20164 ( 57%)], Train Loss: 0.52123\n","Epoch: 00 [11601/20164 ( 58%)], Train Loss: 0.52129\n","Epoch: 00 [11611/20164 ( 58%)], Train Loss: 0.52117\n","Epoch: 00 [11621/20164 ( 58%)], Train Loss: 0.52112\n","Epoch: 00 [11631/20164 ( 58%)], Train Loss: 0.52103\n","Epoch: 00 [11641/20164 ( 58%)], Train Loss: 0.52072\n","Epoch: 00 [11651/20164 ( 58%)], Train Loss: 0.52067\n","Epoch: 00 [11661/20164 ( 58%)], Train Loss: 0.52036\n","Epoch: 00 [11671/20164 ( 58%)], Train Loss: 0.52013\n","Epoch: 00 [11681/20164 ( 58%)], Train Loss: 0.52040\n","Epoch: 00 [11691/20164 ( 58%)], Train Loss: 0.52068\n","Epoch: 00 [11701/20164 ( 58%)], Train Loss: 0.52064\n","Epoch: 00 [11711/20164 ( 58%)], Train Loss: 0.52037\n","Epoch: 00 [11721/20164 ( 58%)], Train Loss: 0.52057\n","Epoch: 00 [11731/20164 ( 58%)], Train Loss: 0.52028\n","Epoch: 00 [11741/20164 ( 58%)], Train Loss: 0.52001\n","Epoch: 00 [11751/20164 ( 58%)], Train Loss: 0.51981\n","Epoch: 00 [11761/20164 ( 58%)], Train Loss: 0.51951\n","Epoch: 00 [11771/20164 ( 58%)], Train Loss: 0.51924\n","Epoch: 00 [11781/20164 ( 58%)], Train Loss: 0.51915\n","Epoch: 00 [11791/20164 ( 58%)], Train Loss: 0.51885\n","Epoch: 00 [11801/20164 ( 59%)], Train Loss: 0.51855\n","Epoch: 00 [11811/20164 ( 59%)], Train Loss: 0.51858\n","Epoch: 00 [11821/20164 ( 59%)], Train Loss: 0.51836\n","Epoch: 00 [11831/20164 ( 59%)], Train Loss: 0.51829\n","Epoch: 00 [11841/20164 ( 59%)], Train Loss: 0.51817\n","Epoch: 00 [11851/20164 ( 59%)], Train Loss: 0.51801\n","Epoch: 00 [11861/20164 ( 59%)], Train Loss: 0.51773\n","Epoch: 00 [11871/20164 ( 59%)], Train Loss: 0.51734\n","Epoch: 00 [11881/20164 ( 59%)], Train Loss: 0.51729\n","Epoch: 00 [11891/20164 ( 59%)], Train Loss: 0.51709\n","Epoch: 00 [11901/20164 ( 59%)], Train Loss: 0.51728\n","Epoch: 00 [11911/20164 ( 59%)], Train Loss: 0.51718\n","Epoch: 00 [11921/20164 ( 59%)], Train Loss: 0.51681\n","Epoch: 00 [11931/20164 ( 59%)], Train Loss: 0.51652\n","Epoch: 00 [11941/20164 ( 59%)], Train Loss: 0.51646\n","Epoch: 00 [11951/20164 ( 59%)], Train Loss: 0.51635\n","Epoch: 00 [11961/20164 ( 59%)], Train Loss: 0.51626\n","Epoch: 00 [11971/20164 ( 59%)], Train Loss: 0.51594\n","Epoch: 00 [11981/20164 ( 59%)], Train Loss: 0.51581\n","Epoch: 00 [11991/20164 ( 59%)], Train Loss: 0.51546\n","Epoch: 00 [12001/20164 ( 60%)], Train Loss: 0.51542\n","Epoch: 00 [12011/20164 ( 60%)], Train Loss: 0.51544\n","Epoch: 00 [12021/20164 ( 60%)], Train Loss: 0.51574\n","Epoch: 00 [12031/20164 ( 60%)], Train Loss: 0.51566\n","Epoch: 00 [12041/20164 ( 60%)], Train Loss: 0.51540\n","Epoch: 00 [12051/20164 ( 60%)], Train Loss: 0.51543\n","Epoch: 00 [12061/20164 ( 60%)], Train Loss: 0.51534\n","Epoch: 00 [12071/20164 ( 60%)], Train Loss: 0.51530\n","Epoch: 00 [12081/20164 ( 60%)], Train Loss: 0.51525\n","Epoch: 00 [12091/20164 ( 60%)], Train Loss: 0.51495\n","Epoch: 00 [12101/20164 ( 60%)], Train Loss: 0.51463\n","Epoch: 00 [12111/20164 ( 60%)], Train Loss: 0.51427\n","Epoch: 00 [12121/20164 ( 60%)], Train Loss: 0.51400\n","Epoch: 00 [12131/20164 ( 60%)], Train Loss: 0.51369\n","Epoch: 00 [12141/20164 ( 60%)], Train Loss: 0.51406\n","Epoch: 00 [12151/20164 ( 60%)], Train Loss: 0.51421\n","Epoch: 00 [12161/20164 ( 60%)], Train Loss: 0.51410\n","Epoch: 00 [12171/20164 ( 60%)], Train Loss: 0.51409\n","Epoch: 00 [12181/20164 ( 60%)], Train Loss: 0.51418\n","Epoch: 00 [12191/20164 ( 60%)], Train Loss: 0.51399\n","Epoch: 00 [12201/20164 ( 61%)], Train Loss: 0.51367\n","Epoch: 00 [12211/20164 ( 61%)], Train Loss: 0.51358\n","Epoch: 00 [12221/20164 ( 61%)], Train Loss: 0.51336\n","Epoch: 00 [12231/20164 ( 61%)], Train Loss: 0.51297\n","Epoch: 00 [12241/20164 ( 61%)], Train Loss: 0.51322\n","Epoch: 00 [12251/20164 ( 61%)], Train Loss: 0.51295\n","Epoch: 00 [12261/20164 ( 61%)], Train Loss: 0.51282\n","Epoch: 00 [12271/20164 ( 61%)], Train Loss: 0.51280\n","Epoch: 00 [12281/20164 ( 61%)], Train Loss: 0.51270\n","Epoch: 00 [12291/20164 ( 61%)], Train Loss: 0.51251\n","Epoch: 00 [12301/20164 ( 61%)], Train Loss: 0.51253\n","Epoch: 00 [12311/20164 ( 61%)], Train Loss: 0.51228\n","Epoch: 00 [12321/20164 ( 61%)], Train Loss: 0.51192\n","Epoch: 00 [12331/20164 ( 61%)], Train Loss: 0.51194\n","Epoch: 00 [12341/20164 ( 61%)], Train Loss: 0.51213\n","Epoch: 00 [12351/20164 ( 61%)], Train Loss: 0.51228\n","Epoch: 00 [12361/20164 ( 61%)], Train Loss: 0.51191\n","Epoch: 00 [12371/20164 ( 61%)], Train Loss: 0.51172\n","Epoch: 00 [12381/20164 ( 61%)], Train Loss: 0.51148\n","Epoch: 00 [12391/20164 ( 61%)], Train Loss: 0.51124\n","Epoch: 00 [12401/20164 ( 62%)], Train Loss: 0.51113\n","Epoch: 00 [12411/20164 ( 62%)], Train Loss: 0.51117\n","Epoch: 00 [12421/20164 ( 62%)], Train Loss: 0.51085\n","Epoch: 00 [12431/20164 ( 62%)], Train Loss: 0.51070\n","Epoch: 00 [12441/20164 ( 62%)], Train Loss: 0.51040\n","Epoch: 00 [12451/20164 ( 62%)], Train Loss: 0.51033\n","Epoch: 00 [12461/20164 ( 62%)], Train Loss: 0.51005\n","Epoch: 00 [12471/20164 ( 62%)], Train Loss: 0.50996\n","Epoch: 00 [12481/20164 ( 62%)], Train Loss: 0.50987\n","Epoch: 00 [12491/20164 ( 62%)], Train Loss: 0.50953\n","Epoch: 00 [12501/20164 ( 62%)], Train Loss: 0.50929\n","Epoch: 00 [12511/20164 ( 62%)], Train Loss: 0.50906\n","Epoch: 00 [12521/20164 ( 62%)], Train Loss: 0.50902\n","Epoch: 00 [12531/20164 ( 62%)], Train Loss: 0.50882\n","Epoch: 00 [12541/20164 ( 62%)], Train Loss: 0.50877\n","Epoch: 00 [12551/20164 ( 62%)], Train Loss: 0.50858\n","Epoch: 00 [12561/20164 ( 62%)], Train Loss: 0.50850\n","Epoch: 00 [12571/20164 ( 62%)], Train Loss: 0.50851\n","Epoch: 00 [12581/20164 ( 62%)], Train Loss: 0.50827\n","Epoch: 00 [12591/20164 ( 62%)], Train Loss: 0.50831\n","Epoch: 00 [12601/20164 ( 62%)], Train Loss: 0.50814\n","Epoch: 00 [12611/20164 ( 63%)], Train Loss: 0.50798\n","Epoch: 00 [12621/20164 ( 63%)], Train Loss: 0.50770\n","Epoch: 00 [12631/20164 ( 63%)], Train Loss: 0.50758\n","Epoch: 00 [12641/20164 ( 63%)], Train Loss: 0.50721\n","Epoch: 00 [12651/20164 ( 63%)], Train Loss: 0.50703\n","Epoch: 00 [12661/20164 ( 63%)], Train Loss: 0.50719\n","Epoch: 00 [12671/20164 ( 63%)], Train Loss: 0.50694\n","Epoch: 00 [12681/20164 ( 63%)], Train Loss: 0.50658\n","Epoch: 00 [12691/20164 ( 63%)], Train Loss: 0.50693\n","Epoch: 00 [12701/20164 ( 63%)], Train Loss: 0.50702\n","Epoch: 00 [12711/20164 ( 63%)], Train Loss: 0.50680\n","Epoch: 00 [12721/20164 ( 63%)], Train Loss: 0.50658\n","Epoch: 00 [12731/20164 ( 63%)], Train Loss: 0.50632\n","Epoch: 00 [12741/20164 ( 63%)], Train Loss: 0.50604\n","Epoch: 00 [12751/20164 ( 63%)], Train Loss: 0.50599\n","Epoch: 00 [12761/20164 ( 63%)], Train Loss: 0.50582\n","Epoch: 00 [12771/20164 ( 63%)], Train Loss: 0.50555\n","Epoch: 00 [12781/20164 ( 63%)], Train Loss: 0.50526\n","Epoch: 00 [12791/20164 ( 63%)], Train Loss: 0.50536\n","Epoch: 00 [12801/20164 ( 63%)], Train Loss: 0.50505\n","Epoch: 00 [12811/20164 ( 64%)], Train Loss: 0.50507\n","Epoch: 00 [12821/20164 ( 64%)], Train Loss: 0.50516\n","Epoch: 00 [12831/20164 ( 64%)], Train Loss: 0.50517\n","Epoch: 00 [12841/20164 ( 64%)], Train Loss: 0.50496\n","Epoch: 00 [12851/20164 ( 64%)], Train Loss: 0.50474\n","Epoch: 00 [12861/20164 ( 64%)], Train Loss: 0.50445\n","Epoch: 00 [12871/20164 ( 64%)], Train Loss: 0.50416\n","Epoch: 00 [12881/20164 ( 64%)], Train Loss: 0.50417\n","Epoch: 00 [12891/20164 ( 64%)], Train Loss: 0.50409\n","Epoch: 00 [12901/20164 ( 64%)], Train Loss: 0.50396\n","Epoch: 00 [12911/20164 ( 64%)], Train Loss: 0.50395\n","Epoch: 00 [12921/20164 ( 64%)], Train Loss: 0.50394\n","Epoch: 00 [12931/20164 ( 64%)], Train Loss: 0.50370\n","Epoch: 00 [12941/20164 ( 64%)], Train Loss: 0.50377\n","Epoch: 00 [12951/20164 ( 64%)], Train Loss: 0.50364\n","Epoch: 00 [12961/20164 ( 64%)], Train Loss: 0.50341\n","Epoch: 00 [12971/20164 ( 64%)], Train Loss: 0.50348\n","Epoch: 00 [12981/20164 ( 64%)], Train Loss: 0.50331\n","Epoch: 00 [12991/20164 ( 64%)], Train Loss: 0.50303\n","Epoch: 00 [13001/20164 ( 64%)], Train Loss: 0.50288\n","Epoch: 00 [13011/20164 ( 65%)], Train Loss: 0.50303\n","Epoch: 00 [13021/20164 ( 65%)], Train Loss: 0.50301\n","Epoch: 00 [13031/20164 ( 65%)], Train Loss: 0.50295\n","Epoch: 00 [13041/20164 ( 65%)], Train Loss: 0.50292\n","Epoch: 00 [13051/20164 ( 65%)], Train Loss: 0.50283\n","Epoch: 00 [13061/20164 ( 65%)], Train Loss: 0.50277\n","Epoch: 00 [13071/20164 ( 65%)], Train Loss: 0.50255\n","Epoch: 00 [13081/20164 ( 65%)], Train Loss: 0.50219\n","Epoch: 00 [13091/20164 ( 65%)], Train Loss: 0.50239\n","Epoch: 00 [13101/20164 ( 65%)], Train Loss: 0.50240\n","Epoch: 00 [13111/20164 ( 65%)], Train Loss: 0.50221\n","Epoch: 00 [13121/20164 ( 65%)], Train Loss: 0.50238\n","Epoch: 00 [13131/20164 ( 65%)], Train Loss: 0.50261\n","Epoch: 00 [13141/20164 ( 65%)], Train Loss: 0.50258\n","Epoch: 00 [13151/20164 ( 65%)], Train Loss: 0.50237\n","Epoch: 00 [13161/20164 ( 65%)], Train Loss: 0.50214\n","Epoch: 00 [13171/20164 ( 65%)], Train Loss: 0.50216\n","Epoch: 00 [13181/20164 ( 65%)], Train Loss: 0.50191\n","Epoch: 00 [13191/20164 ( 65%)], Train Loss: 0.50210\n","Epoch: 00 [13201/20164 ( 65%)], Train Loss: 0.50195\n","Epoch: 00 [13211/20164 ( 66%)], Train Loss: 0.50217\n","Epoch: 00 [13221/20164 ( 66%)], Train Loss: 0.50199\n","Epoch: 00 [13231/20164 ( 66%)], Train Loss: 0.50205\n","Epoch: 00 [13241/20164 ( 66%)], Train Loss: 0.50175\n","Epoch: 00 [13251/20164 ( 66%)], Train Loss: 0.50167\n","Epoch: 00 [13261/20164 ( 66%)], Train Loss: 0.50150\n","Epoch: 00 [13271/20164 ( 66%)], Train Loss: 0.50131\n","Epoch: 00 [13281/20164 ( 66%)], Train Loss: 0.50130\n","Epoch: 00 [13291/20164 ( 66%)], Train Loss: 0.50120\n","Epoch: 00 [13301/20164 ( 66%)], Train Loss: 0.50095\n","Epoch: 00 [13311/20164 ( 66%)], Train Loss: 0.50099\n","Epoch: 00 [13321/20164 ( 66%)], Train Loss: 0.50076\n","Epoch: 00 [13331/20164 ( 66%)], Train Loss: 0.50107\n","Epoch: 00 [13341/20164 ( 66%)], Train Loss: 0.50092\n","Epoch: 00 [13351/20164 ( 66%)], Train Loss: 0.50073\n","Epoch: 00 [13361/20164 ( 66%)], Train Loss: 0.50062\n","Epoch: 00 [13371/20164 ( 66%)], Train Loss: 0.50067\n","Epoch: 00 [13381/20164 ( 66%)], Train Loss: 0.50054\n","Epoch: 00 [13391/20164 ( 66%)], Train Loss: 0.50027\n","Epoch: 00 [13401/20164 ( 66%)], Train Loss: 0.50029\n","Epoch: 00 [13411/20164 ( 67%)], Train Loss: 0.50004\n","Epoch: 00 [13421/20164 ( 67%)], Train Loss: 0.49986\n","Epoch: 00 [13431/20164 ( 67%)], Train Loss: 0.49980\n","Epoch: 00 [13441/20164 ( 67%)], Train Loss: 0.49981\n","Epoch: 00 [13451/20164 ( 67%)], Train Loss: 0.49997\n","Epoch: 00 [13461/20164 ( 67%)], Train Loss: 0.49992\n","Epoch: 00 [13471/20164 ( 67%)], Train Loss: 0.50028\n","Epoch: 00 [13481/20164 ( 67%)], Train Loss: 0.50013\n","Epoch: 00 [13491/20164 ( 67%)], Train Loss: 0.49986\n","Epoch: 00 [13501/20164 ( 67%)], Train Loss: 0.49981\n","Epoch: 00 [13511/20164 ( 67%)], Train Loss: 0.49973\n","Epoch: 00 [13521/20164 ( 67%)], Train Loss: 0.50022\n","Epoch: 00 [13531/20164 ( 67%)], Train Loss: 0.50019\n","Epoch: 00 [13541/20164 ( 67%)], Train Loss: 0.49996\n","Epoch: 00 [13551/20164 ( 67%)], Train Loss: 0.49978\n","Epoch: 00 [13561/20164 ( 67%)], Train Loss: 0.49974\n","Epoch: 00 [13571/20164 ( 67%)], Train Loss: 0.49987\n","Epoch: 00 [13581/20164 ( 67%)], Train Loss: 0.49964\n","Epoch: 00 [13591/20164 ( 67%)], Train Loss: 0.49972\n","Epoch: 00 [13601/20164 ( 67%)], Train Loss: 0.49942\n","Epoch: 00 [13611/20164 ( 68%)], Train Loss: 0.49919\n","Epoch: 00 [13621/20164 ( 68%)], Train Loss: 0.49889\n","Epoch: 00 [13631/20164 ( 68%)], Train Loss: 0.49872\n","Epoch: 00 [13641/20164 ( 68%)], Train Loss: 0.49864\n","Epoch: 00 [13651/20164 ( 68%)], Train Loss: 0.49896\n","Epoch: 00 [13661/20164 ( 68%)], Train Loss: 0.49900\n","Epoch: 00 [13671/20164 ( 68%)], Train Loss: 0.49885\n","Epoch: 00 [13681/20164 ( 68%)], Train Loss: 0.49884\n","Epoch: 00 [13691/20164 ( 68%)], Train Loss: 0.49871\n","Epoch: 00 [13701/20164 ( 68%)], Train Loss: 0.49864\n","Epoch: 00 [13711/20164 ( 68%)], Train Loss: 0.49848\n","Epoch: 00 [13721/20164 ( 68%)], Train Loss: 0.49823\n","Epoch: 00 [13731/20164 ( 68%)], Train Loss: 0.49801\n","Epoch: 00 [13741/20164 ( 68%)], Train Loss: 0.49793\n","Epoch: 00 [13751/20164 ( 68%)], Train Loss: 0.49779\n","Epoch: 00 [13761/20164 ( 68%)], Train Loss: 0.49762\n","Epoch: 00 [13771/20164 ( 68%)], Train Loss: 0.49741\n","Epoch: 00 [13781/20164 ( 68%)], Train Loss: 0.49737\n","Epoch: 00 [13791/20164 ( 68%)], Train Loss: 0.49717\n","Epoch: 00 [13801/20164 ( 68%)], Train Loss: 0.49704\n","Epoch: 00 [13811/20164 ( 68%)], Train Loss: 0.49746\n","Epoch: 00 [13821/20164 ( 69%)], Train Loss: 0.49747\n","Epoch: 00 [13831/20164 ( 69%)], Train Loss: 0.49723\n","Epoch: 00 [13841/20164 ( 69%)], Train Loss: 0.49710\n","Epoch: 00 [13851/20164 ( 69%)], Train Loss: 0.49687\n","Epoch: 00 [13861/20164 ( 69%)], Train Loss: 0.49676\n","Epoch: 00 [13871/20164 ( 69%)], Train Loss: 0.49661\n","Epoch: 00 [13881/20164 ( 69%)], Train Loss: 0.49656\n","Epoch: 00 [13891/20164 ( 69%)], Train Loss: 0.49651\n","Epoch: 00 [13901/20164 ( 69%)], Train Loss: 0.49670\n","Epoch: 00 [13911/20164 ( 69%)], Train Loss: 0.49656\n","Epoch: 00 [13921/20164 ( 69%)], Train Loss: 0.49650\n","Epoch: 00 [13931/20164 ( 69%)], Train Loss: 0.49627\n","Epoch: 00 [13941/20164 ( 69%)], Train Loss: 0.49598\n","Epoch: 00 [13951/20164 ( 69%)], Train Loss: 0.49593\n","Epoch: 00 [13961/20164 ( 69%)], Train Loss: 0.49597\n","Epoch: 00 [13971/20164 ( 69%)], Train Loss: 0.49598\n","Epoch: 00 [13981/20164 ( 69%)], Train Loss: 0.49576\n","Epoch: 00 [13991/20164 ( 69%)], Train Loss: 0.49556\n","Epoch: 00 [14001/20164 ( 69%)], Train Loss: 0.49546\n","Epoch: 00 [14011/20164 ( 69%)], Train Loss: 0.49517\n","Epoch: 00 [14021/20164 ( 70%)], Train Loss: 0.49487\n","Epoch: 00 [14031/20164 ( 70%)], Train Loss: 0.49498\n","Epoch: 00 [14041/20164 ( 70%)], Train Loss: 0.49493\n","Epoch: 00 [14051/20164 ( 70%)], Train Loss: 0.49478\n","Epoch: 00 [14061/20164 ( 70%)], Train Loss: 0.49482\n","Epoch: 00 [14071/20164 ( 70%)], Train Loss: 0.49465\n","Epoch: 00 [14081/20164 ( 70%)], Train Loss: 0.49487\n","Epoch: 00 [14091/20164 ( 70%)], Train Loss: 0.49459\n","Epoch: 00 [14101/20164 ( 70%)], Train Loss: 0.49438\n","Epoch: 00 [14111/20164 ( 70%)], Train Loss: 0.49434\n","Epoch: 00 [14121/20164 ( 70%)], Train Loss: 0.49424\n","Epoch: 00 [14131/20164 ( 70%)], Train Loss: 0.49436\n","Epoch: 00 [14141/20164 ( 70%)], Train Loss: 0.49426\n","Epoch: 00 [14151/20164 ( 70%)], Train Loss: 0.49430\n","Epoch: 00 [14161/20164 ( 70%)], Train Loss: 0.49414\n","Epoch: 00 [14171/20164 ( 70%)], Train Loss: 0.49428\n","Epoch: 00 [14181/20164 ( 70%)], Train Loss: 0.49399\n","Epoch: 00 [14191/20164 ( 70%)], Train Loss: 0.49393\n","Epoch: 00 [14201/20164 ( 70%)], Train Loss: 0.49372\n","Epoch: 00 [14211/20164 ( 70%)], Train Loss: 0.49377\n","Epoch: 00 [14221/20164 ( 71%)], Train Loss: 0.49363\n","Epoch: 00 [14231/20164 ( 71%)], Train Loss: 0.49345\n","Epoch: 00 [14241/20164 ( 71%)], Train Loss: 0.49371\n","Epoch: 00 [14251/20164 ( 71%)], Train Loss: 0.49360\n","Epoch: 00 [14261/20164 ( 71%)], Train Loss: 0.49343\n","Epoch: 00 [14271/20164 ( 71%)], Train Loss: 0.49372\n","Epoch: 00 [14281/20164 ( 71%)], Train Loss: 0.49362\n","Epoch: 00 [14291/20164 ( 71%)], Train Loss: 0.49331\n","Epoch: 00 [14301/20164 ( 71%)], Train Loss: 0.49364\n","Epoch: 00 [14311/20164 ( 71%)], Train Loss: 0.49357\n","Epoch: 00 [14321/20164 ( 71%)], Train Loss: 0.49353\n","Epoch: 00 [14331/20164 ( 71%)], Train Loss: 0.49352\n","Epoch: 00 [14341/20164 ( 71%)], Train Loss: 0.49349\n","Epoch: 00 [14351/20164 ( 71%)], Train Loss: 0.49334\n","Epoch: 00 [14361/20164 ( 71%)], Train Loss: 0.49342\n","Epoch: 00 [14371/20164 ( 71%)], Train Loss: 0.49341\n","Epoch: 00 [14381/20164 ( 71%)], Train Loss: 0.49326\n","Epoch: 00 [14391/20164 ( 71%)], Train Loss: 0.49310\n","Epoch: 00 [14401/20164 ( 71%)], Train Loss: 0.49338\n","Epoch: 00 [14411/20164 ( 71%)], Train Loss: 0.49332\n","Epoch: 00 [14421/20164 ( 72%)], Train Loss: 0.49335\n","Epoch: 00 [14431/20164 ( 72%)], Train Loss: 0.49354\n","Epoch: 00 [14441/20164 ( 72%)], Train Loss: 0.49347\n","Epoch: 00 [14451/20164 ( 72%)], Train Loss: 0.49322\n","Epoch: 00 [14461/20164 ( 72%)], Train Loss: 0.49296\n","Epoch: 00 [14471/20164 ( 72%)], Train Loss: 0.49278\n","Epoch: 00 [14481/20164 ( 72%)], Train Loss: 0.49274\n","Epoch: 00 [14491/20164 ( 72%)], Train Loss: 0.49249\n","Epoch: 00 [14501/20164 ( 72%)], Train Loss: 0.49257\n","Epoch: 00 [14511/20164 ( 72%)], Train Loss: 0.49266\n","Epoch: 00 [14521/20164 ( 72%)], Train Loss: 0.49251\n","Epoch: 00 [14531/20164 ( 72%)], Train Loss: 0.49227\n","Epoch: 00 [14541/20164 ( 72%)], Train Loss: 0.49218\n","Epoch: 00 [14551/20164 ( 72%)], Train Loss: 0.49217\n","Epoch: 00 [14561/20164 ( 72%)], Train Loss: 0.49229\n","Epoch: 00 [14571/20164 ( 72%)], Train Loss: 0.49219\n","Epoch: 00 [14581/20164 ( 72%)], Train Loss: 0.49205\n","Epoch: 00 [14591/20164 ( 72%)], Train Loss: 0.49202\n","Epoch: 00 [14601/20164 ( 72%)], Train Loss: 0.49169\n","Epoch: 00 [14611/20164 ( 72%)], Train Loss: 0.49145\n","Epoch: 00 [14621/20164 ( 73%)], Train Loss: 0.49127\n","Epoch: 00 [14631/20164 ( 73%)], Train Loss: 0.49113\n","Epoch: 00 [14641/20164 ( 73%)], Train Loss: 0.49143\n","Epoch: 00 [14651/20164 ( 73%)], Train Loss: 0.49117\n","Epoch: 00 [14661/20164 ( 73%)], Train Loss: 0.49099\n","Epoch: 00 [14671/20164 ( 73%)], Train Loss: 0.49131\n","Epoch: 00 [14681/20164 ( 73%)], Train Loss: 0.49107\n","Epoch: 00 [14691/20164 ( 73%)], Train Loss: 0.49107\n","Epoch: 00 [14701/20164 ( 73%)], Train Loss: 0.49094\n","Epoch: 00 [14711/20164 ( 73%)], Train Loss: 0.49089\n","Epoch: 00 [14721/20164 ( 73%)], Train Loss: 0.49062\n","Epoch: 00 [14731/20164 ( 73%)], Train Loss: 0.49049\n","Epoch: 00 [14741/20164 ( 73%)], Train Loss: 0.49046\n","Epoch: 00 [14751/20164 ( 73%)], Train Loss: 0.49050\n","Epoch: 00 [14761/20164 ( 73%)], Train Loss: 0.49045\n","Epoch: 00 [14771/20164 ( 73%)], Train Loss: 0.49032\n","Epoch: 00 [14781/20164 ( 73%)], Train Loss: 0.49019\n","Epoch: 00 [14791/20164 ( 73%)], Train Loss: 0.49018\n","Epoch: 00 [14801/20164 ( 73%)], Train Loss: 0.49022\n","Epoch: 00 [14811/20164 ( 73%)], Train Loss: 0.49006\n","Epoch: 00 [14821/20164 ( 74%)], Train Loss: 0.49043\n","Epoch: 00 [14831/20164 ( 74%)], Train Loss: 0.49017\n","Epoch: 00 [14841/20164 ( 74%)], Train Loss: 0.49022\n","Epoch: 00 [14851/20164 ( 74%)], Train Loss: 0.48998\n","Epoch: 00 [14861/20164 ( 74%)], Train Loss: 0.48974\n","Epoch: 00 [14871/20164 ( 74%)], Train Loss: 0.48952\n","Epoch: 00 [14881/20164 ( 74%)], Train Loss: 0.48942\n","Epoch: 00 [14891/20164 ( 74%)], Train Loss: 0.48940\n","Epoch: 00 [14901/20164 ( 74%)], Train Loss: 0.48933\n","Epoch: 00 [14911/20164 ( 74%)], Train Loss: 0.48941\n","Epoch: 00 [14921/20164 ( 74%)], Train Loss: 0.48931\n","Epoch: 00 [14931/20164 ( 74%)], Train Loss: 0.48917\n","Epoch: 00 [14941/20164 ( 74%)], Train Loss: 0.48895\n","Epoch: 00 [14951/20164 ( 74%)], Train Loss: 0.48890\n","Epoch: 00 [14961/20164 ( 74%)], Train Loss: 0.48883\n","Epoch: 00 [14971/20164 ( 74%)], Train Loss: 0.48860\n","Epoch: 00 [14981/20164 ( 74%)], Train Loss: 0.48838\n","Epoch: 00 [14991/20164 ( 74%)], Train Loss: 0.48817\n","Epoch: 00 [15001/20164 ( 74%)], Train Loss: 0.48795\n","Epoch: 00 [15011/20164 ( 74%)], Train Loss: 0.48773\n","Epoch: 00 [15021/20164 ( 74%)], Train Loss: 0.48753\n","Epoch: 00 [15031/20164 ( 75%)], Train Loss: 0.48740\n","Epoch: 00 [15041/20164 ( 75%)], Train Loss: 0.48780\n","Epoch: 00 [15051/20164 ( 75%)], Train Loss: 0.48756\n","Epoch: 00 [15061/20164 ( 75%)], Train Loss: 0.48732\n","Epoch: 00 [15071/20164 ( 75%)], Train Loss: 0.48746\n","Epoch: 00 [15081/20164 ( 75%)], Train Loss: 0.48756\n","Epoch: 00 [15091/20164 ( 75%)], Train Loss: 0.48736\n","Epoch: 00 [15101/20164 ( 75%)], Train Loss: 0.48728\n","Epoch: 00 [15111/20164 ( 75%)], Train Loss: 0.48726\n","Epoch: 00 [15121/20164 ( 75%)], Train Loss: 0.48731\n","Epoch: 00 [15131/20164 ( 75%)], Train Loss: 0.48717\n","Epoch: 00 [15141/20164 ( 75%)], Train Loss: 0.48727\n","Epoch: 00 [15151/20164 ( 75%)], Train Loss: 0.48700\n","Epoch: 00 [15161/20164 ( 75%)], Train Loss: 0.48701\n","Epoch: 00 [15171/20164 ( 75%)], Train Loss: 0.48689\n","Epoch: 00 [15181/20164 ( 75%)], Train Loss: 0.48669\n","Epoch: 00 [15191/20164 ( 75%)], Train Loss: 0.48665\n","Epoch: 00 [15201/20164 ( 75%)], Train Loss: 0.48654\n","Epoch: 00 [15211/20164 ( 75%)], Train Loss: 0.48647\n","Epoch: 00 [15221/20164 ( 75%)], Train Loss: 0.48660\n","Epoch: 00 [15231/20164 ( 76%)], Train Loss: 0.48646\n","Epoch: 00 [15241/20164 ( 76%)], Train Loss: 0.48635\n","Epoch: 00 [15251/20164 ( 76%)], Train Loss: 0.48624\n","Epoch: 00 [15261/20164 ( 76%)], Train Loss: 0.48649\n","Epoch: 00 [15271/20164 ( 76%)], Train Loss: 0.48626\n","Epoch: 00 [15281/20164 ( 76%)], Train Loss: 0.48608\n","Epoch: 00 [15291/20164 ( 76%)], Train Loss: 0.48598\n","Epoch: 00 [15301/20164 ( 76%)], Train Loss: 0.48579\n","Epoch: 00 [15311/20164 ( 76%)], Train Loss: 0.48572\n","Epoch: 00 [15321/20164 ( 76%)], Train Loss: 0.48568\n","Epoch: 00 [15331/20164 ( 76%)], Train Loss: 0.48550\n","Epoch: 00 [15341/20164 ( 76%)], Train Loss: 0.48528\n","Epoch: 00 [15351/20164 ( 76%)], Train Loss: 0.48551\n","Epoch: 00 [15361/20164 ( 76%)], Train Loss: 0.48546\n","Epoch: 00 [15371/20164 ( 76%)], Train Loss: 0.48542\n","Epoch: 00 [15381/20164 ( 76%)], Train Loss: 0.48553\n","Epoch: 00 [15391/20164 ( 76%)], Train Loss: 0.48540\n","Epoch: 00 [15401/20164 ( 76%)], Train Loss: 0.48527\n","Epoch: 00 [15411/20164 ( 76%)], Train Loss: 0.48503\n","Epoch: 00 [15421/20164 ( 76%)], Train Loss: 0.48507\n","Epoch: 00 [15431/20164 ( 77%)], Train Loss: 0.48498\n","Epoch: 00 [15441/20164 ( 77%)], Train Loss: 0.48468\n","Epoch: 00 [15451/20164 ( 77%)], Train Loss: 0.48472\n","Epoch: 00 [15461/20164 ( 77%)], Train Loss: 0.48467\n","Epoch: 00 [15471/20164 ( 77%)], Train Loss: 0.48449\n","Epoch: 00 [15481/20164 ( 77%)], Train Loss: 0.48457\n","Epoch: 00 [15491/20164 ( 77%)], Train Loss: 0.48442\n","Epoch: 00 [15501/20164 ( 77%)], Train Loss: 0.48420\n","Epoch: 00 [15511/20164 ( 77%)], Train Loss: 0.48401\n","Epoch: 00 [15521/20164 ( 77%)], Train Loss: 0.48398\n","Epoch: 00 [15531/20164 ( 77%)], Train Loss: 0.48378\n","Epoch: 00 [15541/20164 ( 77%)], Train Loss: 0.48376\n","Epoch: 00 [15551/20164 ( 77%)], Train Loss: 0.48358\n","Epoch: 00 [15561/20164 ( 77%)], Train Loss: 0.48328\n","Epoch: 00 [15571/20164 ( 77%)], Train Loss: 0.48300\n","Epoch: 00 [15581/20164 ( 77%)], Train Loss: 0.48310\n","Epoch: 00 [15591/20164 ( 77%)], Train Loss: 0.48286\n","Epoch: 00 [15601/20164 ( 77%)], Train Loss: 0.48281\n","Epoch: 00 [15611/20164 ( 77%)], Train Loss: 0.48293\n","Epoch: 00 [15621/20164 ( 77%)], Train Loss: 0.48290\n","Epoch: 00 [15631/20164 ( 78%)], Train Loss: 0.48283\n","Epoch: 00 [15641/20164 ( 78%)], Train Loss: 0.48255\n","Epoch: 00 [15651/20164 ( 78%)], Train Loss: 0.48242\n","Epoch: 00 [15661/20164 ( 78%)], Train Loss: 0.48256\n","Epoch: 00 [15671/20164 ( 78%)], Train Loss: 0.48229\n","Epoch: 00 [15681/20164 ( 78%)], Train Loss: 0.48221\n","Epoch: 00 [15691/20164 ( 78%)], Train Loss: 0.48203\n","Epoch: 00 [15701/20164 ( 78%)], Train Loss: 0.48186\n","Epoch: 00 [15711/20164 ( 78%)], Train Loss: 0.48226\n","Epoch: 00 [15721/20164 ( 78%)], Train Loss: 0.48240\n","Epoch: 00 [15731/20164 ( 78%)], Train Loss: 0.48227\n","Epoch: 00 [15741/20164 ( 78%)], Train Loss: 0.48227\n","Epoch: 00 [15751/20164 ( 78%)], Train Loss: 0.48208\n","Epoch: 00 [15761/20164 ( 78%)], Train Loss: 0.48186\n","Epoch: 00 [15771/20164 ( 78%)], Train Loss: 0.48183\n","Epoch: 00 [15781/20164 ( 78%)], Train Loss: 0.48179\n","Epoch: 00 [15791/20164 ( 78%)], Train Loss: 0.48163\n","Epoch: 00 [15801/20164 ( 78%)], Train Loss: 0.48154\n","Epoch: 00 [15811/20164 ( 78%)], Train Loss: 0.48134\n","Epoch: 00 [15821/20164 ( 78%)], Train Loss: 0.48119\n","Epoch: 00 [15831/20164 ( 79%)], Train Loss: 0.48099\n","Epoch: 00 [15841/20164 ( 79%)], Train Loss: 0.48083\n","Epoch: 00 [15851/20164 ( 79%)], Train Loss: 0.48077\n","Epoch: 00 [15861/20164 ( 79%)], Train Loss: 0.48068\n","Epoch: 00 [15871/20164 ( 79%)], Train Loss: 0.48048\n","Epoch: 00 [15881/20164 ( 79%)], Train Loss: 0.48033\n","Epoch: 00 [15891/20164 ( 79%)], Train Loss: 0.48030\n","Epoch: 00 [15901/20164 ( 79%)], Train Loss: 0.48026\n","Epoch: 00 [15911/20164 ( 79%)], Train Loss: 0.48010\n","Epoch: 00 [15921/20164 ( 79%)], Train Loss: 0.48002\n","Epoch: 00 [15931/20164 ( 79%)], Train Loss: 0.47999\n","Epoch: 00 [15941/20164 ( 79%)], Train Loss: 0.47998\n","Epoch: 00 [15951/20164 ( 79%)], Train Loss: 0.47978\n","Epoch: 00 [15961/20164 ( 79%)], Train Loss: 0.47969\n","Epoch: 00 [15971/20164 ( 79%)], Train Loss: 0.47966\n","Epoch: 00 [15981/20164 ( 79%)], Train Loss: 0.47951\n","Epoch: 00 [15991/20164 ( 79%)], Train Loss: 0.47955\n","Epoch: 00 [16001/20164 ( 79%)], Train Loss: 0.47954\n","Epoch: 00 [16011/20164 ( 79%)], Train Loss: 0.47955\n","Epoch: 00 [16021/20164 ( 79%)], Train Loss: 0.47972\n","Epoch: 00 [16031/20164 ( 80%)], Train Loss: 0.47956\n","Epoch: 00 [16041/20164 ( 80%)], Train Loss: 0.47961\n","Epoch: 00 [16051/20164 ( 80%)], Train Loss: 0.47943\n","Epoch: 00 [16061/20164 ( 80%)], Train Loss: 0.47918\n","Epoch: 00 [16071/20164 ( 80%)], Train Loss: 0.47922\n","Epoch: 00 [16081/20164 ( 80%)], Train Loss: 0.47943\n","Epoch: 00 [16091/20164 ( 80%)], Train Loss: 0.47922\n","Epoch: 00 [16101/20164 ( 80%)], Train Loss: 0.47933\n","Epoch: 00 [16111/20164 ( 80%)], Train Loss: 0.47932\n","Epoch: 00 [16121/20164 ( 80%)], Train Loss: 0.47915\n","Epoch: 00 [16131/20164 ( 80%)], Train Loss: 0.47905\n","Epoch: 00 [16141/20164 ( 80%)], Train Loss: 0.47897\n","Epoch: 00 [16151/20164 ( 80%)], Train Loss: 0.47877\n","Epoch: 00 [16161/20164 ( 80%)], Train Loss: 0.47863\n","Epoch: 00 [16171/20164 ( 80%)], Train Loss: 0.47896\n","Epoch: 00 [16181/20164 ( 80%)], Train Loss: 0.47900\n","Epoch: 00 [16191/20164 ( 80%)], Train Loss: 0.47875\n","Epoch: 00 [16201/20164 ( 80%)], Train Loss: 0.47873\n","Epoch: 00 [16211/20164 ( 80%)], Train Loss: 0.47861\n","Epoch: 00 [16221/20164 ( 80%)], Train Loss: 0.47840\n","Epoch: 00 [16231/20164 ( 80%)], Train Loss: 0.47823\n","Epoch: 00 [16241/20164 ( 81%)], Train Loss: 0.47796\n","Epoch: 00 [16251/20164 ( 81%)], Train Loss: 0.47768\n","Epoch: 00 [16261/20164 ( 81%)], Train Loss: 0.47758\n","Epoch: 00 [16271/20164 ( 81%)], Train Loss: 0.47735\n","Epoch: 00 [16281/20164 ( 81%)], Train Loss: 0.47715\n","Epoch: 00 [16291/20164 ( 81%)], Train Loss: 0.47718\n","Epoch: 00 [16301/20164 ( 81%)], Train Loss: 0.47711\n","Epoch: 00 [16311/20164 ( 81%)], Train Loss: 0.47727\n","Epoch: 00 [16321/20164 ( 81%)], Train Loss: 0.47727\n","Epoch: 00 [16331/20164 ( 81%)], Train Loss: 0.47701\n","Epoch: 00 [16341/20164 ( 81%)], Train Loss: 0.47700\n","Epoch: 00 [16351/20164 ( 81%)], Train Loss: 0.47692\n","Epoch: 00 [16361/20164 ( 81%)], Train Loss: 0.47675\n","Epoch: 00 [16371/20164 ( 81%)], Train Loss: 0.47666\n","Epoch: 00 [16381/20164 ( 81%)], Train Loss: 0.47656\n","Epoch: 00 [16391/20164 ( 81%)], Train Loss: 0.47655\n","Epoch: 00 [16401/20164 ( 81%)], Train Loss: 0.47639\n","Epoch: 00 [16411/20164 ( 81%)], Train Loss: 0.47617\n","Epoch: 00 [16421/20164 ( 81%)], Train Loss: 0.47612\n","Epoch: 00 [16431/20164 ( 81%)], Train Loss: 0.47601\n","Epoch: 00 [16441/20164 ( 82%)], Train Loss: 0.47597\n","Epoch: 00 [16451/20164 ( 82%)], Train Loss: 0.47576\n","Epoch: 00 [16461/20164 ( 82%)], Train Loss: 0.47561\n","Epoch: 00 [16471/20164 ( 82%)], Train Loss: 0.47569\n","Epoch: 00 [16481/20164 ( 82%)], Train Loss: 0.47560\n","Epoch: 00 [16491/20164 ( 82%)], Train Loss: 0.47553\n","Epoch: 00 [16501/20164 ( 82%)], Train Loss: 0.47559\n","Epoch: 00 [16511/20164 ( 82%)], Train Loss: 0.47548\n","Epoch: 00 [16521/20164 ( 82%)], Train Loss: 0.47546\n","Epoch: 00 [16531/20164 ( 82%)], Train Loss: 0.47527\n","Epoch: 00 [16541/20164 ( 82%)], Train Loss: 0.47522\n","Epoch: 00 [16551/20164 ( 82%)], Train Loss: 0.47497\n","Epoch: 00 [16561/20164 ( 82%)], Train Loss: 0.47492\n","Epoch: 00 [16571/20164 ( 82%)], Train Loss: 0.47472\n","Epoch: 00 [16581/20164 ( 82%)], Train Loss: 0.47454\n","Epoch: 00 [16591/20164 ( 82%)], Train Loss: 0.47436\n","Epoch: 00 [16601/20164 ( 82%)], Train Loss: 0.47424\n","Epoch: 00 [16611/20164 ( 82%)], Train Loss: 0.47407\n","Epoch: 00 [16621/20164 ( 82%)], Train Loss: 0.47401\n","Epoch: 00 [16631/20164 ( 82%)], Train Loss: 0.47389\n","Epoch: 00 [16641/20164 ( 83%)], Train Loss: 0.47379\n","Epoch: 00 [16651/20164 ( 83%)], Train Loss: 0.47379\n","Epoch: 00 [16661/20164 ( 83%)], Train Loss: 0.47366\n","Epoch: 00 [16671/20164 ( 83%)], Train Loss: 0.47362\n","Epoch: 00 [16681/20164 ( 83%)], Train Loss: 0.47361\n","Epoch: 00 [16691/20164 ( 83%)], Train Loss: 0.47348\n","Epoch: 00 [16701/20164 ( 83%)], Train Loss: 0.47335\n","Epoch: 00 [16711/20164 ( 83%)], Train Loss: 0.47316\n","Epoch: 00 [16721/20164 ( 83%)], Train Loss: 0.47305\n","Epoch: 00 [16731/20164 ( 83%)], Train Loss: 0.47316\n","Epoch: 00 [16741/20164 ( 83%)], Train Loss: 0.47304\n","Epoch: 00 [16751/20164 ( 83%)], Train Loss: 0.47316\n","Epoch: 00 [16761/20164 ( 83%)], Train Loss: 0.47293\n","Epoch: 00 [16771/20164 ( 83%)], Train Loss: 0.47280\n","Epoch: 00 [16781/20164 ( 83%)], Train Loss: 0.47252\n","Epoch: 00 [16791/20164 ( 83%)], Train Loss: 0.47229\n","Epoch: 00 [16801/20164 ( 83%)], Train Loss: 0.47218\n","Epoch: 00 [16811/20164 ( 83%)], Train Loss: 0.47210\n","Epoch: 00 [16821/20164 ( 83%)], Train Loss: 0.47193\n","Epoch: 00 [16831/20164 ( 83%)], Train Loss: 0.47196\n","Epoch: 00 [16841/20164 ( 84%)], Train Loss: 0.47194\n","Epoch: 00 [16851/20164 ( 84%)], Train Loss: 0.47204\n","Epoch: 00 [16861/20164 ( 84%)], Train Loss: 0.47183\n","Epoch: 00 [16871/20164 ( 84%)], Train Loss: 0.47186\n","Epoch: 00 [16881/20164 ( 84%)], Train Loss: 0.47183\n","Epoch: 00 [16891/20164 ( 84%)], Train Loss: 0.47167\n","Epoch: 00 [16901/20164 ( 84%)], Train Loss: 0.47163\n","Epoch: 00 [16911/20164 ( 84%)], Train Loss: 0.47151\n","Epoch: 00 [16921/20164 ( 84%)], Train Loss: 0.47148\n","Epoch: 00 [16931/20164 ( 84%)], Train Loss: 0.47153\n","Epoch: 00 [16941/20164 ( 84%)], Train Loss: 0.47177\n","Epoch: 00 [16951/20164 ( 84%)], Train Loss: 0.47158\n","Epoch: 00 [16961/20164 ( 84%)], Train Loss: 0.47144\n","Epoch: 00 [16971/20164 ( 84%)], Train Loss: 0.47142\n","Epoch: 00 [16981/20164 ( 84%)], Train Loss: 0.47149\n","Epoch: 00 [16991/20164 ( 84%)], Train Loss: 0.47147\n","Epoch: 00 [17001/20164 ( 84%)], Train Loss: 0.47147\n","Epoch: 00 [17011/20164 ( 84%)], Train Loss: 0.47128\n","Epoch: 00 [17021/20164 ( 84%)], Train Loss: 0.47141\n","Epoch: 00 [17031/20164 ( 84%)], Train Loss: 0.47141\n","Epoch: 00 [17041/20164 ( 85%)], Train Loss: 0.47127\n","Epoch: 00 [17051/20164 ( 85%)], Train Loss: 0.47115\n","Epoch: 00 [17061/20164 ( 85%)], Train Loss: 0.47100\n","Epoch: 00 [17071/20164 ( 85%)], Train Loss: 0.47096\n","Epoch: 00 [17081/20164 ( 85%)], Train Loss: 0.47103\n","Epoch: 00 [17091/20164 ( 85%)], Train Loss: 0.47095\n","Epoch: 00 [17101/20164 ( 85%)], Train Loss: 0.47073\n","Epoch: 00 [17111/20164 ( 85%)], Train Loss: 0.47051\n","Epoch: 00 [17121/20164 ( 85%)], Train Loss: 0.47031\n","Epoch: 00 [17131/20164 ( 85%)], Train Loss: 0.47041\n","Epoch: 00 [17141/20164 ( 85%)], Train Loss: 0.47045\n","Epoch: 00 [17151/20164 ( 85%)], Train Loss: 0.47043\n","Epoch: 00 [17161/20164 ( 85%)], Train Loss: 0.47036\n","Epoch: 00 [17171/20164 ( 85%)], Train Loss: 0.47024\n","Epoch: 00 [17181/20164 ( 85%)], Train Loss: 0.47034\n","Epoch: 00 [17191/20164 ( 85%)], Train Loss: 0.47015\n","Epoch: 00 [17201/20164 ( 85%)], Train Loss: 0.47003\n","Epoch: 00 [17211/20164 ( 85%)], Train Loss: 0.46996\n","Epoch: 00 [17221/20164 ( 85%)], Train Loss: 0.46999\n","Epoch: 00 [17231/20164 ( 85%)], Train Loss: 0.46984\n","Epoch: 00 [17241/20164 ( 86%)], Train Loss: 0.46961\n","Epoch: 00 [17251/20164 ( 86%)], Train Loss: 0.46964\n","Epoch: 00 [17261/20164 ( 86%)], Train Loss: 0.46949\n","Epoch: 00 [17271/20164 ( 86%)], Train Loss: 0.46933\n","Epoch: 00 [17281/20164 ( 86%)], Train Loss: 0.46917\n","Epoch: 00 [17291/20164 ( 86%)], Train Loss: 0.46915\n","Epoch: 00 [17301/20164 ( 86%)], Train Loss: 0.46924\n","Epoch: 00 [17311/20164 ( 86%)], Train Loss: 0.46920\n","Epoch: 00 [17321/20164 ( 86%)], Train Loss: 0.46923\n","Epoch: 00 [17331/20164 ( 86%)], Train Loss: 0.46925\n","Epoch: 00 [17341/20164 ( 86%)], Train Loss: 0.46913\n","Epoch: 00 [17351/20164 ( 86%)], Train Loss: 0.46896\n","Epoch: 00 [17361/20164 ( 86%)], Train Loss: 0.46889\n","Epoch: 00 [17371/20164 ( 86%)], Train Loss: 0.46872\n","Epoch: 00 [17381/20164 ( 86%)], Train Loss: 0.46856\n","Epoch: 00 [17391/20164 ( 86%)], Train Loss: 0.46864\n","Epoch: 00 [17401/20164 ( 86%)], Train Loss: 0.46855\n","Epoch: 00 [17411/20164 ( 86%)], Train Loss: 0.46871\n","Epoch: 00 [17421/20164 ( 86%)], Train Loss: 0.46856\n","Epoch: 00 [17431/20164 ( 86%)], Train Loss: 0.46843\n","Epoch: 00 [17441/20164 ( 86%)], Train Loss: 0.46834\n","Epoch: 00 [17451/20164 ( 87%)], Train Loss: 0.46842\n","Epoch: 00 [17461/20164 ( 87%)], Train Loss: 0.46829\n","Epoch: 00 [17471/20164 ( 87%)], Train Loss: 0.46825\n","Epoch: 00 [17481/20164 ( 87%)], Train Loss: 0.46844\n","Epoch: 00 [17491/20164 ( 87%)], Train Loss: 0.46818\n","Epoch: 00 [17501/20164 ( 87%)], Train Loss: 0.46824\n","Epoch: 00 [17511/20164 ( 87%)], Train Loss: 0.46823\n","Epoch: 00 [17521/20164 ( 87%)], Train Loss: 0.46824\n","Epoch: 00 [17531/20164 ( 87%)], Train Loss: 0.46815\n","Epoch: 00 [17541/20164 ( 87%)], Train Loss: 0.46806\n","Epoch: 00 [17551/20164 ( 87%)], Train Loss: 0.46803\n","Epoch: 00 [17561/20164 ( 87%)], Train Loss: 0.46798\n","Epoch: 00 [17571/20164 ( 87%)], Train Loss: 0.46783\n","Epoch: 00 [17581/20164 ( 87%)], Train Loss: 0.46795\n","Epoch: 00 [17591/20164 ( 87%)], Train Loss: 0.46812\n","Epoch: 00 [17601/20164 ( 87%)], Train Loss: 0.46803\n","Epoch: 00 [17611/20164 ( 87%)], Train Loss: 0.46800\n","Epoch: 00 [17621/20164 ( 87%)], Train Loss: 0.46779\n","Epoch: 00 [17631/20164 ( 87%)], Train Loss: 0.46776\n","Epoch: 00 [17641/20164 ( 87%)], Train Loss: 0.46762\n","Epoch: 00 [17651/20164 ( 88%)], Train Loss: 0.46750\n","Epoch: 00 [17661/20164 ( 88%)], Train Loss: 0.46732\n","Epoch: 00 [17671/20164 ( 88%)], Train Loss: 0.46720\n","Epoch: 00 [17681/20164 ( 88%)], Train Loss: 0.46719\n","Epoch: 00 [17691/20164 ( 88%)], Train Loss: 0.46727\n","Epoch: 00 [17701/20164 ( 88%)], Train Loss: 0.46710\n","Epoch: 00 [17711/20164 ( 88%)], Train Loss: 0.46707\n","Epoch: 00 [17721/20164 ( 88%)], Train Loss: 0.46709\n","Epoch: 00 [17731/20164 ( 88%)], Train Loss: 0.46713\n","Epoch: 00 [17741/20164 ( 88%)], Train Loss: 0.46699\n","Epoch: 00 [17751/20164 ( 88%)], Train Loss: 0.46713\n","Epoch: 00 [17761/20164 ( 88%)], Train Loss: 0.46701\n","Epoch: 00 [17771/20164 ( 88%)], Train Loss: 0.46687\n","Epoch: 00 [17781/20164 ( 88%)], Train Loss: 0.46690\n","Epoch: 00 [17791/20164 ( 88%)], Train Loss: 0.46693\n","Epoch: 00 [17801/20164 ( 88%)], Train Loss: 0.46711\n","Epoch: 00 [17811/20164 ( 88%)], Train Loss: 0.46697\n","Epoch: 00 [17821/20164 ( 88%)], Train Loss: 0.46678\n","Epoch: 00 [17831/20164 ( 88%)], Train Loss: 0.46660\n","Epoch: 00 [17841/20164 ( 88%)], Train Loss: 0.46652\n","Epoch: 00 [17851/20164 ( 89%)], Train Loss: 0.46637\n","Epoch: 00 [17861/20164 ( 89%)], Train Loss: 0.46629\n","Epoch: 00 [17871/20164 ( 89%)], Train Loss: 0.46637\n","Epoch: 00 [17881/20164 ( 89%)], Train Loss: 0.46626\n","Epoch: 00 [17891/20164 ( 89%)], Train Loss: 0.46607\n","Epoch: 00 [17901/20164 ( 89%)], Train Loss: 0.46608\n","Epoch: 00 [17911/20164 ( 89%)], Train Loss: 0.46596\n","Epoch: 00 [17921/20164 ( 89%)], Train Loss: 0.46601\n","Epoch: 00 [17931/20164 ( 89%)], Train Loss: 0.46588\n","Epoch: 00 [17941/20164 ( 89%)], Train Loss: 0.46591\n","Epoch: 00 [17951/20164 ( 89%)], Train Loss: 0.46599\n","Epoch: 00 [17961/20164 ( 89%)], Train Loss: 0.46599\n","Epoch: 00 [17971/20164 ( 89%)], Train Loss: 0.46620\n","Epoch: 00 [17981/20164 ( 89%)], Train Loss: 0.46597\n","Epoch: 00 [17991/20164 ( 89%)], Train Loss: 0.46600\n","Epoch: 00 [18001/20164 ( 89%)], Train Loss: 0.46588\n","Epoch: 00 [18011/20164 ( 89%)], Train Loss: 0.46587\n","Epoch: 00 [18021/20164 ( 89%)], Train Loss: 0.46608\n","Epoch: 00 [18031/20164 ( 89%)], Train Loss: 0.46588\n","Epoch: 00 [18041/20164 ( 89%)], Train Loss: 0.46572\n","Epoch: 00 [18051/20164 ( 90%)], Train Loss: 0.46557\n","Epoch: 00 [18061/20164 ( 90%)], Train Loss: 0.46540\n","Epoch: 00 [18071/20164 ( 90%)], Train Loss: 0.46536\n","Epoch: 00 [18081/20164 ( 90%)], Train Loss: 0.46526\n","Epoch: 00 [18091/20164 ( 90%)], Train Loss: 0.46501\n","Epoch: 00 [18101/20164 ( 90%)], Train Loss: 0.46485\n","Epoch: 00 [18111/20164 ( 90%)], Train Loss: 0.46477\n","Epoch: 00 [18121/20164 ( 90%)], Train Loss: 0.46462\n","Epoch: 00 [18131/20164 ( 90%)], Train Loss: 0.46484\n","Epoch: 00 [18141/20164 ( 90%)], Train Loss: 0.46474\n","Epoch: 00 [18151/20164 ( 90%)], Train Loss: 0.46467\n","Epoch: 00 [18161/20164 ( 90%)], Train Loss: 0.46459\n","Epoch: 00 [18171/20164 ( 90%)], Train Loss: 0.46449\n","Epoch: 00 [18181/20164 ( 90%)], Train Loss: 0.46440\n","Epoch: 00 [18191/20164 ( 90%)], Train Loss: 0.46451\n","Epoch: 00 [18201/20164 ( 90%)], Train Loss: 0.46440\n","Epoch: 00 [18211/20164 ( 90%)], Train Loss: 0.46417\n","Epoch: 00 [18221/20164 ( 90%)], Train Loss: 0.46415\n","Epoch: 00 [18231/20164 ( 90%)], Train Loss: 0.46446\n","Epoch: 00 [18241/20164 ( 90%)], Train Loss: 0.46460\n","Epoch: 00 [18251/20164 ( 91%)], Train Loss: 0.46459\n","Epoch: 00 [18261/20164 ( 91%)], Train Loss: 0.46446\n","Epoch: 00 [18271/20164 ( 91%)], Train Loss: 0.46427\n","Epoch: 00 [18281/20164 ( 91%)], Train Loss: 0.46408\n","Epoch: 00 [18291/20164 ( 91%)], Train Loss: 0.46419\n","Epoch: 00 [18301/20164 ( 91%)], Train Loss: 0.46406\n","Epoch: 00 [18311/20164 ( 91%)], Train Loss: 0.46425\n","Epoch: 00 [18321/20164 ( 91%)], Train Loss: 0.46408\n","Epoch: 00 [18331/20164 ( 91%)], Train Loss: 0.46412\n","Epoch: 00 [18341/20164 ( 91%)], Train Loss: 0.46405\n","Epoch: 00 [18351/20164 ( 91%)], Train Loss: 0.46414\n","Epoch: 00 [18361/20164 ( 91%)], Train Loss: 0.46398\n","Epoch: 00 [18371/20164 ( 91%)], Train Loss: 0.46386\n","Epoch: 00 [18381/20164 ( 91%)], Train Loss: 0.46374\n","Epoch: 00 [18391/20164 ( 91%)], Train Loss: 0.46352\n","Epoch: 00 [18401/20164 ( 91%)], Train Loss: 0.46348\n","Epoch: 00 [18411/20164 ( 91%)], Train Loss: 0.46348\n","Epoch: 00 [18421/20164 ( 91%)], Train Loss: 0.46337\n","Epoch: 00 [18431/20164 ( 91%)], Train Loss: 0.46327\n","Epoch: 00 [18441/20164 ( 91%)], Train Loss: 0.46322\n","Epoch: 00 [18451/20164 ( 92%)], Train Loss: 0.46310\n","Epoch: 00 [18461/20164 ( 92%)], Train Loss: 0.46293\n","Epoch: 00 [18471/20164 ( 92%)], Train Loss: 0.46290\n","Epoch: 00 [18481/20164 ( 92%)], Train Loss: 0.46276\n","Epoch: 00 [18491/20164 ( 92%)], Train Loss: 0.46294\n","Epoch: 00 [18501/20164 ( 92%)], Train Loss: 0.46279\n","Epoch: 00 [18511/20164 ( 92%)], Train Loss: 0.46262\n","Epoch: 00 [18521/20164 ( 92%)], Train Loss: 0.46285\n","Epoch: 00 [18531/20164 ( 92%)], Train Loss: 0.46267\n","Epoch: 00 [18541/20164 ( 92%)], Train Loss: 0.46265\n","Epoch: 00 [18551/20164 ( 92%)], Train Loss: 0.46251\n","Epoch: 00 [18561/20164 ( 92%)], Train Loss: 0.46247\n","Epoch: 00 [18571/20164 ( 92%)], Train Loss: 0.46239\n","Epoch: 00 [18581/20164 ( 92%)], Train Loss: 0.46235\n","Epoch: 00 [18591/20164 ( 92%)], Train Loss: 0.46234\n","Epoch: 00 [18601/20164 ( 92%)], Train Loss: 0.46218\n","Epoch: 00 [18611/20164 ( 92%)], Train Loss: 0.46205\n","Epoch: 00 [18621/20164 ( 92%)], Train Loss: 0.46227\n","Epoch: 00 [18631/20164 ( 92%)], Train Loss: 0.46220\n","Epoch: 00 [18641/20164 ( 92%)], Train Loss: 0.46199\n","Epoch: 00 [18651/20164 ( 92%)], Train Loss: 0.46180\n","Epoch: 00 [18661/20164 ( 93%)], Train Loss: 0.46172\n","Epoch: 00 [18671/20164 ( 93%)], Train Loss: 0.46175\n","Epoch: 00 [18681/20164 ( 93%)], Train Loss: 0.46164\n","Epoch: 00 [18691/20164 ( 93%)], Train Loss: 0.46179\n","Epoch: 00 [18701/20164 ( 93%)], Train Loss: 0.46202\n","Epoch: 00 [18711/20164 ( 93%)], Train Loss: 0.46210\n","Epoch: 00 [18721/20164 ( 93%)], Train Loss: 0.46201\n","Epoch: 00 [18731/20164 ( 93%)], Train Loss: 0.46182\n","Epoch: 00 [18741/20164 ( 93%)], Train Loss: 0.46174\n","Epoch: 00 [18751/20164 ( 93%)], Train Loss: 0.46172\n","Epoch: 00 [18761/20164 ( 93%)], Train Loss: 0.46174\n","Epoch: 00 [18771/20164 ( 93%)], Train Loss: 0.46169\n","Epoch: 00 [18781/20164 ( 93%)], Train Loss: 0.46167\n","Epoch: 00 [18791/20164 ( 93%)], Train Loss: 0.46152\n","Epoch: 00 [18801/20164 ( 93%)], Train Loss: 0.46147\n","Epoch: 00 [18811/20164 ( 93%)], Train Loss: 0.46142\n","Epoch: 00 [18821/20164 ( 93%)], Train Loss: 0.46144\n","Epoch: 00 [18831/20164 ( 93%)], Train Loss: 0.46131\n","Epoch: 00 [18841/20164 ( 93%)], Train Loss: 0.46107\n","Epoch: 00 [18851/20164 ( 93%)], Train Loss: 0.46097\n","Epoch: 00 [18861/20164 ( 94%)], Train Loss: 0.46097\n","Epoch: 00 [18871/20164 ( 94%)], Train Loss: 0.46083\n","Epoch: 00 [18881/20164 ( 94%)], Train Loss: 0.46091\n","Epoch: 00 [18891/20164 ( 94%)], Train Loss: 0.46072\n","Epoch: 00 [18901/20164 ( 94%)], Train Loss: 0.46060\n","Epoch: 00 [18911/20164 ( 94%)], Train Loss: 0.46060\n","Epoch: 00 [18921/20164 ( 94%)], Train Loss: 0.46047\n","Epoch: 00 [18931/20164 ( 94%)], Train Loss: 0.46046\n","Epoch: 00 [18941/20164 ( 94%)], Train Loss: 0.46055\n","Epoch: 00 [18951/20164 ( 94%)], Train Loss: 0.46041\n","Epoch: 00 [18961/20164 ( 94%)], Train Loss: 0.46037\n","Epoch: 00 [18971/20164 ( 94%)], Train Loss: 0.46018\n","Epoch: 00 [18981/20164 ( 94%)], Train Loss: 0.46050\n","Epoch: 00 [18991/20164 ( 94%)], Train Loss: 0.46082\n","Epoch: 00 [19001/20164 ( 94%)], Train Loss: 0.46080\n","Epoch: 00 [19011/20164 ( 94%)], Train Loss: 0.46067\n","Epoch: 00 [19021/20164 ( 94%)], Train Loss: 0.46052\n","Epoch: 00 [19031/20164 ( 94%)], Train Loss: 0.46053\n","Epoch: 00 [19041/20164 ( 94%)], Train Loss: 0.46047\n","Epoch: 00 [19051/20164 ( 94%)], Train Loss: 0.46027\n","Epoch: 00 [19061/20164 ( 95%)], Train Loss: 0.46008\n","Epoch: 00 [19071/20164 ( 95%)], Train Loss: 0.45999\n","Epoch: 00 [19081/20164 ( 95%)], Train Loss: 0.46007\n","Epoch: 00 [19091/20164 ( 95%)], Train Loss: 0.46009\n","Epoch: 00 [19101/20164 ( 95%)], Train Loss: 0.45993\n","Epoch: 00 [19111/20164 ( 95%)], Train Loss: 0.45987\n","Epoch: 00 [19121/20164 ( 95%)], Train Loss: 0.45996\n","Epoch: 00 [19131/20164 ( 95%)], Train Loss: 0.45977\n","Epoch: 00 [19141/20164 ( 95%)], Train Loss: 0.45968\n","Epoch: 00 [19151/20164 ( 95%)], Train Loss: 0.45963\n","Epoch: 00 [19161/20164 ( 95%)], Train Loss: 0.45950\n","Epoch: 00 [19171/20164 ( 95%)], Train Loss: 0.45966\n","Epoch: 00 [19181/20164 ( 95%)], Train Loss: 0.45942\n","Epoch: 00 [19191/20164 ( 95%)], Train Loss: 0.45934\n","Epoch: 00 [19201/20164 ( 95%)], Train Loss: 0.45916\n","Epoch: 00 [19211/20164 ( 95%)], Train Loss: 0.45928\n","Epoch: 00 [19221/20164 ( 95%)], Train Loss: 0.45914\n","Epoch: 00 [19231/20164 ( 95%)], Train Loss: 0.45912\n","Epoch: 00 [19241/20164 ( 95%)], Train Loss: 0.45892\n","Epoch: 00 [19251/20164 ( 95%)], Train Loss: 0.45876\n","Epoch: 00 [19261/20164 ( 96%)], Train Loss: 0.45858\n","Epoch: 00 [19271/20164 ( 96%)], Train Loss: 0.45840\n","Epoch: 00 [19281/20164 ( 96%)], Train Loss: 0.45817\n","Epoch: 00 [19291/20164 ( 96%)], Train Loss: 0.45809\n","Epoch: 00 [19301/20164 ( 96%)], Train Loss: 0.45802\n","Epoch: 00 [19311/20164 ( 96%)], Train Loss: 0.45796\n","Epoch: 00 [19321/20164 ( 96%)], Train Loss: 0.45778\n","Epoch: 00 [19331/20164 ( 96%)], Train Loss: 0.45764\n","Epoch: 00 [19341/20164 ( 96%)], Train Loss: 0.45755\n","Epoch: 00 [19351/20164 ( 96%)], Train Loss: 0.45757\n","Epoch: 00 [19361/20164 ( 96%)], Train Loss: 0.45747\n","Epoch: 00 [19371/20164 ( 96%)], Train Loss: 0.45730\n","Epoch: 00 [19381/20164 ( 96%)], Train Loss: 0.45711\n","Epoch: 00 [19391/20164 ( 96%)], Train Loss: 0.45706\n","Epoch: 00 [19401/20164 ( 96%)], Train Loss: 0.45704\n","Epoch: 00 [19411/20164 ( 96%)], Train Loss: 0.45682\n","Epoch: 00 [19421/20164 ( 96%)], Train Loss: 0.45682\n","Epoch: 00 [19431/20164 ( 96%)], Train Loss: 0.45678\n","Epoch: 00 [19441/20164 ( 96%)], Train Loss: 0.45688\n","Epoch: 00 [19451/20164 ( 96%)], Train Loss: 0.45685\n","Epoch: 00 [19461/20164 ( 97%)], Train Loss: 0.45682\n","Epoch: 00 [19471/20164 ( 97%)], Train Loss: 0.45678\n","Epoch: 00 [19481/20164 ( 97%)], Train Loss: 0.45664\n","Epoch: 00 [19491/20164 ( 97%)], Train Loss: 0.45646\n","Epoch: 00 [19501/20164 ( 97%)], Train Loss: 0.45633\n","Epoch: 00 [19511/20164 ( 97%)], Train Loss: 0.45622\n","Epoch: 00 [19521/20164 ( 97%)], Train Loss: 0.45604\n","Epoch: 00 [19531/20164 ( 97%)], Train Loss: 0.45597\n","Epoch: 00 [19541/20164 ( 97%)], Train Loss: 0.45582\n","Epoch: 00 [19551/20164 ( 97%)], Train Loss: 0.45572\n","Epoch: 00 [19561/20164 ( 97%)], Train Loss: 0.45565\n","Epoch: 00 [19571/20164 ( 97%)], Train Loss: 0.45551\n","Epoch: 00 [19581/20164 ( 97%)], Train Loss: 0.45533\n","Epoch: 00 [19591/20164 ( 97%)], Train Loss: 0.45526\n","Epoch: 00 [19601/20164 ( 97%)], Train Loss: 0.45531\n","Epoch: 00 [19611/20164 ( 97%)], Train Loss: 0.45522\n","Epoch: 00 [19621/20164 ( 97%)], Train Loss: 0.45532\n","Epoch: 00 [19631/20164 ( 97%)], Train Loss: 0.45524\n","Epoch: 00 [19641/20164 ( 97%)], Train Loss: 0.45504\n","Epoch: 00 [19651/20164 ( 97%)], Train Loss: 0.45499\n","Epoch: 00 [19661/20164 ( 98%)], Train Loss: 0.45491\n","Epoch: 00 [19671/20164 ( 98%)], Train Loss: 0.45478\n","Epoch: 00 [19681/20164 ( 98%)], Train Loss: 0.45471\n","Epoch: 00 [19691/20164 ( 98%)], Train Loss: 0.45462\n","Epoch: 00 [19701/20164 ( 98%)], Train Loss: 0.45488\n","Epoch: 00 [19711/20164 ( 98%)], Train Loss: 0.45484\n","Epoch: 00 [19721/20164 ( 98%)], Train Loss: 0.45488\n","Epoch: 00 [19731/20164 ( 98%)], Train Loss: 0.45494\n","Epoch: 00 [19741/20164 ( 98%)], Train Loss: 0.45500\n","Epoch: 00 [19751/20164 ( 98%)], Train Loss: 0.45497\n","Epoch: 00 [19761/20164 ( 98%)], Train Loss: 0.45481\n","Epoch: 00 [19771/20164 ( 98%)], Train Loss: 0.45467\n","Epoch: 00 [19781/20164 ( 98%)], Train Loss: 0.45459\n","Epoch: 00 [19791/20164 ( 98%)], Train Loss: 0.45444\n","Epoch: 00 [19801/20164 ( 98%)], Train Loss: 0.45432\n","Epoch: 00 [19811/20164 ( 98%)], Train Loss: 0.45434\n","Epoch: 00 [19821/20164 ( 98%)], Train Loss: 0.45436\n","Epoch: 00 [19831/20164 ( 98%)], Train Loss: 0.45426\n","Epoch: 00 [19841/20164 ( 98%)], Train Loss: 0.45414\n","Epoch: 00 [19851/20164 ( 98%)], Train Loss: 0.45410\n","Epoch: 00 [19861/20164 ( 98%)], Train Loss: 0.45406\n","Epoch: 00 [19871/20164 ( 99%)], Train Loss: 0.45399\n","Epoch: 00 [19881/20164 ( 99%)], Train Loss: 0.45381\n","Epoch: 00 [19891/20164 ( 99%)], Train Loss: 0.45367\n","Epoch: 00 [19901/20164 ( 99%)], Train Loss: 0.45366\n","Epoch: 00 [19911/20164 ( 99%)], Train Loss: 0.45362\n","Epoch: 00 [19921/20164 ( 99%)], Train Loss: 0.45346\n","Epoch: 00 [19931/20164 ( 99%)], Train Loss: 0.45332\n","Epoch: 00 [19941/20164 ( 99%)], Train Loss: 0.45323\n","Epoch: 00 [19951/20164 ( 99%)], Train Loss: 0.45309\n","Epoch: 00 [19961/20164 ( 99%)], Train Loss: 0.45290\n","Epoch: 00 [19971/20164 ( 99%)], Train Loss: 0.45312\n","Epoch: 00 [19981/20164 ( 99%)], Train Loss: 0.45295\n","Epoch: 00 [19991/20164 ( 99%)], Train Loss: 0.45323\n","Epoch: 00 [20001/20164 ( 99%)], Train Loss: 0.45323\n","Epoch: 00 [20011/20164 ( 99%)], Train Loss: 0.45326\n","Epoch: 00 [20021/20164 ( 99%)], Train Loss: 0.45326\n","Epoch: 00 [20031/20164 ( 99%)], Train Loss: 0.45337\n","Epoch: 00 [20041/20164 ( 99%)], Train Loss: 0.45340\n","Epoch: 00 [20051/20164 ( 99%)], Train Loss: 0.45330\n","Epoch: 00 [20061/20164 ( 99%)], Train Loss: 0.45325\n","Epoch: 00 [20071/20164 (100%)], Train Loss: 0.45317\n","Epoch: 00 [20081/20164 (100%)], Train Loss: 0.45328\n","Epoch: 00 [20091/20164 (100%)], Train Loss: 0.45336\n","Epoch: 00 [20101/20164 (100%)], Train Loss: 0.45343\n","Epoch: 00 [20111/20164 (100%)], Train Loss: 0.45331\n","Epoch: 00 [20121/20164 (100%)], Train Loss: 0.45328\n","Epoch: 00 [20131/20164 (100%)], Train Loss: 0.45317\n","Epoch: 00 [20141/20164 (100%)], Train Loss: 0.45313\n","Epoch: 00 [20151/20164 (100%)], Train Loss: 0.45306\n","Epoch: 00 [20161/20164 (100%)], Train Loss: 0.45305\n","Epoch: 00 [20164/20164 (100%)], Train Loss: 0.45301\n","----Validation Results Summary----\n","Epoch: [0] Valid Loss: 0.22396\n","0 Epoch, Best epoch was updated! Valid Loss: 0.22396\n","Saving model checkpoint to output/checkpoint-fold-4.\n","\n","Total Training Time: 13405.836340904236secs, Average Training Time per Epoch: 13405.836340904236secs.\n","Total Validation Time: 532.119711637497secs, Average Validation Time per Epoch: 532.119711637497secs.\n"]}]},{"cell_type":"code","metadata":{"id":"o6-F5rzbHSK5","executionInfo":{"status":"ok","timestamp":1635311403584,"user_tz":-330,"elapsed":54222,"user":{"displayName":"2020 11004","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06803819529238251334"}}},"source":["! cp -r /content/output/checkpoint-fold-4 /content/drive/Shareddrives/NLP/Dataset/murli"],"execution_count":19,"outputs":[]}]}
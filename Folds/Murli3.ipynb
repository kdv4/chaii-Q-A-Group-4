{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Murli3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MxxpnD8XZQUO","executionInfo":{"status":"ok","timestamp":1635352092525,"user_tz":-330,"elapsed":57460,"user":{"displayName":"2020 11034","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11154967412412261153"}},"outputId":"6cd3f090-d5b7-4e47-8852-b2fceb6a59cd"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"faIyP9zZEd0G","executionInfo":{"status":"ok","timestamp":1635352104616,"user_tz":-330,"elapsed":8989,"user":{"displayName":"2020 11034","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11154967412412261153"}},"outputId":"5d464306-4abe-48db-9728-85efa532bad6"},"source":["! pip install transformers sentencepiece "],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 13.1 MB/s \n","\u001b[?25hCollecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 37.4 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 43.5 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 44.8 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 42.1 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Collecting huggingface-hub>=0.0.17\n","  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n","\u001b[K     |████████████████████████████████| 56 kB 4.4 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.19 pyyaml-6.0 sacremoses-0.0.46 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.11.3\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jDu4BWB1EJJ3","executionInfo":{"status":"ok","timestamp":1635352132684,"user_tz":-330,"elapsed":28074,"user":{"displayName":"2020 11034","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11154967412412261153"}},"outputId":"8e9e155d-fd60-46f5-d06b-ad74e5bbbdc8"},"source":["import os\n","import gc\n","gc.enable()\n","import math\n","import json\n","import time\n","import random\n","import multiprocessing\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm, trange\n","from sklearn import model_selection\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import Parameter\n","import torch.optim as optim\n","from torch.utils.data import (\n","    Dataset, DataLoader,\n","    SequentialSampler, RandomSampler\n",")\n","from torch.utils.data.distributed import DistributedSampler\n","\n","try:\n","    from apex import amp\n","    APEX_INSTALLED = True\n","except ImportError:\n","    APEX_INSTALLED = False\n","\n","import transformers\n","from transformers import (\n","    WEIGHTS_NAME,\n","    AdamW,\n","    AutoConfig,\n","    AutoModel,\n","    AutoTokenizer,\n","    get_cosine_schedule_with_warmup,\n","    get_linear_schedule_with_warmup,\n","    logging,\n","    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n",")\n","logging.set_verbosity_warning()\n","logging.set_verbosity_error()\n","\n","def fix_all_seeds(seed):\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","def optimal_num_of_loader_workers():\n","    num_cpus = multiprocessing.cpu_count()\n","    num_gpus = torch.cuda.device_count()\n","    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n","    return optimal_value\n","\n","print(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\n","MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n","MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Apex AMP Installed :: False\n"]}]},{"cell_type":"code","metadata":{"id":"t8HfXz6SHhE7","executionInfo":{"status":"ok","timestamp":1635352132686,"user_tz":-330,"elapsed":22,"user":{"displayName":"2020 11034","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11154967412412261153"}}},"source":["class Config:\n","    # model\n","    model_type = 'bert'\n","    model_name_or_path = \"/content/drive/Shareddrives/NLP/Tarang/xlm-roberta-large-squad2/model\"\n","    config_name = \"/content/drive/Shareddrives/NLP/Tarang/xlm-roberta-large-squad2/model\"\n","    fp16 = True if APEX_INSTALLED else False\n","    fp16_opt_level = \"O1\"\n","    gradient_accumulation_steps = 2\n","\n","    # tokenizer\n","    tokenizer_name = \"/content/drive/Shareddrives/NLP/Tarang/xlm-roberta-large-squad2/tok\"\n","    max_seq_length = 384\n","    doc_stride = 128\n","\n","    # train\n","    epochs = 1\n","    train_batch_size = 1\n","    eval_batch_size = 1\n","\n","    # optimizer\n","    optimizer_type = 'AdamW'\n","    learning_rate = 1.5e-5\n","    weight_decay = 1e-2\n","    epsilon = 1e-8\n","    max_grad_norm = 1.0\n","\n","    # scheduler\n","    decay_name = 'linear-warmup'\n","    warmup_ratio = 0.1\n","\n","    # logging\n","    logging_steps = 10\n","\n","    # evaluate\n","    output_dir = 'output'\n","    seed = 42"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jzv3krJSETEs","executionInfo":{"status":"ok","timestamp":1635352138113,"user_tz":-330,"elapsed":4715,"user":{"displayName":"2020 11034","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11154967412412261153"}}},"source":["train = pd.read_csv('/content/drive/Shareddrives/NLP/Dataset/train.csv')\n","test = pd.read_csv('/content/drive/Shareddrives/NLP/Dataset/test.csv')\n","external_mlqa = pd.read_csv('/content/drive/Shareddrives/NLP/Dataset/mlqa_hindi.csv')\n","external_xquad = pd.read_csv('/content/drive/Shareddrives/NLP/Dataset/xquad.csv')\n","external_train = pd.concat([external_mlqa, external_xquad])"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"wNv3o7qeFmaB","executionInfo":{"status":"ok","timestamp":1635352138114,"user_tz":-330,"elapsed":37,"user":{"displayName":"2020 11034","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11154967412412261153"}}},"source":["def create_folds(data, num_splits):\n","    data[\"kfold\"] = -1\n","    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=69)\n","    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n","        data.loc[v_, 'kfold'] = f\n","    return data\n","\n","train = create_folds(train, num_splits=5)\n","external_train[\"kfold\"] = -1\n","external_train['id'] = list(np.arange(1, len(external_train)+1))\n","train = pd.concat([train, external_train]).reset_index(drop=True)\n","\n","def convert_answers(row):\n","    return {'answer_start': [row[0]], 'text': [row[1]]}\n","\n","train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"dVmPKiJZGKGc","executionInfo":{"status":"ok","timestamp":1635352138115,"user_tz":-330,"elapsed":34,"user":{"displayName":"2020 11034","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11154967412412261153"}}},"source":["def prepare_train_features(args, example, tokenizer):\n","    example[\"question\"] = example[\"question\"].lstrip()\n","    tokenized_example = tokenizer(\n","        example[\"question\"],\n","        example[\"context\"],\n","        truncation=\"only_second\",\n","        max_length=args.max_seq_length,\n","        stride=args.doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n","    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n","\n","    features = []\n","    for i, offsets in enumerate(offset_mapping):\n","        feature = {}\n","\n","        input_ids = tokenized_example[\"input_ids\"][i]\n","        attention_mask = tokenized_example[\"attention_mask\"][i]\n","\n","        feature['input_ids'] = input_ids\n","        feature['attention_mask'] = attention_mask\n","        feature['offset_mapping'] = offsets\n","\n","        cls_index = input_ids.index(tokenizer.cls_token_id)\n","        sequence_ids = tokenized_example.sequence_ids(i)\n","\n","        sample_index = sample_mapping[i]\n","        answers = example[\"answers\"]\n","\n","        if len(answers[\"answer_start\"]) == 0:\n","            feature[\"start_position\"] = cls_index\n","            feature[\"end_position\"] = cls_index\n","        else:\n","            start_char = answers[\"answer_start\"][0]\n","            end_char = start_char + len(answers[\"text\"][0])\n","\n","            token_start_index = 0\n","            while sequence_ids[token_start_index] != 1:\n","                token_start_index += 1\n","\n","            token_end_index = len(input_ids) - 1\n","            while sequence_ids[token_end_index] != 1:\n","                token_end_index -= 1\n","\n","            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n","                feature[\"start_position\"] = cls_index\n","                feature[\"end_position\"] = cls_index\n","            else:\n","                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n","                    token_start_index += 1\n","                feature[\"start_position\"] = token_start_index - 1\n","                while offsets[token_end_index][1] >= end_char:\n","                    token_end_index -= 1\n","                feature[\"end_position\"] = token_end_index + 1\n","\n","        features.append(feature)\n","    return features"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"1X5iXyuyGRuH","executionInfo":{"status":"ok","timestamp":1635352138116,"user_tz":-330,"elapsed":32,"user":{"displayName":"2020 11034","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11154967412412261153"}}},"source":["class DatasetRetriever(Dataset):\n","    def __init__(self, features, mode='train'):\n","        super(DatasetRetriever, self).__init__()\n","        self.features = features\n","        self.mode = mode\n","        \n","    def __len__(self):\n","        return len(self.features)\n","    \n","    def __getitem__(self, item):   \n","        feature = self.features[item]\n","        if self.mode == 'train':\n","            return {\n","                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n","                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n","                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n","                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n","                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n","            }\n","        else:\n","            return {\n","                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n","                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n","                'offset_mapping':feature['offset_mapping'],\n","                'sequence_ids':feature['sequence_ids'],\n","                'id':feature['example_id'],\n","                'context': feature['context'],\n","                'question': feature['question']\n","            }"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"c83EghN4GXie","executionInfo":{"status":"ok","timestamp":1635352138117,"user_tz":-330,"elapsed":31,"user":{"displayName":"2020 11034","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11154967412412261153"}}},"source":["class Model(nn.Module):\n","    def __init__(self, modelname_or_path, config):\n","        super(Model, self).__init__()\n","        self.config = config\n","        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n","        self.linear_layer = nn.Linear(config.hidden_size, 64)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.qa_outputs = nn.Linear(64, 2)\n","        self._init_weights(self.qa_outputs)\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","\n","    def forward(\n","        self, \n","        input_ids, \n","        attention_mask=None, \n","    ):\n","        outputs = self.xlm_roberta(input_ids,attention_mask=attention_mask)\n","\n","        sequence_output = outputs[0]\n","        pooled_output = outputs[1]\n","        \n","        linear_output = self.linear_layer(sequence_output)\n","        linear_output = self.dropout(linear_output)\n","        qa_logits = self.qa_outputs(linear_output)\n","        \n","        start_logits, end_logits = qa_logits.split(1, dim=-1)\n","        start_logits = start_logits.squeeze(-1)\n","        end_logits = end_logits.squeeze(-1)\n","    \n","        return start_logits, end_logits"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"gKn4pHnvGc4H","executionInfo":{"status":"ok","timestamp":1635352138118,"user_tz":-330,"elapsed":30,"user":{"displayName":"2020 11034","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11154967412412261153"}}},"source":["def loss_fn(preds, labels):\n","    start_preds, end_preds = preds\n","    start_labels, end_labels = labels\n","    \n","    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n","    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n","    total_loss = (start_loss + end_loss) / 2\n","    return total_loss"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"qoV8_pn6GgdW","executionInfo":{"status":"ok","timestamp":1635352138120,"user_tz":-330,"elapsed":29,"user":{"displayName":"2020 11034","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11154967412412261153"}}},"source":["def get_optimizer_grouped_parameters(args, model):\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n","    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n","    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n","    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': args.weight_decay},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': args.weight_decay, 'lr': args.learning_rate/2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': args.weight_decay, 'lr': args.learning_rate},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': args.weight_decay, 'lr': args.learning_rate*2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': args.learning_rate/2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': args.learning_rate},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': args.learning_rate*2.6},\n","        {'params': [p for n, p in model.named_parameters() if args.model_type not in n], 'lr':args.learning_rate*20, \"weight_decay\": 0.0},\n","    ]\n","    return optimizer_grouped_parameters"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"xp_uEUijGrCv","executionInfo":{"status":"ok","timestamp":1635352138121,"user_tz":-330,"elapsed":27,"user":{"displayName":"2020 11034","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11154967412412261153"}}},"source":["class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","        self.max = 0\n","        self.min = 1e5\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","        if val > self.max:\n","            self.max = val\n","        if val < self.min:\n","            self.min = val"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"RKcK3xENGvQ4","executionInfo":{"status":"ok","timestamp":1635352138836,"user_tz":-330,"elapsed":741,"user":{"displayName":"2020 11034","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11154967412412261153"}}},"source":["def make_model(args):\n","    config = AutoConfig.from_pretrained(args.config_name)\n","    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n","    model = Model(args.model_name_or_path, config=config)\n","    return config, tokenizer, model\n","\n","def make_optimizer(args, model):\n","    optimizer_grouped_parameters = get_optimizer_grouped_parameters(args, model)\n","#     no_decay = [\"bias\", \"LayerNorm.weight\"]\n","#     optimizer_grouped_parameters = [\n","#         {\n","#             \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","#             \"weight_decay\": args.weight_decay,\n","#         },\n","#         {\n","#             \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","#             \"weight_decay\": 0.0,\n","#         },\n","#     ]\n","    if args.optimizer_type == \"AdamW\":\n","        optimizer = AdamW(\n","            optimizer_grouped_parameters,\n","            lr=args.learning_rate,\n","            eps=args.epsilon,\n","            correct_bias=True\n","        )\n","        return optimizer\n","\n","def make_scheduler(\n","    args, optimizer, \n","    num_warmup_steps, \n","    num_training_steps\n","):\n","    if args.decay_name == \"cosine-warmup\":\n","        scheduler = get_cosine_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps\n","        )\n","    else:\n","        scheduler = get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps\n","        )\n","    return scheduler    \n","\n","def make_loader(\n","    args, data, \n","    tokenizer, fold\n","):\n","    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n","    \n","    train_features, valid_features = [[] for _ in range(2)]\n","    for i, row in train_set.iterrows():\n","        train_features += prepare_train_features(args, row, tokenizer)\n","    for i, row in valid_set.iterrows():\n","        valid_features += prepare_train_features(args, row, tokenizer)\n","\n","    train_dataset = DatasetRetriever(train_features)\n","    valid_dataset = DatasetRetriever(valid_features)\n","    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n","    \n","    train_sampler = RandomSampler(train_dataset)\n","    valid_sampler = SequentialSampler(valid_dataset)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=args.train_batch_size,\n","        sampler=train_sampler,\n","        num_workers=optimal_num_of_loader_workers(),\n","        pin_memory=True,\n","        drop_last=False \n","    )\n","\n","    valid_dataloader = DataLoader(\n","        valid_dataset,\n","        batch_size=args.eval_batch_size, \n","        sampler=valid_sampler,\n","        num_workers=optimal_num_of_loader_workers(),\n","        pin_memory=True, \n","        drop_last=False\n","    )\n","\n","    return train_dataloader, valid_dataloader"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"YYXpJ2UaG4ft","executionInfo":{"status":"ok","timestamp":1635352138840,"user_tz":-330,"elapsed":18,"user":{"displayName":"2020 11034","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11154967412412261153"}}},"source":["class Trainer:\n","    def __init__(\n","        self, model, tokenizer, \n","        optimizer, scheduler\n","    ):\n","        self.model = model\n","        self.tokenizer = tokenizer\n","        self.optimizer = optimizer\n","        self.scheduler = scheduler\n","\n","    def train(\n","        self, args, \n","        train_dataloader, \n","        epoch, result_dict\n","    ):\n","        count = 0\n","        losses = AverageMeter()\n","        \n","        self.model.zero_grad()\n","        self.model.train()\n","        \n","        fix_all_seeds(args.seed)\n","        \n","        for batch_idx, batch_data in enumerate(train_dataloader):\n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                batch_data['input_ids'], batch_data['attention_mask'], \\\n","                    batch_data['start_position'], batch_data['end_position']\n","            \n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n","\n","            outputs_start, outputs_end = self.model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","            )\n","            \n","            loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n","            loss = loss / args.gradient_accumulation_steps\n","\n","            if args.fp16:\n","                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","            else:\n","                loss.backward()\n","\n","            count += input_ids.size(0)\n","            losses.update(loss.item(), input_ids.size(0))\n","\n","            # if args.fp16:\n","            #     torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), args.max_grad_norm)\n","            # else:\n","            #     torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.max_grad_norm)\n","\n","            if batch_idx % args.gradient_accumulation_steps == 0 or batch_idx == len(train_dataloader) - 1:\n","                self.optimizer.step()\n","                self.scheduler.step()\n","                self.optimizer.zero_grad()\n","\n","            if (batch_idx % args.logging_steps == 0) or (batch_idx+1)==len(train_dataloader):\n","                _s = str(len(str(len(train_dataloader.sampler))))\n","                ret = [\n","                    ('Epoch: {:0>2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_dataloader.sampler), 100 * count / len(train_dataloader.sampler)),\n","                    'Train Loss: {: >4.5f}'.format(losses.avg),\n","                ]\n","                print(', '.join(ret))\n","\n","        result_dict['train_loss'].append(losses.avg)\n","        return result_dict"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"c7lo_72cG88t","executionInfo":{"status":"ok","timestamp":1635352141923,"user_tz":-330,"elapsed":932,"user":{"displayName":"2020 11034","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11154967412412261153"}}},"source":["class Evaluator:\n","    def __init__(self, model):\n","        self.model = model\n","    \n","    def save(self, result, output_dir):\n","        with open(f'{output_dir}/result_dict.json', 'w') as f:\n","            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n","\n","    def evaluate(self, valid_dataloader, epoch, result_dict):\n","        losses = AverageMeter()\n","        for batch_idx, batch_data in enumerate(valid_dataloader):\n","            self.model = self.model.eval()\n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                batch_data['input_ids'], batch_data['attention_mask'], \\\n","                    batch_data['start_position'], batch_data['end_position']\n","            \n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n","            \n","            with torch.no_grad():            \n","                outputs_start, outputs_end = self.model(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                )\n","                \n","                loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n","                losses.update(loss.item(), input_ids.size(0))\n","                \n","        print('----Validation Results Summary----')\n","        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n","        result_dict['val_loss'].append(losses.avg)        \n","        return result_dict"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wv14GLumHBTO","executionInfo":{"status":"ok","timestamp":1635352146070,"user_tz":-330,"elapsed":7,"user":{"displayName":"2020 11034","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11154967412412261153"}}},"source":["def init_training(args, data, fold):\n","    fix_all_seeds(args.seed)\n","    \n","    if not os.path.exists(args.output_dir):\n","        os.makedirs(args.output_dir)\n","    \n","    # model\n","    model_config, tokenizer, model = make_model(args)\n","    if torch.cuda.device_count() >= 1:\n","        print('Model pushed to {} GPU(s), type {}.'.format(\n","            torch.cuda.device_count(), \n","            torch.cuda.get_device_name(0))\n","        )\n","        model = model.cuda() \n","    else:\n","        raise ValueError('CPU training is not supported')\n","    \n","    # data loaders\n","    train_dataloader, valid_dataloader = make_loader(args, data, tokenizer, fold)\n","\n","    # optimizer\n","    optimizer = make_optimizer(args, model)\n","\n","    # scheduler\n","    num_training_steps = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps) * args.epochs\n","    if args.warmup_ratio > 0:\n","        num_warmup_steps = int(args.warmup_ratio * num_training_steps)\n","    else:\n","        num_warmup_steps = 0\n","    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n","    scheduler = make_scheduler(args, optimizer, num_warmup_steps, num_training_steps)\n","\n","    # mixed precision training with NVIDIA Apex\n","    if args.fp16:\n","        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n","    \n","    result_dict = {\n","        'epoch':[], \n","        'train_loss': [], \n","        'val_loss' : [], \n","        'best_val_loss': np.inf\n","    }\n","\n","    return (\n","        model, model_config, tokenizer, optimizer, scheduler, \n","        train_dataloader, valid_dataloader, result_dict\n","    )"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"XUjSnwFpHKAi","executionInfo":{"status":"ok","timestamp":1635352151220,"user_tz":-330,"elapsed":513,"user":{"displayName":"2020 11034","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11154967412412261153"}}},"source":["def run(data, fold):\n","    args = Config()\n","    model, model_config, tokenizer, optimizer, scheduler, train_dataloader, \\\n","        valid_dataloader, result_dict = init_training(args, data, fold)\n","    \n","    trainer = Trainer(model, tokenizer, optimizer, scheduler)\n","    evaluator = Evaluator(model)\n","\n","    train_time_list = []\n","    valid_time_list = []\n","\n","    for epoch in range(args.epochs):\n","        result_dict['epoch'].append(epoch)\n","\n","        # Train\n","        torch.cuda.synchronize()\n","        tic1 = time.time()\n","        result_dict = trainer.train(\n","            args, train_dataloader, \n","            epoch, result_dict\n","        )\n","        torch.cuda.synchronize()\n","        tic2 = time.time() \n","        train_time_list.append(tic2 - tic1)\n","        \n","        # Evaluate\n","        torch.cuda.synchronize()\n","        tic3 = time.time()\n","        result_dict = evaluator.evaluate(\n","            valid_dataloader, epoch, result_dict\n","        )\n","        torch.cuda.synchronize()\n","        tic4 = time.time() \n","        valid_time_list.append(tic4 - tic3)\n","            \n","        output_dir = os.path.join(args.output_dir, f\"checkpoint-fold-{fold}\")\n","        if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n","            print(\"{} Epoch, Best epoch was updated! Valid Loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n","            result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]        \n","            \n","            os.makedirs(output_dir, exist_ok=True)\n","            torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n","            model_config.save_pretrained(output_dir)\n","            tokenizer.save_pretrained(output_dir)\n","            print(f\"Saving model checkpoint to {output_dir}.\")\n","            \n","        print()\n","\n","    evaluator.save(result_dict, output_dir)\n","    \n","    print(f\"Total Training Time: {np.sum(train_time_list)}secs, Average Training Time per Epoch: {np.mean(train_time_list)}secs.\")\n","    print(f\"Total Validation Time: {np.sum(valid_time_list)}secs, Average Validation Time per Epoch: {np.mean(valid_time_list)}secs.\")\n","    \n","    torch.cuda.empty_cache()\n","    del trainer, evaluator\n","    del model, model_config, tokenizer\n","    del optimizer, scheduler\n","    del train_dataloader, valid_dataloader, result_dict\n","    gc.collect()\n"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bduls9AyHO8p","outputId":"64d917bc-40f7-4cfc-a50f-2464edd27569"},"source":["for fold in range(2,3):\n","    print();print()\n","    print('-'*50)\n","    print(f'FOLD: {fold}')\n","    print('-'*50)\n","    run(train, fold)\n","\n","\n","! cp -r /content/output/checkpoint-fold-2 /content/drive/Shareddrives/NLP/Dataset/murli"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","--------------------------------------------------\n","FOLD: 2\n","--------------------------------------------------\n","Model pushed to 1 GPU(s), type Tesla K80.\n","Num examples Train= 20299, Num examples Valid=2770\n","Total Training Steps: 10150, Total Warmup Steps: 1015\n","Epoch: 00 [    1/20299 (  0%)], Train Loss: 2.96676\n","Epoch: 00 [   11/20299 (  0%)], Train Loss: 2.96313\n","Epoch: 00 [   21/20299 (  0%)], Train Loss: 2.96029\n","Epoch: 00 [   31/20299 (  0%)], Train Loss: 2.96146\n","Epoch: 00 [   41/20299 (  0%)], Train Loss: 2.95817\n","Epoch: 00 [   51/20299 (  0%)], Train Loss: 2.95487\n","Epoch: 00 [   61/20299 (  0%)], Train Loss: 2.95182\n","Epoch: 00 [   71/20299 (  0%)], Train Loss: 2.94532\n","Epoch: 00 [   81/20299 (  0%)], Train Loss: 2.93819\n","Epoch: 00 [   91/20299 (  0%)], Train Loss: 2.92924\n","Epoch: 00 [  101/20299 (  0%)], Train Loss: 2.92183\n","Epoch: 00 [  111/20299 (  1%)], Train Loss: 2.91314\n","Epoch: 00 [  121/20299 (  1%)], Train Loss: 2.90378\n","Epoch: 00 [  131/20299 (  1%)], Train Loss: 2.88950\n","Epoch: 00 [  141/20299 (  1%)], Train Loss: 2.87618\n","Epoch: 00 [  151/20299 (  1%)], Train Loss: 2.86020\n","Epoch: 00 [  161/20299 (  1%)], Train Loss: 2.83908\n","Epoch: 00 [  171/20299 (  1%)], Train Loss: 2.81969\n","Epoch: 00 [  181/20299 (  1%)], Train Loss: 2.79676\n","Epoch: 00 [  191/20299 (  1%)], Train Loss: 2.77659\n","Epoch: 00 [  201/20299 (  1%)], Train Loss: 2.75652\n","Epoch: 00 [  211/20299 (  1%)], Train Loss: 2.72631\n","Epoch: 00 [  221/20299 (  1%)], Train Loss: 2.68856\n","Epoch: 00 [  231/20299 (  1%)], Train Loss: 2.65625\n","Epoch: 00 [  241/20299 (  1%)], Train Loss: 2.62541\n","Epoch: 00 [  251/20299 (  1%)], Train Loss: 2.58357\n","Epoch: 00 [  261/20299 (  1%)], Train Loss: 2.54507\n","Epoch: 00 [  271/20299 (  1%)], Train Loss: 2.50052\n","Epoch: 00 [  281/20299 (  1%)], Train Loss: 2.45661\n","Epoch: 00 [  291/20299 (  1%)], Train Loss: 2.42291\n","Epoch: 00 [  301/20299 (  1%)], Train Loss: 2.38571\n","Epoch: 00 [  311/20299 (  2%)], Train Loss: 2.34228\n","Epoch: 00 [  321/20299 (  2%)], Train Loss: 2.28378\n","Epoch: 00 [  331/20299 (  2%)], Train Loss: 2.24382\n","Epoch: 00 [  341/20299 (  2%)], Train Loss: 2.18952\n","Epoch: 00 [  351/20299 (  2%)], Train Loss: 2.15192\n","Epoch: 00 [  361/20299 (  2%)], Train Loss: 2.11113\n","Epoch: 00 [  371/20299 (  2%)], Train Loss: 2.07590\n","Epoch: 00 [  381/20299 (  2%)], Train Loss: 2.03606\n","Epoch: 00 [  391/20299 (  2%)], Train Loss: 1.99103\n","Epoch: 00 [  401/20299 (  2%)], Train Loss: 1.95972\n","Epoch: 00 [  411/20299 (  2%)], Train Loss: 1.92917\n","Epoch: 00 [  421/20299 (  2%)], Train Loss: 1.89229\n","Epoch: 00 [  431/20299 (  2%)], Train Loss: 1.86471\n","Epoch: 00 [  441/20299 (  2%)], Train Loss: 1.84037\n","Epoch: 00 [  451/20299 (  2%)], Train Loss: 1.81211\n","Epoch: 00 [  461/20299 (  2%)], Train Loss: 1.77920\n","Epoch: 00 [  471/20299 (  2%)], Train Loss: 1.75315\n","Epoch: 00 [  481/20299 (  2%)], Train Loss: 1.72179\n","Epoch: 00 [  491/20299 (  2%)], Train Loss: 1.69132\n","Epoch: 00 [  501/20299 (  2%)], Train Loss: 1.67016\n","Epoch: 00 [  511/20299 (  3%)], Train Loss: 1.64608\n","Epoch: 00 [  521/20299 (  3%)], Train Loss: 1.63181\n","Epoch: 00 [  531/20299 (  3%)], Train Loss: 1.61673\n","Epoch: 00 [  541/20299 (  3%)], Train Loss: 1.59563\n","Epoch: 00 [  551/20299 (  3%)], Train Loss: 1.57604\n","Epoch: 00 [  561/20299 (  3%)], Train Loss: 1.55425\n","Epoch: 00 [  571/20299 (  3%)], Train Loss: 1.53289\n","Epoch: 00 [  581/20299 (  3%)], Train Loss: 1.52268\n","Epoch: 00 [  591/20299 (  3%)], Train Loss: 1.50426\n","Epoch: 00 [  601/20299 (  3%)], Train Loss: 1.48268\n","Epoch: 00 [  611/20299 (  3%)], Train Loss: 1.46768\n","Epoch: 00 [  621/20299 (  3%)], Train Loss: 1.45217\n","Epoch: 00 [  631/20299 (  3%)], Train Loss: 1.43967\n","Epoch: 00 [  641/20299 (  3%)], Train Loss: 1.42323\n","Epoch: 00 [  651/20299 (  3%)], Train Loss: 1.40911\n","Epoch: 00 [  661/20299 (  3%)], Train Loss: 1.39063\n","Epoch: 00 [  671/20299 (  3%)], Train Loss: 1.37761\n","Epoch: 00 [  681/20299 (  3%)], Train Loss: 1.36713\n","Epoch: 00 [  691/20299 (  3%)], Train Loss: 1.36151\n","Epoch: 00 [  701/20299 (  3%)], Train Loss: 1.34704\n","Epoch: 00 [  711/20299 (  4%)], Train Loss: 1.33198\n","Epoch: 00 [  721/20299 (  4%)], Train Loss: 1.32728\n","Epoch: 00 [  731/20299 (  4%)], Train Loss: 1.31013\n","Epoch: 00 [  741/20299 (  4%)], Train Loss: 1.29552\n","Epoch: 00 [  751/20299 (  4%)], Train Loss: 1.28234\n","Epoch: 00 [  761/20299 (  4%)], Train Loss: 1.27217\n","Epoch: 00 [  771/20299 (  4%)], Train Loss: 1.25955\n","Epoch: 00 [  781/20299 (  4%)], Train Loss: 1.24606\n","Epoch: 00 [  791/20299 (  4%)], Train Loss: 1.24154\n","Epoch: 00 [  801/20299 (  4%)], Train Loss: 1.23979\n","Epoch: 00 [  811/20299 (  4%)], Train Loss: 1.23067\n","Epoch: 00 [  821/20299 (  4%)], Train Loss: 1.21901\n","Epoch: 00 [  831/20299 (  4%)], Train Loss: 1.20874\n","Epoch: 00 [  841/20299 (  4%)], Train Loss: 1.19696\n","Epoch: 00 [  851/20299 (  4%)], Train Loss: 1.19018\n","Epoch: 00 [  861/20299 (  4%)], Train Loss: 1.18147\n","Epoch: 00 [  871/20299 (  4%)], Train Loss: 1.17364\n","Epoch: 00 [  881/20299 (  4%)], Train Loss: 1.16598\n","Epoch: 00 [  891/20299 (  4%)], Train Loss: 1.15714\n","Epoch: 00 [  901/20299 (  4%)], Train Loss: 1.14987\n","Epoch: 00 [  911/20299 (  4%)], Train Loss: 1.14311\n","Epoch: 00 [  921/20299 (  5%)], Train Loss: 1.13964\n","Epoch: 00 [  931/20299 (  5%)], Train Loss: 1.12986\n","Epoch: 00 [  941/20299 (  5%)], Train Loss: 1.12243\n","Epoch: 00 [  951/20299 (  5%)], Train Loss: 1.12202\n","Epoch: 00 [  961/20299 (  5%)], Train Loss: 1.11188\n","Epoch: 00 [  971/20299 (  5%)], Train Loss: 1.10470\n","Epoch: 00 [  981/20299 (  5%)], Train Loss: 1.09634\n","Epoch: 00 [  991/20299 (  5%)], Train Loss: 1.09187\n","Epoch: 00 [ 1001/20299 (  5%)], Train Loss: 1.08614\n","Epoch: 00 [ 1011/20299 (  5%)], Train Loss: 1.08195\n","Epoch: 00 [ 1021/20299 (  5%)], Train Loss: 1.07411\n","Epoch: 00 [ 1031/20299 (  5%)], Train Loss: 1.06702\n","Epoch: 00 [ 1041/20299 (  5%)], Train Loss: 1.06094\n","Epoch: 00 [ 1051/20299 (  5%)], Train Loss: 1.05414\n","Epoch: 00 [ 1061/20299 (  5%)], Train Loss: 1.05321\n","Epoch: 00 [ 1071/20299 (  5%)], Train Loss: 1.04391\n","Epoch: 00 [ 1081/20299 (  5%)], Train Loss: 1.03877\n","Epoch: 00 [ 1091/20299 (  5%)], Train Loss: 1.03238\n","Epoch: 00 [ 1101/20299 (  5%)], Train Loss: 1.02577\n","Epoch: 00 [ 1111/20299 (  5%)], Train Loss: 1.02716\n","Epoch: 00 [ 1121/20299 (  6%)], Train Loss: 1.01916\n","Epoch: 00 [ 1131/20299 (  6%)], Train Loss: 1.01443\n","Epoch: 00 [ 1141/20299 (  6%)], Train Loss: 1.01384\n","Epoch: 00 [ 1151/20299 (  6%)], Train Loss: 1.01030\n","Epoch: 00 [ 1161/20299 (  6%)], Train Loss: 1.00734\n","Epoch: 00 [ 1171/20299 (  6%)], Train Loss: 1.00075\n","Epoch: 00 [ 1181/20299 (  6%)], Train Loss: 1.00024\n","Epoch: 00 [ 1191/20299 (  6%)], Train Loss: 0.99866\n","Epoch: 00 [ 1201/20299 (  6%)], Train Loss: 0.99812\n","Epoch: 00 [ 1211/20299 (  6%)], Train Loss: 0.99293\n","Epoch: 00 [ 1221/20299 (  6%)], Train Loss: 0.98727\n","Epoch: 00 [ 1231/20299 (  6%)], Train Loss: 0.98419\n","Epoch: 00 [ 1241/20299 (  6%)], Train Loss: 0.97886\n","Epoch: 00 [ 1251/20299 (  6%)], Train Loss: 0.97383\n","Epoch: 00 [ 1261/20299 (  6%)], Train Loss: 0.96974\n","Epoch: 00 [ 1271/20299 (  6%)], Train Loss: 0.96503\n","Epoch: 00 [ 1281/20299 (  6%)], Train Loss: 0.96408\n","Epoch: 00 [ 1291/20299 (  6%)], Train Loss: 0.96456\n","Epoch: 00 [ 1301/20299 (  6%)], Train Loss: 0.96002\n","Epoch: 00 [ 1311/20299 (  6%)], Train Loss: 0.95927\n","Epoch: 00 [ 1321/20299 (  7%)], Train Loss: 0.95521\n","Epoch: 00 [ 1331/20299 (  7%)], Train Loss: 0.95561\n","Epoch: 00 [ 1341/20299 (  7%)], Train Loss: 0.95195\n","Epoch: 00 [ 1351/20299 (  7%)], Train Loss: 0.94658\n","Epoch: 00 [ 1361/20299 (  7%)], Train Loss: 0.94095\n","Epoch: 00 [ 1371/20299 (  7%)], Train Loss: 0.93719\n","Epoch: 00 [ 1381/20299 (  7%)], Train Loss: 0.93308\n","Epoch: 00 [ 1391/20299 (  7%)], Train Loss: 0.92964\n","Epoch: 00 [ 1401/20299 (  7%)], Train Loss: 0.92481\n","Epoch: 00 [ 1411/20299 (  7%)], Train Loss: 0.91941\n","Epoch: 00 [ 1421/20299 (  7%)], Train Loss: 0.91993\n","Epoch: 00 [ 1431/20299 (  7%)], Train Loss: 0.91498\n","Epoch: 00 [ 1441/20299 (  7%)], Train Loss: 0.91028\n","Epoch: 00 [ 1451/20299 (  7%)], Train Loss: 0.90956\n","Epoch: 00 [ 1461/20299 (  7%)], Train Loss: 0.90801\n","Epoch: 00 [ 1471/20299 (  7%)], Train Loss: 0.90486\n","Epoch: 00 [ 1481/20299 (  7%)], Train Loss: 0.89880\n","Epoch: 00 [ 1491/20299 (  7%)], Train Loss: 0.89641\n","Epoch: 00 [ 1501/20299 (  7%)], Train Loss: 0.89179\n","Epoch: 00 [ 1511/20299 (  7%)], Train Loss: 0.88654\n","Epoch: 00 [ 1521/20299 (  7%)], Train Loss: 0.88260\n","Epoch: 00 [ 1531/20299 (  8%)], Train Loss: 0.87883\n","Epoch: 00 [ 1541/20299 (  8%)], Train Loss: 0.87689\n","Epoch: 00 [ 1551/20299 (  8%)], Train Loss: 0.87752\n","Epoch: 00 [ 1561/20299 (  8%)], Train Loss: 0.87348\n","Epoch: 00 [ 1571/20299 (  8%)], Train Loss: 0.87037\n","Epoch: 00 [ 1581/20299 (  8%)], Train Loss: 0.86715\n","Epoch: 00 [ 1591/20299 (  8%)], Train Loss: 0.86319\n","Epoch: 00 [ 1601/20299 (  8%)], Train Loss: 0.85874\n","Epoch: 00 [ 1611/20299 (  8%)], Train Loss: 0.85477\n","Epoch: 00 [ 1621/20299 (  8%)], Train Loss: 0.85321\n","Epoch: 00 [ 1631/20299 (  8%)], Train Loss: 0.85149\n","Epoch: 00 [ 1641/20299 (  8%)], Train Loss: 0.84863\n","Epoch: 00 [ 1651/20299 (  8%)], Train Loss: 0.84707\n","Epoch: 00 [ 1661/20299 (  8%)], Train Loss: 0.84291\n","Epoch: 00 [ 1671/20299 (  8%)], Train Loss: 0.83912\n","Epoch: 00 [ 1681/20299 (  8%)], Train Loss: 0.84069\n","Epoch: 00 [ 1691/20299 (  8%)], Train Loss: 0.83649\n","Epoch: 00 [ 1701/20299 (  8%)], Train Loss: 0.83745\n","Epoch: 00 [ 1711/20299 (  8%)], Train Loss: 0.83514\n","Epoch: 00 [ 1721/20299 (  8%)], Train Loss: 0.83360\n","Epoch: 00 [ 1731/20299 (  9%)], Train Loss: 0.83258\n","Epoch: 00 [ 1741/20299 (  9%)], Train Loss: 0.83018\n","Epoch: 00 [ 1751/20299 (  9%)], Train Loss: 0.82663\n","Epoch: 00 [ 1761/20299 (  9%)], Train Loss: 0.82458\n","Epoch: 00 [ 1771/20299 (  9%)], Train Loss: 0.82280\n","Epoch: 00 [ 1781/20299 (  9%)], Train Loss: 0.82076\n","Epoch: 00 [ 1791/20299 (  9%)], Train Loss: 0.82046\n","Epoch: 00 [ 1801/20299 (  9%)], Train Loss: 0.81807\n","Epoch: 00 [ 1811/20299 (  9%)], Train Loss: 0.81608\n","Epoch: 00 [ 1821/20299 (  9%)], Train Loss: 0.81400\n","Epoch: 00 [ 1831/20299 (  9%)], Train Loss: 0.81539\n","Epoch: 00 [ 1841/20299 (  9%)], Train Loss: 0.81372\n","Epoch: 00 [ 1851/20299 (  9%)], Train Loss: 0.81226\n","Epoch: 00 [ 1861/20299 (  9%)], Train Loss: 0.81015\n","Epoch: 00 [ 1871/20299 (  9%)], Train Loss: 0.80771\n","Epoch: 00 [ 1881/20299 (  9%)], Train Loss: 0.80661\n","Epoch: 00 [ 1891/20299 (  9%)], Train Loss: 0.80600\n","Epoch: 00 [ 1901/20299 (  9%)], Train Loss: 0.80362\n","Epoch: 00 [ 1911/20299 (  9%)], Train Loss: 0.80003\n","Epoch: 00 [ 1921/20299 (  9%)], Train Loss: 0.79709\n","Epoch: 00 [ 1931/20299 ( 10%)], Train Loss: 0.79434\n","Epoch: 00 [ 1941/20299 ( 10%)], Train Loss: 0.79037\n","Epoch: 00 [ 1951/20299 ( 10%)], Train Loss: 0.78778\n","Epoch: 00 [ 1961/20299 ( 10%)], Train Loss: 0.78468\n","Epoch: 00 [ 1971/20299 ( 10%)], Train Loss: 0.78324\n","Epoch: 00 [ 1981/20299 ( 10%)], Train Loss: 0.78114\n","Epoch: 00 [ 1991/20299 ( 10%)], Train Loss: 0.77758\n","Epoch: 00 [ 2001/20299 ( 10%)], Train Loss: 0.77724\n","Epoch: 00 [ 2011/20299 ( 10%)], Train Loss: 0.77661\n","Epoch: 00 [ 2021/20299 ( 10%)], Train Loss: 0.77370\n","Epoch: 00 [ 2031/20299 ( 10%)], Train Loss: 0.77224\n","Epoch: 00 [ 2041/20299 ( 10%)], Train Loss: 0.77176\n","Epoch: 00 [ 2051/20299 ( 10%)], Train Loss: 0.77163\n","Epoch: 00 [ 2061/20299 ( 10%)], Train Loss: 0.76999\n","Epoch: 00 [ 2071/20299 ( 10%)], Train Loss: 0.76917\n","Epoch: 00 [ 2081/20299 ( 10%)], Train Loss: 0.77074\n","Epoch: 00 [ 2091/20299 ( 10%)], Train Loss: 0.76975\n","Epoch: 00 [ 2101/20299 ( 10%)], Train Loss: 0.76684\n","Epoch: 00 [ 2111/20299 ( 10%)], Train Loss: 0.76615\n","Epoch: 00 [ 2121/20299 ( 10%)], Train Loss: 0.76585\n","Epoch: 00 [ 2131/20299 ( 10%)], Train Loss: 0.76534\n","Epoch: 00 [ 2141/20299 ( 11%)], Train Loss: 0.76489\n","Epoch: 00 [ 2151/20299 ( 11%)], Train Loss: 0.76555\n","Epoch: 00 [ 2161/20299 ( 11%)], Train Loss: 0.76326\n","Epoch: 00 [ 2171/20299 ( 11%)], Train Loss: 0.76175\n","Epoch: 00 [ 2181/20299 ( 11%)], Train Loss: 0.75890\n","Epoch: 00 [ 2191/20299 ( 11%)], Train Loss: 0.75562\n","Epoch: 00 [ 2201/20299 ( 11%)], Train Loss: 0.75332\n","Epoch: 00 [ 2211/20299 ( 11%)], Train Loss: 0.75058\n","Epoch: 00 [ 2221/20299 ( 11%)], Train Loss: 0.75023\n","Epoch: 00 [ 2231/20299 ( 11%)], Train Loss: 0.74851\n","Epoch: 00 [ 2241/20299 ( 11%)], Train Loss: 0.74920\n","Epoch: 00 [ 2251/20299 ( 11%)], Train Loss: 0.74836\n","Epoch: 00 [ 2261/20299 ( 11%)], Train Loss: 0.74763\n","Epoch: 00 [ 2271/20299 ( 11%)], Train Loss: 0.74713\n","Epoch: 00 [ 2281/20299 ( 11%)], Train Loss: 0.74560\n","Epoch: 00 [ 2291/20299 ( 11%)], Train Loss: 0.74340\n","Epoch: 00 [ 2301/20299 ( 11%)], Train Loss: 0.74186\n","Epoch: 00 [ 2311/20299 ( 11%)], Train Loss: 0.73997\n","Epoch: 00 [ 2321/20299 ( 11%)], Train Loss: 0.73837\n","Epoch: 00 [ 2331/20299 ( 11%)], Train Loss: 0.73747\n","Epoch: 00 [ 2341/20299 ( 12%)], Train Loss: 0.73558\n","Epoch: 00 [ 2351/20299 ( 12%)], Train Loss: 0.73548\n","Epoch: 00 [ 2361/20299 ( 12%)], Train Loss: 0.73290\n","Epoch: 00 [ 2371/20299 ( 12%)], Train Loss: 0.73186\n","Epoch: 00 [ 2381/20299 ( 12%)], Train Loss: 0.72971\n","Epoch: 00 [ 2391/20299 ( 12%)], Train Loss: 0.72809\n","Epoch: 00 [ 2401/20299 ( 12%)], Train Loss: 0.72732\n","Epoch: 00 [ 2411/20299 ( 12%)], Train Loss: 0.72591\n","Epoch: 00 [ 2421/20299 ( 12%)], Train Loss: 0.72564\n","Epoch: 00 [ 2431/20299 ( 12%)], Train Loss: 0.72339\n","Epoch: 00 [ 2441/20299 ( 12%)], Train Loss: 0.72126\n","Epoch: 00 [ 2451/20299 ( 12%)], Train Loss: 0.71997\n","Epoch: 00 [ 2461/20299 ( 12%)], Train Loss: 0.71879\n","Epoch: 00 [ 2471/20299 ( 12%)], Train Loss: 0.71867\n","Epoch: 00 [ 2481/20299 ( 12%)], Train Loss: 0.71703\n","Epoch: 00 [ 2491/20299 ( 12%)], Train Loss: 0.71508\n","Epoch: 00 [ 2501/20299 ( 12%)], Train Loss: 0.71385\n","Epoch: 00 [ 2511/20299 ( 12%)], Train Loss: 0.71259\n","Epoch: 00 [ 2521/20299 ( 12%)], Train Loss: 0.71070\n","Epoch: 00 [ 2531/20299 ( 12%)], Train Loss: 0.71153\n","Epoch: 00 [ 2541/20299 ( 13%)], Train Loss: 0.71311\n","Epoch: 00 [ 2551/20299 ( 13%)], Train Loss: 0.71159\n","Epoch: 00 [ 2561/20299 ( 13%)], Train Loss: 0.70980\n","Epoch: 00 [ 2571/20299 ( 13%)], Train Loss: 0.70923\n","Epoch: 00 [ 2581/20299 ( 13%)], Train Loss: 0.70806\n","Epoch: 00 [ 2591/20299 ( 13%)], Train Loss: 0.70788\n","Epoch: 00 [ 2601/20299 ( 13%)], Train Loss: 0.70798\n","Epoch: 00 [ 2611/20299 ( 13%)], Train Loss: 0.70722\n","Epoch: 00 [ 2621/20299 ( 13%)], Train Loss: 0.70681\n","Epoch: 00 [ 2631/20299 ( 13%)], Train Loss: 0.70691\n","Epoch: 00 [ 2641/20299 ( 13%)], Train Loss: 0.70859\n","Epoch: 00 [ 2651/20299 ( 13%)], Train Loss: 0.70775\n","Epoch: 00 [ 2661/20299 ( 13%)], Train Loss: 0.70585\n","Epoch: 00 [ 2671/20299 ( 13%)], Train Loss: 0.70593\n","Epoch: 00 [ 2681/20299 ( 13%)], Train Loss: 0.70527\n","Epoch: 00 [ 2691/20299 ( 13%)], Train Loss: 0.70359\n","Epoch: 00 [ 2701/20299 ( 13%)], Train Loss: 0.70418\n","Epoch: 00 [ 2711/20299 ( 13%)], Train Loss: 0.70256\n","Epoch: 00 [ 2721/20299 ( 13%)], Train Loss: 0.70085\n","Epoch: 00 [ 2731/20299 ( 13%)], Train Loss: 0.70148\n","Epoch: 00 [ 2741/20299 ( 14%)], Train Loss: 0.70054\n","Epoch: 00 [ 2751/20299 ( 14%)], Train Loss: 0.69999\n","Epoch: 00 [ 2761/20299 ( 14%)], Train Loss: 0.70005\n","Epoch: 00 [ 2771/20299 ( 14%)], Train Loss: 0.69957\n","Epoch: 00 [ 2781/20299 ( 14%)], Train Loss: 0.69777\n","Epoch: 00 [ 2791/20299 ( 14%)], Train Loss: 0.69774\n","Epoch: 00 [ 2801/20299 ( 14%)], Train Loss: 0.69594\n","Epoch: 00 [ 2811/20299 ( 14%)], Train Loss: 0.69762\n","Epoch: 00 [ 2821/20299 ( 14%)], Train Loss: 0.69844\n","Epoch: 00 [ 2831/20299 ( 14%)], Train Loss: 0.69675\n","Epoch: 00 [ 2841/20299 ( 14%)], Train Loss: 0.69611\n","Epoch: 00 [ 2851/20299 ( 14%)], Train Loss: 0.69520\n","Epoch: 00 [ 2861/20299 ( 14%)], Train Loss: 0.69300\n","Epoch: 00 [ 2871/20299 ( 14%)], Train Loss: 0.69288\n","Epoch: 00 [ 2881/20299 ( 14%)], Train Loss: 0.69233\n","Epoch: 00 [ 2891/20299 ( 14%)], Train Loss: 0.69221\n","Epoch: 00 [ 2901/20299 ( 14%)], Train Loss: 0.69062\n","Epoch: 00 [ 2911/20299 ( 14%)], Train Loss: 0.69101\n","Epoch: 00 [ 2921/20299 ( 14%)], Train Loss: 0.69114\n","Epoch: 00 [ 2931/20299 ( 14%)], Train Loss: 0.69001\n","Epoch: 00 [ 2941/20299 ( 14%)], Train Loss: 0.68892\n","Epoch: 00 [ 2951/20299 ( 15%)], Train Loss: 0.69038\n","Epoch: 00 [ 2961/20299 ( 15%)], Train Loss: 0.68944\n","Epoch: 00 [ 2971/20299 ( 15%)], Train Loss: 0.68798\n","Epoch: 00 [ 2981/20299 ( 15%)], Train Loss: 0.68631\n","Epoch: 00 [ 2991/20299 ( 15%)], Train Loss: 0.68713\n","Epoch: 00 [ 3001/20299 ( 15%)], Train Loss: 0.68866\n","Epoch: 00 [ 3011/20299 ( 15%)], Train Loss: 0.68922\n","Epoch: 00 [ 3021/20299 ( 15%)], Train Loss: 0.69118\n","Epoch: 00 [ 3031/20299 ( 15%)], Train Loss: 0.69210\n","Epoch: 00 [ 3041/20299 ( 15%)], Train Loss: 0.69224\n","Epoch: 00 [ 3051/20299 ( 15%)], Train Loss: 0.69126\n","Epoch: 00 [ 3061/20299 ( 15%)], Train Loss: 0.69381\n","Epoch: 00 [ 3071/20299 ( 15%)], Train Loss: 0.69535\n","Epoch: 00 [ 3081/20299 ( 15%)], Train Loss: 0.69504\n","Epoch: 00 [ 3091/20299 ( 15%)], Train Loss: 0.69362\n","Epoch: 00 [ 3101/20299 ( 15%)], Train Loss: 0.69205\n","Epoch: 00 [ 3111/20299 ( 15%)], Train Loss: 0.69036\n","Epoch: 00 [ 3121/20299 ( 15%)], Train Loss: 0.68884\n","Epoch: 00 [ 3131/20299 ( 15%)], Train Loss: 0.69031\n","Epoch: 00 [ 3141/20299 ( 15%)], Train Loss: 0.69060\n","Epoch: 00 [ 3151/20299 ( 16%)], Train Loss: 0.68903\n","Epoch: 00 [ 3161/20299 ( 16%)], Train Loss: 0.68921\n","Epoch: 00 [ 3171/20299 ( 16%)], Train Loss: 0.68857\n","Epoch: 00 [ 3181/20299 ( 16%)], Train Loss: 0.68829\n","Epoch: 00 [ 3191/20299 ( 16%)], Train Loss: 0.69113\n","Epoch: 00 [ 3201/20299 ( 16%)], Train Loss: 0.69106\n","Epoch: 00 [ 3211/20299 ( 16%)], Train Loss: 0.69240\n","Epoch: 00 [ 3221/20299 ( 16%)], Train Loss: 0.69126\n","Epoch: 00 [ 3231/20299 ( 16%)], Train Loss: 0.69040\n","Epoch: 00 [ 3241/20299 ( 16%)], Train Loss: 0.69023\n","Epoch: 00 [ 3251/20299 ( 16%)], Train Loss: 0.68926\n","Epoch: 00 [ 3261/20299 ( 16%)], Train Loss: 0.68945\n","Epoch: 00 [ 3271/20299 ( 16%)], Train Loss: 0.68926\n","Epoch: 00 [ 3281/20299 ( 16%)], Train Loss: 0.68756\n","Epoch: 00 [ 3291/20299 ( 16%)], Train Loss: 0.68663\n","Epoch: 00 [ 3301/20299 ( 16%)], Train Loss: 0.68486\n","Epoch: 00 [ 3311/20299 ( 16%)], Train Loss: 0.68416\n","Epoch: 00 [ 3321/20299 ( 16%)], Train Loss: 0.68393\n","Epoch: 00 [ 3331/20299 ( 16%)], Train Loss: 0.68221\n","Epoch: 00 [ 3341/20299 ( 16%)], Train Loss: 0.68140\n","Epoch: 00 [ 3351/20299 ( 17%)], Train Loss: 0.68193\n","Epoch: 00 [ 3361/20299 ( 17%)], Train Loss: 0.68102\n","Epoch: 00 [ 3371/20299 ( 17%)], Train Loss: 0.67909\n","Epoch: 00 [ 3381/20299 ( 17%)], Train Loss: 0.67972\n","Epoch: 00 [ 3391/20299 ( 17%)], Train Loss: 0.68152\n","Epoch: 00 [ 3401/20299 ( 17%)], Train Loss: 0.68138\n","Epoch: 00 [ 3411/20299 ( 17%)], Train Loss: 0.68020\n","Epoch: 00 [ 3421/20299 ( 17%)], Train Loss: 0.68116\n","Epoch: 00 [ 3431/20299 ( 17%)], Train Loss: 0.67952\n","Epoch: 00 [ 3441/20299 ( 17%)], Train Loss: 0.67845\n","Epoch: 00 [ 3451/20299 ( 17%)], Train Loss: 0.67783\n","Epoch: 00 [ 3461/20299 ( 17%)], Train Loss: 0.67592\n","Epoch: 00 [ 3471/20299 ( 17%)], Train Loss: 0.67613\n","Epoch: 00 [ 3481/20299 ( 17%)], Train Loss: 0.67464\n","Epoch: 00 [ 3491/20299 ( 17%)], Train Loss: 0.67326\n","Epoch: 00 [ 3501/20299 ( 17%)], Train Loss: 0.67215\n","Epoch: 00 [ 3511/20299 ( 17%)], Train Loss: 0.67217\n","Epoch: 00 [ 3521/20299 ( 17%)], Train Loss: 0.67286\n","Epoch: 00 [ 3531/20299 ( 17%)], Train Loss: 0.67296\n","Epoch: 00 [ 3541/20299 ( 17%)], Train Loss: 0.67258\n","Epoch: 00 [ 3551/20299 ( 17%)], Train Loss: 0.67104\n","Epoch: 00 [ 3561/20299 ( 18%)], Train Loss: 0.66948\n","Epoch: 00 [ 3571/20299 ( 18%)], Train Loss: 0.67056\n","Epoch: 00 [ 3581/20299 ( 18%)], Train Loss: 0.66963\n","Epoch: 00 [ 3591/20299 ( 18%)], Train Loss: 0.66962\n","Epoch: 00 [ 3601/20299 ( 18%)], Train Loss: 0.66975\n","Epoch: 00 [ 3611/20299 ( 18%)], Train Loss: 0.66969\n","Epoch: 00 [ 3621/20299 ( 18%)], Train Loss: 0.66873\n","Epoch: 00 [ 3631/20299 ( 18%)], Train Loss: 0.66869\n","Epoch: 00 [ 3641/20299 ( 18%)], Train Loss: 0.66694\n","Epoch: 00 [ 3651/20299 ( 18%)], Train Loss: 0.66650\n","Epoch: 00 [ 3661/20299 ( 18%)], Train Loss: 0.66539\n","Epoch: 00 [ 3671/20299 ( 18%)], Train Loss: 0.66437\n","Epoch: 00 [ 3681/20299 ( 18%)], Train Loss: 0.66443\n","Epoch: 00 [ 3691/20299 ( 18%)], Train Loss: 0.66405\n","Epoch: 00 [ 3701/20299 ( 18%)], Train Loss: 0.66337\n","Epoch: 00 [ 3711/20299 ( 18%)], Train Loss: 0.66294\n","Epoch: 00 [ 3721/20299 ( 18%)], Train Loss: 0.66273\n","Epoch: 00 [ 3731/20299 ( 18%)], Train Loss: 0.66115\n","Epoch: 00 [ 3741/20299 ( 18%)], Train Loss: 0.66056\n","Epoch: 00 [ 3751/20299 ( 18%)], Train Loss: 0.65927\n","Epoch: 00 [ 3761/20299 ( 19%)], Train Loss: 0.65850\n","Epoch: 00 [ 3771/20299 ( 19%)], Train Loss: 0.65710\n","Epoch: 00 [ 3781/20299 ( 19%)], Train Loss: 0.65747\n","Epoch: 00 [ 3791/20299 ( 19%)], Train Loss: 0.65620\n","Epoch: 00 [ 3801/20299 ( 19%)], Train Loss: 0.65676\n","Epoch: 00 [ 3811/20299 ( 19%)], Train Loss: 0.65737\n","Epoch: 00 [ 3821/20299 ( 19%)], Train Loss: 0.65931\n","Epoch: 00 [ 3831/20299 ( 19%)], Train Loss: 0.65853\n","Epoch: 00 [ 3841/20299 ( 19%)], Train Loss: 0.65807\n","Epoch: 00 [ 3851/20299 ( 19%)], Train Loss: 0.65696\n","Epoch: 00 [ 3861/20299 ( 19%)], Train Loss: 0.65656\n","Epoch: 00 [ 3871/20299 ( 19%)], Train Loss: 0.65652\n","Epoch: 00 [ 3881/20299 ( 19%)], Train Loss: 0.65688\n","Epoch: 00 [ 3891/20299 ( 19%)], Train Loss: 0.65565\n","Epoch: 00 [ 3901/20299 ( 19%)], Train Loss: 0.65515\n","Epoch: 00 [ 3911/20299 ( 19%)], Train Loss: 0.65496\n","Epoch: 00 [ 3921/20299 ( 19%)], Train Loss: 0.65374\n","Epoch: 00 [ 3931/20299 ( 19%)], Train Loss: 0.65300\n","Epoch: 00 [ 3941/20299 ( 19%)], Train Loss: 0.65167\n","Epoch: 00 [ 3951/20299 ( 19%)], Train Loss: 0.65077\n","Epoch: 00 [ 3961/20299 ( 20%)], Train Loss: 0.65135\n","Epoch: 00 [ 3971/20299 ( 20%)], Train Loss: 0.65092\n","Epoch: 00 [ 3981/20299 ( 20%)], Train Loss: 0.65013\n","Epoch: 00 [ 3991/20299 ( 20%)], Train Loss: 0.64903\n","Epoch: 00 [ 4001/20299 ( 20%)], Train Loss: 0.64876\n","Epoch: 00 [ 4011/20299 ( 20%)], Train Loss: 0.64791\n","Epoch: 00 [ 4021/20299 ( 20%)], Train Loss: 0.64854\n","Epoch: 00 [ 4031/20299 ( 20%)], Train Loss: 0.64787\n","Epoch: 00 [ 4041/20299 ( 20%)], Train Loss: 0.64649\n","Epoch: 00 [ 4051/20299 ( 20%)], Train Loss: 0.64545\n","Epoch: 00 [ 4061/20299 ( 20%)], Train Loss: 0.64405\n","Epoch: 00 [ 4071/20299 ( 20%)], Train Loss: 0.64467\n","Epoch: 00 [ 4081/20299 ( 20%)], Train Loss: 0.64451\n","Epoch: 00 [ 4091/20299 ( 20%)], Train Loss: 0.64390\n","Epoch: 00 [ 4101/20299 ( 20%)], Train Loss: 0.64332\n","Epoch: 00 [ 4111/20299 ( 20%)], Train Loss: 0.64194\n","Epoch: 00 [ 4121/20299 ( 20%)], Train Loss: 0.64193\n","Epoch: 00 [ 4131/20299 ( 20%)], Train Loss: 0.64073\n","Epoch: 00 [ 4141/20299 ( 20%)], Train Loss: 0.64101\n","Epoch: 00 [ 4151/20299 ( 20%)], Train Loss: 0.64143\n","Epoch: 00 [ 4161/20299 ( 20%)], Train Loss: 0.64069\n","Epoch: 00 [ 4171/20299 ( 21%)], Train Loss: 0.64018\n","Epoch: 00 [ 4181/20299 ( 21%)], Train Loss: 0.63998\n","Epoch: 00 [ 4191/20299 ( 21%)], Train Loss: 0.63873\n","Epoch: 00 [ 4201/20299 ( 21%)], Train Loss: 0.63837\n","Epoch: 00 [ 4211/20299 ( 21%)], Train Loss: 0.63855\n","Epoch: 00 [ 4221/20299 ( 21%)], Train Loss: 0.63788\n","Epoch: 00 [ 4231/20299 ( 21%)], Train Loss: 0.63658\n","Epoch: 00 [ 4241/20299 ( 21%)], Train Loss: 0.63725\n","Epoch: 00 [ 4251/20299 ( 21%)], Train Loss: 0.63626\n","Epoch: 00 [ 4261/20299 ( 21%)], Train Loss: 0.63650\n","Epoch: 00 [ 4271/20299 ( 21%)], Train Loss: 0.63616\n","Epoch: 00 [ 4281/20299 ( 21%)], Train Loss: 0.63510\n","Epoch: 00 [ 4291/20299 ( 21%)], Train Loss: 0.63433\n","Epoch: 00 [ 4301/20299 ( 21%)], Train Loss: 0.63403\n","Epoch: 00 [ 4311/20299 ( 21%)], Train Loss: 0.63268\n","Epoch: 00 [ 4321/20299 ( 21%)], Train Loss: 0.63195\n","Epoch: 00 [ 4331/20299 ( 21%)], Train Loss: 0.63143\n","Epoch: 00 [ 4341/20299 ( 21%)], Train Loss: 0.63135\n","Epoch: 00 [ 4351/20299 ( 21%)], Train Loss: 0.63048\n","Epoch: 00 [ 4361/20299 ( 21%)], Train Loss: 0.62957\n","Epoch: 00 [ 4371/20299 ( 22%)], Train Loss: 0.62875\n","Epoch: 00 [ 4381/20299 ( 22%)], Train Loss: 0.62901\n","Epoch: 00 [ 4391/20299 ( 22%)], Train Loss: 0.62846\n","Epoch: 00 [ 4401/20299 ( 22%)], Train Loss: 0.62824\n","Epoch: 00 [ 4411/20299 ( 22%)], Train Loss: 0.62693\n","Epoch: 00 [ 4421/20299 ( 22%)], Train Loss: 0.62641\n","Epoch: 00 [ 4431/20299 ( 22%)], Train Loss: 0.62704\n","Epoch: 00 [ 4441/20299 ( 22%)], Train Loss: 0.62564\n","Epoch: 00 [ 4451/20299 ( 22%)], Train Loss: 0.62604\n","Epoch: 00 [ 4461/20299 ( 22%)], Train Loss: 0.62607\n","Epoch: 00 [ 4471/20299 ( 22%)], Train Loss: 0.62591\n","Epoch: 00 [ 4481/20299 ( 22%)], Train Loss: 0.62489\n","Epoch: 00 [ 4491/20299 ( 22%)], Train Loss: 0.62541\n","Epoch: 00 [ 4501/20299 ( 22%)], Train Loss: 0.62472\n","Epoch: 00 [ 4511/20299 ( 22%)], Train Loss: 0.62427\n","Epoch: 00 [ 4521/20299 ( 22%)], Train Loss: 0.62314\n","Epoch: 00 [ 4531/20299 ( 22%)], Train Loss: 0.62348\n","Epoch: 00 [ 4541/20299 ( 22%)], Train Loss: 0.62283\n","Epoch: 00 [ 4551/20299 ( 22%)], Train Loss: 0.62323\n","Epoch: 00 [ 4561/20299 ( 22%)], Train Loss: 0.62376\n","Epoch: 00 [ 4571/20299 ( 23%)], Train Loss: 0.62367\n","Epoch: 00 [ 4581/20299 ( 23%)], Train Loss: 0.62251\n","Epoch: 00 [ 4591/20299 ( 23%)], Train Loss: 0.62165\n","Epoch: 00 [ 4601/20299 ( 23%)], Train Loss: 0.62135\n","Epoch: 00 [ 4611/20299 ( 23%)], Train Loss: 0.62037\n","Epoch: 00 [ 4621/20299 ( 23%)], Train Loss: 0.61960\n","Epoch: 00 [ 4631/20299 ( 23%)], Train Loss: 0.61957\n","Epoch: 00 [ 4641/20299 ( 23%)], Train Loss: 0.62066\n","Epoch: 00 [ 4651/20299 ( 23%)], Train Loss: 0.61993\n","Epoch: 00 [ 4661/20299 ( 23%)], Train Loss: 0.62024\n","Epoch: 00 [ 4671/20299 ( 23%)], Train Loss: 0.61971\n","Epoch: 00 [ 4681/20299 ( 23%)], Train Loss: 0.61923\n","Epoch: 00 [ 4691/20299 ( 23%)], Train Loss: 0.61976\n","Epoch: 00 [ 4701/20299 ( 23%)], Train Loss: 0.61946\n","Epoch: 00 [ 4711/20299 ( 23%)], Train Loss: 0.61968\n","Epoch: 00 [ 4721/20299 ( 23%)], Train Loss: 0.62000\n","Epoch: 00 [ 4731/20299 ( 23%)], Train Loss: 0.62107\n","Epoch: 00 [ 4741/20299 ( 23%)], Train Loss: 0.62124\n","Epoch: 00 [ 4751/20299 ( 23%)], Train Loss: 0.62053\n","Epoch: 00 [ 4761/20299 ( 23%)], Train Loss: 0.62072\n","Epoch: 00 [ 4771/20299 ( 24%)], Train Loss: 0.62018\n","Epoch: 00 [ 4781/20299 ( 24%)], Train Loss: 0.61998\n","Epoch: 00 [ 4791/20299 ( 24%)], Train Loss: 0.61948\n","Epoch: 00 [ 4801/20299 ( 24%)], Train Loss: 0.61937\n","Epoch: 00 [ 4811/20299 ( 24%)], Train Loss: 0.61883\n","Epoch: 00 [ 4821/20299 ( 24%)], Train Loss: 0.61974\n","Epoch: 00 [ 4831/20299 ( 24%)], Train Loss: 0.61962\n","Epoch: 00 [ 4841/20299 ( 24%)], Train Loss: 0.61941\n","Epoch: 00 [ 4851/20299 ( 24%)], Train Loss: 0.61862\n","Epoch: 00 [ 4861/20299 ( 24%)], Train Loss: 0.61828\n","Epoch: 00 [ 4871/20299 ( 24%)], Train Loss: 0.61812\n","Epoch: 00 [ 4881/20299 ( 24%)], Train Loss: 0.61940\n","Epoch: 00 [ 4891/20299 ( 24%)], Train Loss: 0.61877\n","Epoch: 00 [ 4901/20299 ( 24%)], Train Loss: 0.61818\n","Epoch: 00 [ 4911/20299 ( 24%)], Train Loss: 0.61826\n","Epoch: 00 [ 4921/20299 ( 24%)], Train Loss: 0.61874\n","Epoch: 00 [ 4931/20299 ( 24%)], Train Loss: 0.61819\n","Epoch: 00 [ 4941/20299 ( 24%)], Train Loss: 0.61898\n","Epoch: 00 [ 4951/20299 ( 24%)], Train Loss: 0.61928\n","Epoch: 00 [ 4961/20299 ( 24%)], Train Loss: 0.61939\n","Epoch: 00 [ 4971/20299 ( 24%)], Train Loss: 0.61991\n","Epoch: 00 [ 4981/20299 ( 25%)], Train Loss: 0.61923\n","Epoch: 00 [ 4991/20299 ( 25%)], Train Loss: 0.61907\n","Epoch: 00 [ 5001/20299 ( 25%)], Train Loss: 0.61842\n","Epoch: 00 [ 5011/20299 ( 25%)], Train Loss: 0.61757\n","Epoch: 00 [ 5021/20299 ( 25%)], Train Loss: 0.61829\n","Epoch: 00 [ 5031/20299 ( 25%)], Train Loss: 0.61900\n","Epoch: 00 [ 5041/20299 ( 25%)], Train Loss: 0.61879\n","Epoch: 00 [ 5051/20299 ( 25%)], Train Loss: 0.61802\n","Epoch: 00 [ 5061/20299 ( 25%)], Train Loss: 0.61866\n","Epoch: 00 [ 5071/20299 ( 25%)], Train Loss: 0.61859\n","Epoch: 00 [ 5081/20299 ( 25%)], Train Loss: 0.61790\n","Epoch: 00 [ 5091/20299 ( 25%)], Train Loss: 0.61743\n","Epoch: 00 [ 5101/20299 ( 25%)], Train Loss: 0.61738\n","Epoch: 00 [ 5111/20299 ( 25%)], Train Loss: 0.61713\n","Epoch: 00 [ 5121/20299 ( 25%)], Train Loss: 0.61670\n","Epoch: 00 [ 5131/20299 ( 25%)], Train Loss: 0.61677\n","Epoch: 00 [ 5141/20299 ( 25%)], Train Loss: 0.61749\n","Epoch: 00 [ 5151/20299 ( 25%)], Train Loss: 0.61680\n","Epoch: 00 [ 5161/20299 ( 25%)], Train Loss: 0.61666\n","Epoch: 00 [ 5171/20299 ( 25%)], Train Loss: 0.61601\n","Epoch: 00 [ 5181/20299 ( 26%)], Train Loss: 0.61562\n","Epoch: 00 [ 5191/20299 ( 26%)], Train Loss: 0.61503\n","Epoch: 00 [ 5201/20299 ( 26%)], Train Loss: 0.61503\n","Epoch: 00 [ 5211/20299 ( 26%)], Train Loss: 0.61462\n","Epoch: 00 [ 5221/20299 ( 26%)], Train Loss: 0.61364\n","Epoch: 00 [ 5231/20299 ( 26%)], Train Loss: 0.61315\n","Epoch: 00 [ 5241/20299 ( 26%)], Train Loss: 0.61237\n","Epoch: 00 [ 5251/20299 ( 26%)], Train Loss: 0.61162\n","Epoch: 00 [ 5261/20299 ( 26%)], Train Loss: 0.61090\n","Epoch: 00 [ 5271/20299 ( 26%)], Train Loss: 0.61102\n","Epoch: 00 [ 5281/20299 ( 26%)], Train Loss: 0.61063\n","Epoch: 00 [ 5291/20299 ( 26%)], Train Loss: 0.61013\n","Epoch: 00 [ 5301/20299 ( 26%)], Train Loss: 0.60997\n","Epoch: 00 [ 5311/20299 ( 26%)], Train Loss: 0.60965\n","Epoch: 00 [ 5321/20299 ( 26%)], Train Loss: 0.61016\n","Epoch: 00 [ 5331/20299 ( 26%)], Train Loss: 0.60948\n","Epoch: 00 [ 5341/20299 ( 26%)], Train Loss: 0.61022\n","Epoch: 00 [ 5351/20299 ( 26%)], Train Loss: 0.61068\n","Epoch: 00 [ 5361/20299 ( 26%)], Train Loss: 0.61078\n","Epoch: 00 [ 5371/20299 ( 26%)], Train Loss: 0.61056\n","Epoch: 00 [ 5381/20299 ( 27%)], Train Loss: 0.61054\n","Epoch: 00 [ 5391/20299 ( 27%)], Train Loss: 0.61024\n","Epoch: 00 [ 5401/20299 ( 27%)], Train Loss: 0.60983\n","Epoch: 00 [ 5411/20299 ( 27%)], Train Loss: 0.61001\n","Epoch: 00 [ 5421/20299 ( 27%)], Train Loss: 0.61054\n","Epoch: 00 [ 5431/20299 ( 27%)], Train Loss: 0.61041\n","Epoch: 00 [ 5441/20299 ( 27%)], Train Loss: 0.60997\n","Epoch: 00 [ 5451/20299 ( 27%)], Train Loss: 0.60993\n","Epoch: 00 [ 5461/20299 ( 27%)], Train Loss: 0.60999\n","Epoch: 00 [ 5471/20299 ( 27%)], Train Loss: 0.61012\n","Epoch: 00 [ 5481/20299 ( 27%)], Train Loss: 0.60958\n","Epoch: 00 [ 5491/20299 ( 27%)], Train Loss: 0.61066\n","Epoch: 00 [ 5501/20299 ( 27%)], Train Loss: 0.61035\n","Epoch: 00 [ 5511/20299 ( 27%)], Train Loss: 0.60985\n","Epoch: 00 [ 5521/20299 ( 27%)], Train Loss: 0.61047\n","Epoch: 00 [ 5531/20299 ( 27%)], Train Loss: 0.61067\n","Epoch: 00 [ 5541/20299 ( 27%)], Train Loss: 0.61038\n","Epoch: 00 [ 5551/20299 ( 27%)], Train Loss: 0.60998\n","Epoch: 00 [ 5561/20299 ( 27%)], Train Loss: 0.60999\n","Epoch: 00 [ 5571/20299 ( 27%)], Train Loss: 0.60950\n","Epoch: 00 [ 5581/20299 ( 27%)], Train Loss: 0.60927\n","Epoch: 00 [ 5591/20299 ( 28%)], Train Loss: 0.60838\n","Epoch: 00 [ 5601/20299 ( 28%)], Train Loss: 0.60821\n","Epoch: 00 [ 5611/20299 ( 28%)], Train Loss: 0.60824\n","Epoch: 00 [ 5621/20299 ( 28%)], Train Loss: 0.60754\n","Epoch: 00 [ 5631/20299 ( 28%)], Train Loss: 0.60722\n","Epoch: 00 [ 5641/20299 ( 28%)], Train Loss: 0.60695\n","Epoch: 00 [ 5651/20299 ( 28%)], Train Loss: 0.60700\n","Epoch: 00 [ 5661/20299 ( 28%)], Train Loss: 0.60646\n","Epoch: 00 [ 5671/20299 ( 28%)], Train Loss: 0.60628\n","Epoch: 00 [ 5681/20299 ( 28%)], Train Loss: 0.60599\n","Epoch: 00 [ 5691/20299 ( 28%)], Train Loss: 0.60589\n","Epoch: 00 [ 5701/20299 ( 28%)], Train Loss: 0.60528\n","Epoch: 00 [ 5711/20299 ( 28%)], Train Loss: 0.60446\n","Epoch: 00 [ 5721/20299 ( 28%)], Train Loss: 0.60426\n","Epoch: 00 [ 5731/20299 ( 28%)], Train Loss: 0.60430\n","Epoch: 00 [ 5741/20299 ( 28%)], Train Loss: 0.60453\n","Epoch: 00 [ 5751/20299 ( 28%)], Train Loss: 0.60405\n","Epoch: 00 [ 5761/20299 ( 28%)], Train Loss: 0.60402\n","Epoch: 00 [ 5771/20299 ( 28%)], Train Loss: 0.60349\n","Epoch: 00 [ 5781/20299 ( 28%)], Train Loss: 0.60326\n","Epoch: 00 [ 5791/20299 ( 29%)], Train Loss: 0.60293\n","Epoch: 00 [ 5801/20299 ( 29%)], Train Loss: 0.60245\n","Epoch: 00 [ 5811/20299 ( 29%)], Train Loss: 0.60180\n","Epoch: 00 [ 5821/20299 ( 29%)], Train Loss: 0.60168\n","Epoch: 00 [ 5831/20299 ( 29%)], Train Loss: 0.60146\n","Epoch: 00 [ 5841/20299 ( 29%)], Train Loss: 0.60110\n","Epoch: 00 [ 5851/20299 ( 29%)], Train Loss: 0.60045\n","Epoch: 00 [ 5861/20299 ( 29%)], Train Loss: 0.59998\n","Epoch: 00 [ 5871/20299 ( 29%)], Train Loss: 0.59924\n","Epoch: 00 [ 5881/20299 ( 29%)], Train Loss: 0.59925\n","Epoch: 00 [ 5891/20299 ( 29%)], Train Loss: 0.59896\n","Epoch: 00 [ 5901/20299 ( 29%)], Train Loss: 0.59870\n","Epoch: 00 [ 5911/20299 ( 29%)], Train Loss: 0.59861\n","Epoch: 00 [ 5921/20299 ( 29%)], Train Loss: 0.59846\n","Epoch: 00 [ 5931/20299 ( 29%)], Train Loss: 0.59842\n","Epoch: 00 [ 5941/20299 ( 29%)], Train Loss: 0.59807\n","Epoch: 00 [ 5951/20299 ( 29%)], Train Loss: 0.59748\n","Epoch: 00 [ 5961/20299 ( 29%)], Train Loss: 0.59732\n","Epoch: 00 [ 5971/20299 ( 29%)], Train Loss: 0.59702\n","Epoch: 00 [ 5981/20299 ( 29%)], Train Loss: 0.59702\n","Epoch: 00 [ 5991/20299 ( 30%)], Train Loss: 0.59609\n","Epoch: 00 [ 6001/20299 ( 30%)], Train Loss: 0.59547\n","Epoch: 00 [ 6011/20299 ( 30%)], Train Loss: 0.59504\n","Epoch: 00 [ 6021/20299 ( 30%)], Train Loss: 0.59490\n","Epoch: 00 [ 6031/20299 ( 30%)], Train Loss: 0.59430\n","Epoch: 00 [ 6041/20299 ( 30%)], Train Loss: 0.59496\n","Epoch: 00 [ 6051/20299 ( 30%)], Train Loss: 0.59441\n","Epoch: 00 [ 6061/20299 ( 30%)], Train Loss: 0.59403\n","Epoch: 00 [ 6071/20299 ( 30%)], Train Loss: 0.59306\n","Epoch: 00 [ 6081/20299 ( 30%)], Train Loss: 0.59276\n","Epoch: 00 [ 6091/20299 ( 30%)], Train Loss: 0.59210\n","Epoch: 00 [ 6101/20299 ( 30%)], Train Loss: 0.59220\n","Epoch: 00 [ 6111/20299 ( 30%)], Train Loss: 0.59192\n","Epoch: 00 [ 6121/20299 ( 30%)], Train Loss: 0.59177\n","Epoch: 00 [ 6131/20299 ( 30%)], Train Loss: 0.59133\n","Epoch: 00 [ 6141/20299 ( 30%)], Train Loss: 0.59173\n","Epoch: 00 [ 6151/20299 ( 30%)], Train Loss: 0.59173\n","Epoch: 00 [ 6161/20299 ( 30%)], Train Loss: 0.59191\n","Epoch: 00 [ 6171/20299 ( 30%)], Train Loss: 0.59110\n","Epoch: 00 [ 6181/20299 ( 30%)], Train Loss: 0.59063\n","Epoch: 00 [ 6191/20299 ( 30%)], Train Loss: 0.59052\n","Epoch: 00 [ 6201/20299 ( 31%)], Train Loss: 0.59034\n","Epoch: 00 [ 6211/20299 ( 31%)], Train Loss: 0.59029\n","Epoch: 00 [ 6221/20299 ( 31%)], Train Loss: 0.58970\n","Epoch: 00 [ 6231/20299 ( 31%)], Train Loss: 0.58974\n","Epoch: 00 [ 6241/20299 ( 31%)], Train Loss: 0.58969\n","Epoch: 00 [ 6251/20299 ( 31%)], Train Loss: 0.58920\n","Epoch: 00 [ 6261/20299 ( 31%)], Train Loss: 0.58854\n","Epoch: 00 [ 6271/20299 ( 31%)], Train Loss: 0.58880\n","Epoch: 00 [ 6281/20299 ( 31%)], Train Loss: 0.58810\n","Epoch: 00 [ 6291/20299 ( 31%)], Train Loss: 0.58888\n","Epoch: 00 [ 6301/20299 ( 31%)], Train Loss: 0.58875\n","Epoch: 00 [ 6311/20299 ( 31%)], Train Loss: 0.58856\n","Epoch: 00 [ 6321/20299 ( 31%)], Train Loss: 0.58783\n","Epoch: 00 [ 6331/20299 ( 31%)], Train Loss: 0.58740\n","Epoch: 00 [ 6341/20299 ( 31%)], Train Loss: 0.58727\n","Epoch: 00 [ 6351/20299 ( 31%)], Train Loss: 0.58704\n","Epoch: 00 [ 6361/20299 ( 31%)], Train Loss: 0.58643\n","Epoch: 00 [ 6371/20299 ( 31%)], Train Loss: 0.58606\n","Epoch: 00 [ 6381/20299 ( 31%)], Train Loss: 0.58552\n","Epoch: 00 [ 6391/20299 ( 31%)], Train Loss: 0.58495\n","Epoch: 00 [ 6401/20299 ( 32%)], Train Loss: 0.58460\n","Epoch: 00 [ 6411/20299 ( 32%)], Train Loss: 0.58408\n","Epoch: 00 [ 6421/20299 ( 32%)], Train Loss: 0.58415\n","Epoch: 00 [ 6431/20299 ( 32%)], Train Loss: 0.58353\n","Epoch: 00 [ 6441/20299 ( 32%)], Train Loss: 0.58329\n","Epoch: 00 [ 6451/20299 ( 32%)], Train Loss: 0.58309\n","Epoch: 00 [ 6461/20299 ( 32%)], Train Loss: 0.58301\n","Epoch: 00 [ 6471/20299 ( 32%)], Train Loss: 0.58289\n","Epoch: 00 [ 6481/20299 ( 32%)], Train Loss: 0.58293\n","Epoch: 00 [ 6491/20299 ( 32%)], Train Loss: 0.58235\n","Epoch: 00 [ 6501/20299 ( 32%)], Train Loss: 0.58188\n","Epoch: 00 [ 6511/20299 ( 32%)], Train Loss: 0.58157\n","Epoch: 00 [ 6521/20299 ( 32%)], Train Loss: 0.58087\n","Epoch: 00 [ 6531/20299 ( 32%)], Train Loss: 0.58002\n","Epoch: 00 [ 6541/20299 ( 32%)], Train Loss: 0.58007\n","Epoch: 00 [ 6551/20299 ( 32%)], Train Loss: 0.58030\n","Epoch: 00 [ 6561/20299 ( 32%)], Train Loss: 0.57994\n","Epoch: 00 [ 6571/20299 ( 32%)], Train Loss: 0.57917\n","Epoch: 00 [ 6581/20299 ( 32%)], Train Loss: 0.57897\n","Epoch: 00 [ 6591/20299 ( 32%)], Train Loss: 0.57844\n","Epoch: 00 [ 6601/20299 ( 33%)], Train Loss: 0.57871\n","Epoch: 00 [ 6611/20299 ( 33%)], Train Loss: 0.57832\n","Epoch: 00 [ 6621/20299 ( 33%)], Train Loss: 0.57816\n","Epoch: 00 [ 6631/20299 ( 33%)], Train Loss: 0.57744\n","Epoch: 00 [ 6641/20299 ( 33%)], Train Loss: 0.57761\n","Epoch: 00 [ 6651/20299 ( 33%)], Train Loss: 0.57775\n","Epoch: 00 [ 6661/20299 ( 33%)], Train Loss: 0.57740\n","Epoch: 00 [ 6671/20299 ( 33%)], Train Loss: 0.57706\n","Epoch: 00 [ 6681/20299 ( 33%)], Train Loss: 0.57669\n","Epoch: 00 [ 6691/20299 ( 33%)], Train Loss: 0.57662\n","Epoch: 00 [ 6701/20299 ( 33%)], Train Loss: 0.57685\n","Epoch: 00 [ 6711/20299 ( 33%)], Train Loss: 0.57649\n","Epoch: 00 [ 6721/20299 ( 33%)], Train Loss: 0.57660\n","Epoch: 00 [ 6731/20299 ( 33%)], Train Loss: 0.57658\n","Epoch: 00 [ 6741/20299 ( 33%)], Train Loss: 0.57668\n","Epoch: 00 [ 6751/20299 ( 33%)], Train Loss: 0.57687\n","Epoch: 00 [ 6761/20299 ( 33%)], Train Loss: 0.57740\n","Epoch: 00 [ 6771/20299 ( 33%)], Train Loss: 0.57688\n","Epoch: 00 [ 6781/20299 ( 33%)], Train Loss: 0.57689\n","Epoch: 00 [ 6791/20299 ( 33%)], Train Loss: 0.57672\n","Epoch: 00 [ 6801/20299 ( 34%)], Train Loss: 0.57642\n","Epoch: 00 [ 6811/20299 ( 34%)], Train Loss: 0.57633\n","Epoch: 00 [ 6821/20299 ( 34%)], Train Loss: 0.57613\n","Epoch: 00 [ 6831/20299 ( 34%)], Train Loss: 0.57534\n","Epoch: 00 [ 6841/20299 ( 34%)], Train Loss: 0.57538\n","Epoch: 00 [ 6851/20299 ( 34%)], Train Loss: 0.57517\n","Epoch: 00 [ 6861/20299 ( 34%)], Train Loss: 0.57469\n","Epoch: 00 [ 6871/20299 ( 34%)], Train Loss: 0.57411\n","Epoch: 00 [ 6881/20299 ( 34%)], Train Loss: 0.57442\n","Epoch: 00 [ 6891/20299 ( 34%)], Train Loss: 0.57393\n","Epoch: 00 [ 6901/20299 ( 34%)], Train Loss: 0.57407\n","Epoch: 00 [ 6911/20299 ( 34%)], Train Loss: 0.57502\n","Epoch: 00 [ 6921/20299 ( 34%)], Train Loss: 0.57449\n","Epoch: 00 [ 6931/20299 ( 34%)], Train Loss: 0.57477\n","Epoch: 00 [ 6941/20299 ( 34%)], Train Loss: 0.57469\n","Epoch: 00 [ 6951/20299 ( 34%)], Train Loss: 0.57422\n","Epoch: 00 [ 6961/20299 ( 34%)], Train Loss: 0.57393\n","Epoch: 00 [ 6971/20299 ( 34%)], Train Loss: 0.57405\n","Epoch: 00 [ 6981/20299 ( 34%)], Train Loss: 0.57359\n","Epoch: 00 [ 6991/20299 ( 34%)], Train Loss: 0.57329\n","Epoch: 00 [ 7001/20299 ( 34%)], Train Loss: 0.57356\n","Epoch: 00 [ 7011/20299 ( 35%)], Train Loss: 0.57316\n","Epoch: 00 [ 7021/20299 ( 35%)], Train Loss: 0.57296\n","Epoch: 00 [ 7031/20299 ( 35%)], Train Loss: 0.57316\n","Epoch: 00 [ 7041/20299 ( 35%)], Train Loss: 0.57270\n","Epoch: 00 [ 7051/20299 ( 35%)], Train Loss: 0.57279\n","Epoch: 00 [ 7061/20299 ( 35%)], Train Loss: 0.57238\n","Epoch: 00 [ 7071/20299 ( 35%)], Train Loss: 0.57250\n","Epoch: 00 [ 7081/20299 ( 35%)], Train Loss: 0.57207\n","Epoch: 00 [ 7091/20299 ( 35%)], Train Loss: 0.57222\n","Epoch: 00 [ 7101/20299 ( 35%)], Train Loss: 0.57179\n","Epoch: 00 [ 7111/20299 ( 35%)], Train Loss: 0.57233\n","Epoch: 00 [ 7121/20299 ( 35%)], Train Loss: 0.57252\n","Epoch: 00 [ 7131/20299 ( 35%)], Train Loss: 0.57206\n","Epoch: 00 [ 7141/20299 ( 35%)], Train Loss: 0.57149\n","Epoch: 00 [ 7151/20299 ( 35%)], Train Loss: 0.57222\n","Epoch: 00 [ 7161/20299 ( 35%)], Train Loss: 0.57217\n","Epoch: 00 [ 7171/20299 ( 35%)], Train Loss: 0.57171\n","Epoch: 00 [ 7181/20299 ( 35%)], Train Loss: 0.57117\n","Epoch: 00 [ 7191/20299 ( 35%)], Train Loss: 0.57096\n","Epoch: 00 [ 7201/20299 ( 35%)], Train Loss: 0.57110\n","Epoch: 00 [ 7211/20299 ( 36%)], Train Loss: 0.57063\n","Epoch: 00 [ 7221/20299 ( 36%)], Train Loss: 0.57056\n","Epoch: 00 [ 7231/20299 ( 36%)], Train Loss: 0.57012\n","Epoch: 00 [ 7241/20299 ( 36%)], Train Loss: 0.57016\n","Epoch: 00 [ 7251/20299 ( 36%)], Train Loss: 0.56978\n","Epoch: 00 [ 7261/20299 ( 36%)], Train Loss: 0.56962\n","Epoch: 00 [ 7271/20299 ( 36%)], Train Loss: 0.56941\n","Epoch: 00 [ 7281/20299 ( 36%)], Train Loss: 0.56936\n","Epoch: 00 [ 7291/20299 ( 36%)], Train Loss: 0.56895\n","Epoch: 00 [ 7301/20299 ( 36%)], Train Loss: 0.56890\n","Epoch: 00 [ 7311/20299 ( 36%)], Train Loss: 0.56837\n","Epoch: 00 [ 7321/20299 ( 36%)], Train Loss: 0.56773\n","Epoch: 00 [ 7331/20299 ( 36%)], Train Loss: 0.56758\n","Epoch: 00 [ 7341/20299 ( 36%)], Train Loss: 0.56749\n","Epoch: 00 [ 7351/20299 ( 36%)], Train Loss: 0.56740\n","Epoch: 00 [ 7361/20299 ( 36%)], Train Loss: 0.56667\n","Epoch: 00 [ 7371/20299 ( 36%)], Train Loss: 0.56667\n","Epoch: 00 [ 7381/20299 ( 36%)], Train Loss: 0.56662\n","Epoch: 00 [ 7391/20299 ( 36%)], Train Loss: 0.56598\n","Epoch: 00 [ 7401/20299 ( 36%)], Train Loss: 0.56528\n","Epoch: 00 [ 7411/20299 ( 37%)], Train Loss: 0.56487\n","Epoch: 00 [ 7421/20299 ( 37%)], Train Loss: 0.56459\n","Epoch: 00 [ 7431/20299 ( 37%)], Train Loss: 0.56393\n","Epoch: 00 [ 7441/20299 ( 37%)], Train Loss: 0.56334\n","Epoch: 00 [ 7451/20299 ( 37%)], Train Loss: 0.56275\n","Epoch: 00 [ 7461/20299 ( 37%)], Train Loss: 0.56228\n","Epoch: 00 [ 7471/20299 ( 37%)], Train Loss: 0.56246\n","Epoch: 00 [ 7481/20299 ( 37%)], Train Loss: 0.56322\n","Epoch: 00 [ 7491/20299 ( 37%)], Train Loss: 0.56276\n","Epoch: 00 [ 7501/20299 ( 37%)], Train Loss: 0.56273\n","Epoch: 00 [ 7511/20299 ( 37%)], Train Loss: 0.56276\n","Epoch: 00 [ 7521/20299 ( 37%)], Train Loss: 0.56284\n","Epoch: 00 [ 7531/20299 ( 37%)], Train Loss: 0.56213\n","Epoch: 00 [ 7541/20299 ( 37%)], Train Loss: 0.56178\n","Epoch: 00 [ 7551/20299 ( 37%)], Train Loss: 0.56170\n","Epoch: 00 [ 7561/20299 ( 37%)], Train Loss: 0.56160\n","Epoch: 00 [ 7571/20299 ( 37%)], Train Loss: 0.56118\n","Epoch: 00 [ 7581/20299 ( 37%)], Train Loss: 0.56194\n","Epoch: 00 [ 7591/20299 ( 37%)], Train Loss: 0.56132\n","Epoch: 00 [ 7601/20299 ( 37%)], Train Loss: 0.56140\n","Epoch: 00 [ 7611/20299 ( 37%)], Train Loss: 0.56137\n","Epoch: 00 [ 7621/20299 ( 38%)], Train Loss: 0.56113\n","Epoch: 00 [ 7631/20299 ( 38%)], Train Loss: 0.56106\n","Epoch: 00 [ 7641/20299 ( 38%)], Train Loss: 0.56075\n","Epoch: 00 [ 7651/20299 ( 38%)], Train Loss: 0.56061\n","Epoch: 00 [ 7661/20299 ( 38%)], Train Loss: 0.56036\n","Epoch: 00 [ 7671/20299 ( 38%)], Train Loss: 0.56068\n","Epoch: 00 [ 7681/20299 ( 38%)], Train Loss: 0.56089\n","Epoch: 00 [ 7691/20299 ( 38%)], Train Loss: 0.56067\n","Epoch: 00 [ 7701/20299 ( 38%)], Train Loss: 0.56053\n","Epoch: 00 [ 7711/20299 ( 38%)], Train Loss: 0.56027\n","Epoch: 00 [ 7721/20299 ( 38%)], Train Loss: 0.56009\n","Epoch: 00 [ 7731/20299 ( 38%)], Train Loss: 0.55977\n","Epoch: 00 [ 7741/20299 ( 38%)], Train Loss: 0.55938\n","Epoch: 00 [ 7751/20299 ( 38%)], Train Loss: 0.55887\n","Epoch: 00 [ 7761/20299 ( 38%)], Train Loss: 0.55843\n","Epoch: 00 [ 7771/20299 ( 38%)], Train Loss: 0.55794\n","Epoch: 00 [ 7781/20299 ( 38%)], Train Loss: 0.55758\n","Epoch: 00 [ 7791/20299 ( 38%)], Train Loss: 0.55798\n","Epoch: 00 [ 7801/20299 ( 38%)], Train Loss: 0.55772\n","Epoch: 00 [ 7811/20299 ( 38%)], Train Loss: 0.55775\n","Epoch: 00 [ 7821/20299 ( 39%)], Train Loss: 0.55820\n","Epoch: 00 [ 7831/20299 ( 39%)], Train Loss: 0.55850\n","Epoch: 00 [ 7841/20299 ( 39%)], Train Loss: 0.55815\n","Epoch: 00 [ 7851/20299 ( 39%)], Train Loss: 0.55758\n","Epoch: 00 [ 7861/20299 ( 39%)], Train Loss: 0.55758\n","Epoch: 00 [ 7871/20299 ( 39%)], Train Loss: 0.55741\n","Epoch: 00 [ 7881/20299 ( 39%)], Train Loss: 0.55733\n","Epoch: 00 [ 7891/20299 ( 39%)], Train Loss: 0.55716\n","Epoch: 00 [ 7901/20299 ( 39%)], Train Loss: 0.55732\n","Epoch: 00 [ 7911/20299 ( 39%)], Train Loss: 0.55726\n","Epoch: 00 [ 7921/20299 ( 39%)], Train Loss: 0.55713\n","Epoch: 00 [ 7931/20299 ( 39%)], Train Loss: 0.55711\n","Epoch: 00 [ 7941/20299 ( 39%)], Train Loss: 0.55710\n","Epoch: 00 [ 7951/20299 ( 39%)], Train Loss: 0.55720\n","Epoch: 00 [ 7961/20299 ( 39%)], Train Loss: 0.55694\n","Epoch: 00 [ 7971/20299 ( 39%)], Train Loss: 0.55649\n","Epoch: 00 [ 7981/20299 ( 39%)], Train Loss: 0.55636\n","Epoch: 00 [ 7991/20299 ( 39%)], Train Loss: 0.55605\n","Epoch: 00 [ 8001/20299 ( 39%)], Train Loss: 0.55624\n","Epoch: 00 [ 8011/20299 ( 39%)], Train Loss: 0.55611\n","Epoch: 00 [ 8021/20299 ( 40%)], Train Loss: 0.55599\n","Epoch: 00 [ 8031/20299 ( 40%)], Train Loss: 0.55606\n","Epoch: 00 [ 8041/20299 ( 40%)], Train Loss: 0.55584\n","Epoch: 00 [ 8051/20299 ( 40%)], Train Loss: 0.55626\n","Epoch: 00 [ 8061/20299 ( 40%)], Train Loss: 0.55643\n","Epoch: 00 [ 8071/20299 ( 40%)], Train Loss: 0.55623\n","Epoch: 00 [ 8081/20299 ( 40%)], Train Loss: 0.55611\n","Epoch: 00 [ 8091/20299 ( 40%)], Train Loss: 0.55574\n","Epoch: 00 [ 8101/20299 ( 40%)], Train Loss: 0.55571\n","Epoch: 00 [ 8111/20299 ( 40%)], Train Loss: 0.55533\n","Epoch: 00 [ 8121/20299 ( 40%)], Train Loss: 0.55607\n","Epoch: 00 [ 8131/20299 ( 40%)], Train Loss: 0.55657\n","Epoch: 00 [ 8141/20299 ( 40%)], Train Loss: 0.55639\n","Epoch: 00 [ 8151/20299 ( 40%)], Train Loss: 0.55590\n","Epoch: 00 [ 8161/20299 ( 40%)], Train Loss: 0.55580\n","Epoch: 00 [ 8171/20299 ( 40%)], Train Loss: 0.55552\n","Epoch: 00 [ 8181/20299 ( 40%)], Train Loss: 0.55568\n","Epoch: 00 [ 8191/20299 ( 40%)], Train Loss: 0.55531\n","Epoch: 00 [ 8201/20299 ( 40%)], Train Loss: 0.55538\n","Epoch: 00 [ 8211/20299 ( 40%)], Train Loss: 0.55500\n","Epoch: 00 [ 8221/20299 ( 40%)], Train Loss: 0.55491\n","Epoch: 00 [ 8231/20299 ( 41%)], Train Loss: 0.55494\n","Epoch: 00 [ 8241/20299 ( 41%)], Train Loss: 0.55472\n","Epoch: 00 [ 8251/20299 ( 41%)], Train Loss: 0.55497\n","Epoch: 00 [ 8261/20299 ( 41%)], Train Loss: 0.55503\n","Epoch: 00 [ 8271/20299 ( 41%)], Train Loss: 0.55491\n","Epoch: 00 [ 8281/20299 ( 41%)], Train Loss: 0.55492\n","Epoch: 00 [ 8291/20299 ( 41%)], Train Loss: 0.55479\n","Epoch: 00 [ 8301/20299 ( 41%)], Train Loss: 0.55460\n","Epoch: 00 [ 8311/20299 ( 41%)], Train Loss: 0.55417\n","Epoch: 00 [ 8321/20299 ( 41%)], Train Loss: 0.55401\n","Epoch: 00 [ 8331/20299 ( 41%)], Train Loss: 0.55392\n","Epoch: 00 [ 8341/20299 ( 41%)], Train Loss: 0.55349\n","Epoch: 00 [ 8351/20299 ( 41%)], Train Loss: 0.55291\n","Epoch: 00 [ 8361/20299 ( 41%)], Train Loss: 0.55254\n","Epoch: 00 [ 8371/20299 ( 41%)], Train Loss: 0.55205\n","Epoch: 00 [ 8381/20299 ( 41%)], Train Loss: 0.55186\n","Epoch: 00 [ 8391/20299 ( 41%)], Train Loss: 0.55186\n","Epoch: 00 [ 8401/20299 ( 41%)], Train Loss: 0.55133\n","Epoch: 00 [ 8411/20299 ( 41%)], Train Loss: 0.55157\n","Epoch: 00 [ 8421/20299 ( 41%)], Train Loss: 0.55117\n","Epoch: 00 [ 8431/20299 ( 42%)], Train Loss: 0.55080\n","Epoch: 00 [ 8441/20299 ( 42%)], Train Loss: 0.55110\n","Epoch: 00 [ 8451/20299 ( 42%)], Train Loss: 0.55127\n","Epoch: 00 [ 8461/20299 ( 42%)], Train Loss: 0.55158\n","Epoch: 00 [ 8471/20299 ( 42%)], Train Loss: 0.55188\n","Epoch: 00 [ 8481/20299 ( 42%)], Train Loss: 0.55169\n","Epoch: 00 [ 8491/20299 ( 42%)], Train Loss: 0.55117\n","Epoch: 00 [ 8501/20299 ( 42%)], Train Loss: 0.55104\n","Epoch: 00 [ 8511/20299 ( 42%)], Train Loss: 0.55086\n","Epoch: 00 [ 8521/20299 ( 42%)], Train Loss: 0.55061\n","Epoch: 00 [ 8531/20299 ( 42%)], Train Loss: 0.55036\n","Epoch: 00 [ 8541/20299 ( 42%)], Train Loss: 0.54984\n","Epoch: 00 [ 8551/20299 ( 42%)], Train Loss: 0.54962\n","Epoch: 00 [ 8561/20299 ( 42%)], Train Loss: 0.54971\n","Epoch: 00 [ 8571/20299 ( 42%)], Train Loss: 0.54978\n","Epoch: 00 [ 8581/20299 ( 42%)], Train Loss: 0.54952\n","Epoch: 00 [ 8591/20299 ( 42%)], Train Loss: 0.54921\n","Epoch: 00 [ 8601/20299 ( 42%)], Train Loss: 0.54907\n","Epoch: 00 [ 8611/20299 ( 42%)], Train Loss: 0.54880\n","Epoch: 00 [ 8621/20299 ( 42%)], Train Loss: 0.54843\n","Epoch: 00 [ 8631/20299 ( 43%)], Train Loss: 0.54825\n","Epoch: 00 [ 8641/20299 ( 43%)], Train Loss: 0.54797\n","Epoch: 00 [ 8651/20299 ( 43%)], Train Loss: 0.54772\n","Epoch: 00 [ 8661/20299 ( 43%)], Train Loss: 0.54718\n","Epoch: 00 [ 8671/20299 ( 43%)], Train Loss: 0.54704\n","Epoch: 00 [ 8681/20299 ( 43%)], Train Loss: 0.54708\n","Epoch: 00 [ 8691/20299 ( 43%)], Train Loss: 0.54722\n","Epoch: 00 [ 8701/20299 ( 43%)], Train Loss: 0.54728\n","Epoch: 00 [ 8711/20299 ( 43%)], Train Loss: 0.54708\n","Epoch: 00 [ 8721/20299 ( 43%)], Train Loss: 0.54704\n","Epoch: 00 [ 8731/20299 ( 43%)], Train Loss: 0.54704\n","Epoch: 00 [ 8741/20299 ( 43%)], Train Loss: 0.54702\n","Epoch: 00 [ 8751/20299 ( 43%)], Train Loss: 0.54668\n","Epoch: 00 [ 8761/20299 ( 43%)], Train Loss: 0.54662\n","Epoch: 00 [ 8771/20299 ( 43%)], Train Loss: 0.54631\n","Epoch: 00 [ 8781/20299 ( 43%)], Train Loss: 0.54609\n","Epoch: 00 [ 8791/20299 ( 43%)], Train Loss: 0.54624\n","Epoch: 00 [ 8801/20299 ( 43%)], Train Loss: 0.54600\n","Epoch: 00 [ 8811/20299 ( 43%)], Train Loss: 0.54562\n","Epoch: 00 [ 8821/20299 ( 43%)], Train Loss: 0.54552\n","Epoch: 00 [ 8831/20299 ( 44%)], Train Loss: 0.54608\n","Epoch: 00 [ 8841/20299 ( 44%)], Train Loss: 0.54643\n","Epoch: 00 [ 8851/20299 ( 44%)], Train Loss: 0.54668\n","Epoch: 00 [ 8861/20299 ( 44%)], Train Loss: 0.54664\n","Epoch: 00 [ 8871/20299 ( 44%)], Train Loss: 0.54677\n","Epoch: 00 [ 8881/20299 ( 44%)], Train Loss: 0.54674\n","Epoch: 00 [ 8891/20299 ( 44%)], Train Loss: 0.54642\n","Epoch: 00 [ 8901/20299 ( 44%)], Train Loss: 0.54583\n","Epoch: 00 [ 8911/20299 ( 44%)], Train Loss: 0.54545\n","Epoch: 00 [ 8921/20299 ( 44%)], Train Loss: 0.54499\n","Epoch: 00 [ 8931/20299 ( 44%)], Train Loss: 0.54518\n","Epoch: 00 [ 8941/20299 ( 44%)], Train Loss: 0.54501\n","Epoch: 00 [ 8951/20299 ( 44%)], Train Loss: 0.54492\n","Epoch: 00 [ 8961/20299 ( 44%)], Train Loss: 0.54482\n","Epoch: 00 [ 8971/20299 ( 44%)], Train Loss: 0.54471\n","Epoch: 00 [ 8981/20299 ( 44%)], Train Loss: 0.54456\n","Epoch: 00 [ 8991/20299 ( 44%)], Train Loss: 0.54441\n","Epoch: 00 [ 9001/20299 ( 44%)], Train Loss: 0.54410\n","Epoch: 00 [ 9011/20299 ( 44%)], Train Loss: 0.54367\n","Epoch: 00 [ 9021/20299 ( 44%)], Train Loss: 0.54355\n","Epoch: 00 [ 9031/20299 ( 44%)], Train Loss: 0.54311\n","Epoch: 00 [ 9041/20299 ( 45%)], Train Loss: 0.54377\n","Epoch: 00 [ 9051/20299 ( 45%)], Train Loss: 0.54350\n","Epoch: 00 [ 9061/20299 ( 45%)], Train Loss: 0.54393\n","Epoch: 00 [ 9071/20299 ( 45%)], Train Loss: 0.54364\n","Epoch: 00 [ 9081/20299 ( 45%)], Train Loss: 0.54373\n","Epoch: 00 [ 9091/20299 ( 45%)], Train Loss: 0.54411\n","Epoch: 00 [ 9101/20299 ( 45%)], Train Loss: 0.54384\n","Epoch: 00 [ 9111/20299 ( 45%)], Train Loss: 0.54388\n","Epoch: 00 [ 9121/20299 ( 45%)], Train Loss: 0.54421\n","Epoch: 00 [ 9131/20299 ( 45%)], Train Loss: 0.54404\n","Epoch: 00 [ 9141/20299 ( 45%)], Train Loss: 0.54426\n","Epoch: 00 [ 9151/20299 ( 45%)], Train Loss: 0.54420\n","Epoch: 00 [ 9161/20299 ( 45%)], Train Loss: 0.54403\n","Epoch: 00 [ 9171/20299 ( 45%)], Train Loss: 0.54370\n","Epoch: 00 [ 9181/20299 ( 45%)], Train Loss: 0.54358\n","Epoch: 00 [ 9191/20299 ( 45%)], Train Loss: 0.54309\n","Epoch: 00 [ 9201/20299 ( 45%)], Train Loss: 0.54308\n","Epoch: 00 [ 9211/20299 ( 45%)], Train Loss: 0.54292\n","Epoch: 00 [ 9221/20299 ( 45%)], Train Loss: 0.54319\n","Epoch: 00 [ 9231/20299 ( 45%)], Train Loss: 0.54316\n","Epoch: 00 [ 9241/20299 ( 46%)], Train Loss: 0.54312\n","Epoch: 00 [ 9251/20299 ( 46%)], Train Loss: 0.54286\n","Epoch: 00 [ 9261/20299 ( 46%)], Train Loss: 0.54284\n","Epoch: 00 [ 9271/20299 ( 46%)], Train Loss: 0.54246\n","Epoch: 00 [ 9281/20299 ( 46%)], Train Loss: 0.54283\n","Epoch: 00 [ 9291/20299 ( 46%)], Train Loss: 0.54233\n","Epoch: 00 [ 9301/20299 ( 46%)], Train Loss: 0.54201\n","Epoch: 00 [ 9311/20299 ( 46%)], Train Loss: 0.54205\n","Epoch: 00 [ 9321/20299 ( 46%)], Train Loss: 0.54186\n","Epoch: 00 [ 9331/20299 ( 46%)], Train Loss: 0.54235\n","Epoch: 00 [ 9341/20299 ( 46%)], Train Loss: 0.54207\n","Epoch: 00 [ 9351/20299 ( 46%)], Train Loss: 0.54188\n","Epoch: 00 [ 9361/20299 ( 46%)], Train Loss: 0.54195\n","Epoch: 00 [ 9371/20299 ( 46%)], Train Loss: 0.54170\n","Epoch: 00 [ 9381/20299 ( 46%)], Train Loss: 0.54186\n","Epoch: 00 [ 9391/20299 ( 46%)], Train Loss: 0.54171\n","Epoch: 00 [ 9401/20299 ( 46%)], Train Loss: 0.54132\n","Epoch: 00 [ 9411/20299 ( 46%)], Train Loss: 0.54108\n","Epoch: 00 [ 9421/20299 ( 46%)], Train Loss: 0.54080\n","Epoch: 00 [ 9431/20299 ( 46%)], Train Loss: 0.54051\n","Epoch: 00 [ 9441/20299 ( 47%)], Train Loss: 0.54065\n","Epoch: 00 [ 9451/20299 ( 47%)], Train Loss: 0.54025\n","Epoch: 00 [ 9461/20299 ( 47%)], Train Loss: 0.54024\n","Epoch: 00 [ 9471/20299 ( 47%)], Train Loss: 0.54017\n","Epoch: 00 [ 9481/20299 ( 47%)], Train Loss: 0.53981\n","Epoch: 00 [ 9491/20299 ( 47%)], Train Loss: 0.53965\n","Epoch: 00 [ 9501/20299 ( 47%)], Train Loss: 0.53943\n","Epoch: 00 [ 9511/20299 ( 47%)], Train Loss: 0.53938\n","Epoch: 00 [ 9521/20299 ( 47%)], Train Loss: 0.53923\n","Epoch: 00 [ 9531/20299 ( 47%)], Train Loss: 0.53912\n","Epoch: 00 [ 9541/20299 ( 47%)], Train Loss: 0.53884\n","Epoch: 00 [ 9551/20299 ( 47%)], Train Loss: 0.53880\n","Epoch: 00 [ 9561/20299 ( 47%)], Train Loss: 0.53921\n","Epoch: 00 [ 9571/20299 ( 47%)], Train Loss: 0.53908\n","Epoch: 00 [ 9581/20299 ( 47%)], Train Loss: 0.53897\n","Epoch: 00 [ 9591/20299 ( 47%)], Train Loss: 0.53894\n","Epoch: 00 [ 9601/20299 ( 47%)], Train Loss: 0.53894\n","Epoch: 00 [ 9611/20299 ( 47%)], Train Loss: 0.53860\n","Epoch: 00 [ 9621/20299 ( 47%)], Train Loss: 0.53861\n","Epoch: 00 [ 9631/20299 ( 47%)], Train Loss: 0.53830\n","Epoch: 00 [ 9641/20299 ( 47%)], Train Loss: 0.53790\n","Epoch: 00 [ 9651/20299 ( 48%)], Train Loss: 0.53742\n","Epoch: 00 [ 9661/20299 ( 48%)], Train Loss: 0.53772\n","Epoch: 00 [ 9671/20299 ( 48%)], Train Loss: 0.53759\n","Epoch: 00 [ 9681/20299 ( 48%)], Train Loss: 0.53730\n","Epoch: 00 [ 9691/20299 ( 48%)], Train Loss: 0.53742\n","Epoch: 00 [ 9701/20299 ( 48%)], Train Loss: 0.53723\n","Epoch: 00 [ 9711/20299 ( 48%)], Train Loss: 0.53674\n","Epoch: 00 [ 9721/20299 ( 48%)], Train Loss: 0.53665\n","Epoch: 00 [ 9731/20299 ( 48%)], Train Loss: 0.53630\n","Epoch: 00 [ 9741/20299 ( 48%)], Train Loss: 0.53625\n","Epoch: 00 [ 9751/20299 ( 48%)], Train Loss: 0.53644\n","Epoch: 00 [ 9761/20299 ( 48%)], Train Loss: 0.53636\n","Epoch: 00 [ 9771/20299 ( 48%)], Train Loss: 0.53596\n","Epoch: 00 [ 9781/20299 ( 48%)], Train Loss: 0.53573\n","Epoch: 00 [ 9791/20299 ( 48%)], Train Loss: 0.53543\n","Epoch: 00 [ 9801/20299 ( 48%)], Train Loss: 0.53510\n","Epoch: 00 [ 9811/20299 ( 48%)], Train Loss: 0.53495\n","Epoch: 00 [ 9821/20299 ( 48%)], Train Loss: 0.53454\n","Epoch: 00 [ 9831/20299 ( 48%)], Train Loss: 0.53406\n","Epoch: 00 [ 9841/20299 ( 48%)], Train Loss: 0.53464\n","Epoch: 00 [ 9851/20299 ( 49%)], Train Loss: 0.53417\n","Epoch: 00 [ 9861/20299 ( 49%)], Train Loss: 0.53384\n","Epoch: 00 [ 9871/20299 ( 49%)], Train Loss: 0.53359\n","Epoch: 00 [ 9881/20299 ( 49%)], Train Loss: 0.53380\n","Epoch: 00 [ 9891/20299 ( 49%)], Train Loss: 0.53372\n","Epoch: 00 [ 9901/20299 ( 49%)], Train Loss: 0.53373\n","Epoch: 00 [ 9911/20299 ( 49%)], Train Loss: 0.53396\n","Epoch: 00 [ 9921/20299 ( 49%)], Train Loss: 0.53394\n","Epoch: 00 [ 9931/20299 ( 49%)], Train Loss: 0.53382\n","Epoch: 00 [ 9941/20299 ( 49%)], Train Loss: 0.53400\n","Epoch: 00 [ 9951/20299 ( 49%)], Train Loss: 0.53426\n","Epoch: 00 [ 9961/20299 ( 49%)], Train Loss: 0.53420\n","Epoch: 00 [ 9971/20299 ( 49%)], Train Loss: 0.53425\n","Epoch: 00 [ 9981/20299 ( 49%)], Train Loss: 0.53451\n","Epoch: 00 [ 9991/20299 ( 49%)], Train Loss: 0.53430\n","Epoch: 00 [10001/20299 ( 49%)], Train Loss: 0.53438\n","Epoch: 00 [10011/20299 ( 49%)], Train Loss: 0.53422\n","Epoch: 00 [10021/20299 ( 49%)], Train Loss: 0.53377\n","Epoch: 00 [10031/20299 ( 49%)], Train Loss: 0.53327\n","Epoch: 00 [10041/20299 ( 49%)], Train Loss: 0.53395\n","Epoch: 00 [10051/20299 ( 50%)], Train Loss: 0.53383\n","Epoch: 00 [10061/20299 ( 50%)], Train Loss: 0.53368\n","Epoch: 00 [10071/20299 ( 50%)], Train Loss: 0.53362\n","Epoch: 00 [10081/20299 ( 50%)], Train Loss: 0.53333\n","Epoch: 00 [10091/20299 ( 50%)], Train Loss: 0.53309\n","Epoch: 00 [10101/20299 ( 50%)], Train Loss: 0.53313\n","Epoch: 00 [10111/20299 ( 50%)], Train Loss: 0.53304\n","Epoch: 00 [10121/20299 ( 50%)], Train Loss: 0.53281\n","Epoch: 00 [10131/20299 ( 50%)], Train Loss: 0.53258\n","Epoch: 00 [10141/20299 ( 50%)], Train Loss: 0.53209\n","Epoch: 00 [10151/20299 ( 50%)], Train Loss: 0.53195\n","Epoch: 00 [10161/20299 ( 50%)], Train Loss: 0.53189\n","Epoch: 00 [10171/20299 ( 50%)], Train Loss: 0.53176\n","Epoch: 00 [10181/20299 ( 50%)], Train Loss: 0.53188\n","Epoch: 00 [10191/20299 ( 50%)], Train Loss: 0.53173\n","Epoch: 00 [10201/20299 ( 50%)], Train Loss: 0.53179\n","Epoch: 00 [10211/20299 ( 50%)], Train Loss: 0.53186\n","Epoch: 00 [10221/20299 ( 50%)], Train Loss: 0.53190\n","Epoch: 00 [10231/20299 ( 50%)], Train Loss: 0.53159\n","Epoch: 00 [10241/20299 ( 50%)], Train Loss: 0.53108\n","Epoch: 00 [10251/20299 ( 51%)], Train Loss: 0.53072\n","Epoch: 00 [10261/20299 ( 51%)], Train Loss: 0.53069\n","Epoch: 00 [10271/20299 ( 51%)], Train Loss: 0.53053\n","Epoch: 00 [10281/20299 ( 51%)], Train Loss: 0.53042\n","Epoch: 00 [10291/20299 ( 51%)], Train Loss: 0.52996\n","Epoch: 00 [10301/20299 ( 51%)], Train Loss: 0.52987\n","Epoch: 00 [10311/20299 ( 51%)], Train Loss: 0.52963\n","Epoch: 00 [10321/20299 ( 51%)], Train Loss: 0.52954\n","Epoch: 00 [10331/20299 ( 51%)], Train Loss: 0.52962\n","Epoch: 00 [10341/20299 ( 51%)], Train Loss: 0.52942\n","Epoch: 00 [10351/20299 ( 51%)], Train Loss: 0.52932\n","Epoch: 00 [10361/20299 ( 51%)], Train Loss: 0.52922\n","Epoch: 00 [10371/20299 ( 51%)], Train Loss: 0.52894\n","Epoch: 00 [10381/20299 ( 51%)], Train Loss: 0.52938\n","Epoch: 00 [10391/20299 ( 51%)], Train Loss: 0.52941\n","Epoch: 00 [10401/20299 ( 51%)], Train Loss: 0.52926\n","Epoch: 00 [10411/20299 ( 51%)], Train Loss: 0.52940\n","Epoch: 00 [10421/20299 ( 51%)], Train Loss: 0.52919\n","Epoch: 00 [10431/20299 ( 51%)], Train Loss: 0.52885\n","Epoch: 00 [10441/20299 ( 51%)], Train Loss: 0.52843\n","Epoch: 00 [10451/20299 ( 51%)], Train Loss: 0.52831\n","Epoch: 00 [10461/20299 ( 52%)], Train Loss: 0.52822\n","Epoch: 00 [10471/20299 ( 52%)], Train Loss: 0.52806\n","Epoch: 00 [10481/20299 ( 52%)], Train Loss: 0.52813\n","Epoch: 00 [10491/20299 ( 52%)], Train Loss: 0.52794\n","Epoch: 00 [10501/20299 ( 52%)], Train Loss: 0.52756\n","Epoch: 00 [10511/20299 ( 52%)], Train Loss: 0.52738\n","Epoch: 00 [10521/20299 ( 52%)], Train Loss: 0.52731\n","Epoch: 00 [10531/20299 ( 52%)], Train Loss: 0.52703\n","Epoch: 00 [10541/20299 ( 52%)], Train Loss: 0.52697\n","Epoch: 00 [10551/20299 ( 52%)], Train Loss: 0.52673\n","Epoch: 00 [10561/20299 ( 52%)], Train Loss: 0.52668\n","Epoch: 00 [10571/20299 ( 52%)], Train Loss: 0.52636\n","Epoch: 00 [10581/20299 ( 52%)], Train Loss: 0.52597\n","Epoch: 00 [10591/20299 ( 52%)], Train Loss: 0.52574\n","Epoch: 00 [10601/20299 ( 52%)], Train Loss: 0.52566\n","Epoch: 00 [10611/20299 ( 52%)], Train Loss: 0.52535\n","Epoch: 00 [10621/20299 ( 52%)], Train Loss: 0.52531\n","Epoch: 00 [10631/20299 ( 52%)], Train Loss: 0.52532\n","Epoch: 00 [10641/20299 ( 52%)], Train Loss: 0.52499\n","Epoch: 00 [10651/20299 ( 52%)], Train Loss: 0.52489\n","Epoch: 00 [10661/20299 ( 53%)], Train Loss: 0.52477\n","Epoch: 00 [10671/20299 ( 53%)], Train Loss: 0.52490\n","Epoch: 00 [10681/20299 ( 53%)], Train Loss: 0.52447\n","Epoch: 00 [10691/20299 ( 53%)], Train Loss: 0.52465\n","Epoch: 00 [10701/20299 ( 53%)], Train Loss: 0.52431\n","Epoch: 00 [10711/20299 ( 53%)], Train Loss: 0.52430\n","Epoch: 00 [10721/20299 ( 53%)], Train Loss: 0.52409\n","Epoch: 00 [10731/20299 ( 53%)], Train Loss: 0.52426\n","Epoch: 00 [10741/20299 ( 53%)], Train Loss: 0.52408\n","Epoch: 00 [10751/20299 ( 53%)], Train Loss: 0.52429\n","Epoch: 00 [10761/20299 ( 53%)], Train Loss: 0.52415\n","Epoch: 00 [10771/20299 ( 53%)], Train Loss: 0.52390\n","Epoch: 00 [10781/20299 ( 53%)], Train Loss: 0.52386\n","Epoch: 00 [10791/20299 ( 53%)], Train Loss: 0.52348\n","Epoch: 00 [10801/20299 ( 53%)], Train Loss: 0.52347\n","Epoch: 00 [10811/20299 ( 53%)], Train Loss: 0.52329\n","Epoch: 00 [10821/20299 ( 53%)], Train Loss: 0.52305\n","Epoch: 00 [10831/20299 ( 53%)], Train Loss: 0.52264\n","Epoch: 00 [10841/20299 ( 53%)], Train Loss: 0.52239\n","Epoch: 00 [10851/20299 ( 53%)], Train Loss: 0.52252\n","Epoch: 00 [10861/20299 ( 54%)], Train Loss: 0.52262\n","Epoch: 00 [10871/20299 ( 54%)], Train Loss: 0.52296\n","Epoch: 00 [10881/20299 ( 54%)], Train Loss: 0.52299\n","Epoch: 00 [10891/20299 ( 54%)], Train Loss: 0.52291\n","Epoch: 00 [10901/20299 ( 54%)], Train Loss: 0.52310\n","Epoch: 00 [10911/20299 ( 54%)], Train Loss: 0.52345\n","Epoch: 00 [10921/20299 ( 54%)], Train Loss: 0.52324\n","Epoch: 00 [10931/20299 ( 54%)], Train Loss: 0.52314\n","Epoch: 00 [10941/20299 ( 54%)], Train Loss: 0.52302\n","Epoch: 00 [10951/20299 ( 54%)], Train Loss: 0.52278\n","Epoch: 00 [10961/20299 ( 54%)], Train Loss: 0.52274\n","Epoch: 00 [10971/20299 ( 54%)], Train Loss: 0.52238\n","Epoch: 00 [10981/20299 ( 54%)], Train Loss: 0.52223\n","Epoch: 00 [10991/20299 ( 54%)], Train Loss: 0.52205\n","Epoch: 00 [11001/20299 ( 54%)], Train Loss: 0.52208\n","Epoch: 00 [11011/20299 ( 54%)], Train Loss: 0.52173\n","Epoch: 00 [11021/20299 ( 54%)], Train Loss: 0.52166\n","Epoch: 00 [11031/20299 ( 54%)], Train Loss: 0.52163\n","Epoch: 00 [11041/20299 ( 54%)], Train Loss: 0.52188\n","Epoch: 00 [11051/20299 ( 54%)], Train Loss: 0.52174\n","Epoch: 00 [11061/20299 ( 54%)], Train Loss: 0.52159\n","Epoch: 00 [11071/20299 ( 55%)], Train Loss: 0.52195\n","Epoch: 00 [11081/20299 ( 55%)], Train Loss: 0.52185\n","Epoch: 00 [11091/20299 ( 55%)], Train Loss: 0.52163\n","Epoch: 00 [11101/20299 ( 55%)], Train Loss: 0.52165\n","Epoch: 00 [11111/20299 ( 55%)], Train Loss: 0.52137\n","Epoch: 00 [11121/20299 ( 55%)], Train Loss: 0.52129\n","Epoch: 00 [11131/20299 ( 55%)], Train Loss: 0.52097\n","Epoch: 00 [11141/20299 ( 55%)], Train Loss: 0.52084\n","Epoch: 00 [11151/20299 ( 55%)], Train Loss: 0.52082\n","Epoch: 00 [11161/20299 ( 55%)], Train Loss: 0.52066\n","Epoch: 00 [11171/20299 ( 55%)], Train Loss: 0.52087\n","Epoch: 00 [11181/20299 ( 55%)], Train Loss: 0.52063\n","Epoch: 00 [11191/20299 ( 55%)], Train Loss: 0.52037\n","Epoch: 00 [11201/20299 ( 55%)], Train Loss: 0.52024\n","Epoch: 00 [11211/20299 ( 55%)], Train Loss: 0.52062\n","Epoch: 00 [11221/20299 ( 55%)], Train Loss: 0.52030\n","Epoch: 00 [11231/20299 ( 55%)], Train Loss: 0.52028\n","Epoch: 00 [11241/20299 ( 55%)], Train Loss: 0.52032\n","Epoch: 00 [11251/20299 ( 55%)], Train Loss: 0.52007\n","Epoch: 00 [11261/20299 ( 55%)], Train Loss: 0.51962\n","Epoch: 00 [11271/20299 ( 56%)], Train Loss: 0.51965\n","Epoch: 00 [11281/20299 ( 56%)], Train Loss: 0.51927\n","Epoch: 00 [11291/20299 ( 56%)], Train Loss: 0.51917\n","Epoch: 00 [11301/20299 ( 56%)], Train Loss: 0.51967\n","Epoch: 00 [11311/20299 ( 56%)], Train Loss: 0.51985\n","Epoch: 00 [11321/20299 ( 56%)], Train Loss: 0.52009\n","Epoch: 00 [11331/20299 ( 56%)], Train Loss: 0.51997\n","Epoch: 00 [11341/20299 ( 56%)], Train Loss: 0.52005\n","Epoch: 00 [11351/20299 ( 56%)], Train Loss: 0.51990\n","Epoch: 00 [11361/20299 ( 56%)], Train Loss: 0.51984\n","Epoch: 00 [11371/20299 ( 56%)], Train Loss: 0.51948\n","Epoch: 00 [11381/20299 ( 56%)], Train Loss: 0.51939\n","Epoch: 00 [11391/20299 ( 56%)], Train Loss: 0.51947\n","Epoch: 00 [11401/20299 ( 56%)], Train Loss: 0.51924\n","Epoch: 00 [11411/20299 ( 56%)], Train Loss: 0.51893\n","Epoch: 00 [11421/20299 ( 56%)], Train Loss: 0.51908\n","Epoch: 00 [11431/20299 ( 56%)], Train Loss: 0.51881\n","Epoch: 00 [11441/20299 ( 56%)], Train Loss: 0.51852\n","Epoch: 00 [11451/20299 ( 56%)], Train Loss: 0.51815\n","Epoch: 00 [11461/20299 ( 56%)], Train Loss: 0.51799\n","Epoch: 00 [11471/20299 ( 57%)], Train Loss: 0.51769\n","Epoch: 00 [11481/20299 ( 57%)], Train Loss: 0.51767\n","Epoch: 00 [11491/20299 ( 57%)], Train Loss: 0.51824\n","Epoch: 00 [11501/20299 ( 57%)], Train Loss: 0.51808\n","Epoch: 00 [11511/20299 ( 57%)], Train Loss: 0.51833\n","Epoch: 00 [11521/20299 ( 57%)], Train Loss: 0.51845\n","Epoch: 00 [11531/20299 ( 57%)], Train Loss: 0.51837\n","Epoch: 00 [11541/20299 ( 57%)], Train Loss: 0.51828\n","Epoch: 00 [11551/20299 ( 57%)], Train Loss: 0.51820\n","Epoch: 00 [11561/20299 ( 57%)], Train Loss: 0.51784\n","Epoch: 00 [11571/20299 ( 57%)], Train Loss: 0.51824\n","Epoch: 00 [11581/20299 ( 57%)], Train Loss: 0.51810\n","Epoch: 00 [11591/20299 ( 57%)], Train Loss: 0.51810\n","Epoch: 00 [11601/20299 ( 57%)], Train Loss: 0.51834\n","Epoch: 00 [11611/20299 ( 57%)], Train Loss: 0.51812\n","Epoch: 00 [11621/20299 ( 57%)], Train Loss: 0.51778\n","Epoch: 00 [11631/20299 ( 57%)], Train Loss: 0.51786\n","Epoch: 00 [11641/20299 ( 57%)], Train Loss: 0.51800\n","Epoch: 00 [11651/20299 ( 57%)], Train Loss: 0.51763\n","Epoch: 00 [11661/20299 ( 57%)], Train Loss: 0.51760\n","Epoch: 00 [11671/20299 ( 57%)], Train Loss: 0.51748\n","Epoch: 00 [11681/20299 ( 58%)], Train Loss: 0.51731\n","Epoch: 00 [11691/20299 ( 58%)], Train Loss: 0.51715\n","Epoch: 00 [11701/20299 ( 58%)], Train Loss: 0.51688\n","Epoch: 00 [11711/20299 ( 58%)], Train Loss: 0.51653\n","Epoch: 00 [11721/20299 ( 58%)], Train Loss: 0.51643\n","Epoch: 00 [11731/20299 ( 58%)], Train Loss: 0.51644\n","Epoch: 00 [11741/20299 ( 58%)], Train Loss: 0.51622\n","Epoch: 00 [11751/20299 ( 58%)], Train Loss: 0.51605\n","Epoch: 00 [11761/20299 ( 58%)], Train Loss: 0.51591\n","Epoch: 00 [11771/20299 ( 58%)], Train Loss: 0.51595\n","Epoch: 00 [11781/20299 ( 58%)], Train Loss: 0.51569\n","Epoch: 00 [11791/20299 ( 58%)], Train Loss: 0.51546\n","Epoch: 00 [11801/20299 ( 58%)], Train Loss: 0.51585\n","Epoch: 00 [11811/20299 ( 58%)], Train Loss: 0.51608\n","Epoch: 00 [11821/20299 ( 58%)], Train Loss: 0.51578\n","Epoch: 00 [11831/20299 ( 58%)], Train Loss: 0.51600\n","Epoch: 00 [11841/20299 ( 58%)], Train Loss: 0.51646\n","Epoch: 00 [11851/20299 ( 58%)], Train Loss: 0.51642\n","Epoch: 00 [11861/20299 ( 58%)], Train Loss: 0.51622\n","Epoch: 00 [11871/20299 ( 58%)], Train Loss: 0.51620\n","Epoch: 00 [11881/20299 ( 59%)], Train Loss: 0.51592\n","Epoch: 00 [11891/20299 ( 59%)], Train Loss: 0.51577\n","Epoch: 00 [11901/20299 ( 59%)], Train Loss: 0.51572\n","Epoch: 00 [11911/20299 ( 59%)], Train Loss: 0.51569\n","Epoch: 00 [11921/20299 ( 59%)], Train Loss: 0.51592\n","Epoch: 00 [11931/20299 ( 59%)], Train Loss: 0.51565\n","Epoch: 00 [11941/20299 ( 59%)], Train Loss: 0.51568\n","Epoch: 00 [11951/20299 ( 59%)], Train Loss: 0.51556\n","Epoch: 00 [11961/20299 ( 59%)], Train Loss: 0.51532\n","Epoch: 00 [11971/20299 ( 59%)], Train Loss: 0.51536\n","Epoch: 00 [11981/20299 ( 59%)], Train Loss: 0.51510\n","Epoch: 00 [11991/20299 ( 59%)], Train Loss: 0.51487\n","Epoch: 00 [12001/20299 ( 59%)], Train Loss: 0.51476\n","Epoch: 00 [12011/20299 ( 59%)], Train Loss: 0.51478\n","Epoch: 00 [12021/20299 ( 59%)], Train Loss: 0.51468\n","Epoch: 00 [12031/20299 ( 59%)], Train Loss: 0.51450\n","Epoch: 00 [12041/20299 ( 59%)], Train Loss: 0.51434\n","Epoch: 00 [12051/20299 ( 59%)], Train Loss: 0.51422\n","Epoch: 00 [12061/20299 ( 59%)], Train Loss: 0.51410\n","Epoch: 00 [12071/20299 ( 59%)], Train Loss: 0.51396\n","Epoch: 00 [12081/20299 ( 60%)], Train Loss: 0.51373\n","Epoch: 00 [12091/20299 ( 60%)], Train Loss: 0.51374\n","Epoch: 00 [12101/20299 ( 60%)], Train Loss: 0.51385\n","Epoch: 00 [12111/20299 ( 60%)], Train Loss: 0.51387\n","Epoch: 00 [12121/20299 ( 60%)], Train Loss: 0.51375\n","Epoch: 00 [12131/20299 ( 60%)], Train Loss: 0.51350\n","Epoch: 00 [12141/20299 ( 60%)], Train Loss: 0.51310\n","Epoch: 00 [12151/20299 ( 60%)], Train Loss: 0.51273\n","Epoch: 00 [12161/20299 ( 60%)], Train Loss: 0.51297\n","Epoch: 00 [12171/20299 ( 60%)], Train Loss: 0.51312\n","Epoch: 00 [12181/20299 ( 60%)], Train Loss: 0.51323\n","Epoch: 00 [12191/20299 ( 60%)], Train Loss: 0.51327\n","Epoch: 00 [12201/20299 ( 60%)], Train Loss: 0.51330\n","Epoch: 00 [12211/20299 ( 60%)], Train Loss: 0.51328\n","Epoch: 00 [12221/20299 ( 60%)], Train Loss: 0.51342\n","Epoch: 00 [12231/20299 ( 60%)], Train Loss: 0.51318\n","Epoch: 00 [12241/20299 ( 60%)], Train Loss: 0.51317\n","Epoch: 00 [12251/20299 ( 60%)], Train Loss: 0.51309\n","Epoch: 00 [12261/20299 ( 60%)], Train Loss: 0.51320\n","Epoch: 00 [12271/20299 ( 60%)], Train Loss: 0.51296\n","Epoch: 00 [12281/20299 ( 61%)], Train Loss: 0.51302\n","Epoch: 00 [12291/20299 ( 61%)], Train Loss: 0.51281\n","Epoch: 00 [12301/20299 ( 61%)], Train Loss: 0.51295\n","Epoch: 00 [12311/20299 ( 61%)], Train Loss: 0.51296\n","Epoch: 00 [12321/20299 ( 61%)], Train Loss: 0.51266\n","Epoch: 00 [12331/20299 ( 61%)], Train Loss: 0.51254\n","Epoch: 00 [12341/20299 ( 61%)], Train Loss: 0.51249\n","Epoch: 00 [12351/20299 ( 61%)], Train Loss: 0.51228\n","Epoch: 00 [12361/20299 ( 61%)], Train Loss: 0.51217\n","Epoch: 00 [12371/20299 ( 61%)], Train Loss: 0.51194\n","Epoch: 00 [12381/20299 ( 61%)], Train Loss: 0.51186\n","Epoch: 00 [12391/20299 ( 61%)], Train Loss: 0.51156\n","Epoch: 00 [12401/20299 ( 61%)], Train Loss: 0.51173\n","Epoch: 00 [12411/20299 ( 61%)], Train Loss: 0.51150\n","Epoch: 00 [12421/20299 ( 61%)], Train Loss: 0.51132\n","Epoch: 00 [12431/20299 ( 61%)], Train Loss: 0.51107\n","Epoch: 00 [12441/20299 ( 61%)], Train Loss: 0.51093\n","Epoch: 00 [12451/20299 ( 61%)], Train Loss: 0.51081\n","Epoch: 00 [12461/20299 ( 61%)], Train Loss: 0.51067\n","Epoch: 00 [12471/20299 ( 61%)], Train Loss: 0.51079\n","Epoch: 00 [12481/20299 ( 61%)], Train Loss: 0.51070\n","Epoch: 00 [12491/20299 ( 62%)], Train Loss: 0.51041\n","Epoch: 00 [12501/20299 ( 62%)], Train Loss: 0.51042\n","Epoch: 00 [12511/20299 ( 62%)], Train Loss: 0.51018\n","Epoch: 00 [12521/20299 ( 62%)], Train Loss: 0.51005\n","Epoch: 00 [12531/20299 ( 62%)], Train Loss: 0.51027\n","Epoch: 00 [12541/20299 ( 62%)], Train Loss: 0.51020\n","Epoch: 00 [12551/20299 ( 62%)], Train Loss: 0.51005\n","Epoch: 00 [12561/20299 ( 62%)], Train Loss: 0.51012\n","Epoch: 00 [12571/20299 ( 62%)], Train Loss: 0.51006\n","Epoch: 00 [12581/20299 ( 62%)], Train Loss: 0.50977\n","Epoch: 00 [12591/20299 ( 62%)], Train Loss: 0.50965\n","Epoch: 00 [12601/20299 ( 62%)], Train Loss: 0.51016\n","Epoch: 00 [12611/20299 ( 62%)], Train Loss: 0.51011\n","Epoch: 00 [12621/20299 ( 62%)], Train Loss: 0.50997\n","Epoch: 00 [12631/20299 ( 62%)], Train Loss: 0.50970\n","Epoch: 00 [12641/20299 ( 62%)], Train Loss: 0.50968\n","Epoch: 00 [12651/20299 ( 62%)], Train Loss: 0.50953\n","Epoch: 00 [12661/20299 ( 62%)], Train Loss: 0.50955\n","Epoch: 00 [12671/20299 ( 62%)], Train Loss: 0.50935\n","Epoch: 00 [12681/20299 ( 62%)], Train Loss: 0.50925\n","Epoch: 00 [12691/20299 ( 63%)], Train Loss: 0.50897\n","Epoch: 00 [12701/20299 ( 63%)], Train Loss: 0.50901\n","Epoch: 00 [12711/20299 ( 63%)], Train Loss: 0.50877\n","Epoch: 00 [12721/20299 ( 63%)], Train Loss: 0.50854\n","Epoch: 00 [12731/20299 ( 63%)], Train Loss: 0.50823\n","Epoch: 00 [12741/20299 ( 63%)], Train Loss: 0.50803\n","Epoch: 00 [12751/20299 ( 63%)], Train Loss: 0.50792\n","Epoch: 00 [12761/20299 ( 63%)], Train Loss: 0.50791\n","Epoch: 00 [12771/20299 ( 63%)], Train Loss: 0.50783\n","Epoch: 00 [12781/20299 ( 63%)], Train Loss: 0.50792\n","Epoch: 00 [12791/20299 ( 63%)], Train Loss: 0.50815\n","Epoch: 00 [12801/20299 ( 63%)], Train Loss: 0.50807\n","Epoch: 00 [12811/20299 ( 63%)], Train Loss: 0.50793\n","Epoch: 00 [12821/20299 ( 63%)], Train Loss: 0.50766\n","Epoch: 00 [12831/20299 ( 63%)], Train Loss: 0.50783\n","Epoch: 00 [12841/20299 ( 63%)], Train Loss: 0.50763\n","Epoch: 00 [12851/20299 ( 63%)], Train Loss: 0.50748\n","Epoch: 00 [12861/20299 ( 63%)], Train Loss: 0.50734\n","Epoch: 00 [12871/20299 ( 63%)], Train Loss: 0.50707\n","Epoch: 00 [12881/20299 ( 63%)], Train Loss: 0.50718\n","Epoch: 00 [12891/20299 ( 64%)], Train Loss: 0.50706\n","Epoch: 00 [12901/20299 ( 64%)], Train Loss: 0.50703\n","Epoch: 00 [12911/20299 ( 64%)], Train Loss: 0.50694\n","Epoch: 00 [12921/20299 ( 64%)], Train Loss: 0.50697\n","Epoch: 00 [12931/20299 ( 64%)], Train Loss: 0.50678\n","Epoch: 00 [12941/20299 ( 64%)], Train Loss: 0.50652\n","Epoch: 00 [12951/20299 ( 64%)], Train Loss: 0.50631\n","Epoch: 00 [12961/20299 ( 64%)], Train Loss: 0.50661\n","Epoch: 00 [12971/20299 ( 64%)], Train Loss: 0.50643\n","Epoch: 00 [12981/20299 ( 64%)], Train Loss: 0.50671\n","Epoch: 00 [12991/20299 ( 64%)], Train Loss: 0.50659\n","Epoch: 00 [13001/20299 ( 64%)], Train Loss: 0.50680\n","Epoch: 00 [13011/20299 ( 64%)], Train Loss: 0.50675\n","Epoch: 00 [13021/20299 ( 64%)], Train Loss: 0.50670\n","Epoch: 00 [13031/20299 ( 64%)], Train Loss: 0.50667\n","Epoch: 00 [13041/20299 ( 64%)], Train Loss: 0.50667\n","Epoch: 00 [13051/20299 ( 64%)], Train Loss: 0.50642\n","Epoch: 00 [13061/20299 ( 64%)], Train Loss: 0.50653\n","Epoch: 00 [13071/20299 ( 64%)], Train Loss: 0.50648\n","Epoch: 00 [13081/20299 ( 64%)], Train Loss: 0.50694\n","Epoch: 00 [13091/20299 ( 64%)], Train Loss: 0.50663\n","Epoch: 00 [13101/20299 ( 65%)], Train Loss: 0.50652\n","Epoch: 00 [13111/20299 ( 65%)], Train Loss: 0.50642\n","Epoch: 00 [13121/20299 ( 65%)], Train Loss: 0.50624\n","Epoch: 00 [13131/20299 ( 65%)], Train Loss: 0.50614\n","Epoch: 00 [13141/20299 ( 65%)], Train Loss: 0.50611\n","Epoch: 00 [13151/20299 ( 65%)], Train Loss: 0.50589\n","Epoch: 00 [13161/20299 ( 65%)], Train Loss: 0.50571\n","Epoch: 00 [13171/20299 ( 65%)], Train Loss: 0.50551\n","Epoch: 00 [13181/20299 ( 65%)], Train Loss: 0.50522\n","Epoch: 00 [13191/20299 ( 65%)], Train Loss: 0.50519\n","Epoch: 00 [13201/20299 ( 65%)], Train Loss: 0.50495\n","Epoch: 00 [13211/20299 ( 65%)], Train Loss: 0.50470\n","Epoch: 00 [13221/20299 ( 65%)], Train Loss: 0.50475\n","Epoch: 00 [13231/20299 ( 65%)], Train Loss: 0.50461\n","Epoch: 00 [13241/20299 ( 65%)], Train Loss: 0.50429\n","Epoch: 00 [13251/20299 ( 65%)], Train Loss: 0.50419\n","Epoch: 00 [13261/20299 ( 65%)], Train Loss: 0.50383\n","Epoch: 00 [13271/20299 ( 65%)], Train Loss: 0.50371\n","Epoch: 00 [13281/20299 ( 65%)], Train Loss: 0.50363\n","Epoch: 00 [13291/20299 ( 65%)], Train Loss: 0.50354\n","Epoch: 00 [13301/20299 ( 66%)], Train Loss: 0.50327\n","Epoch: 00 [13311/20299 ( 66%)], Train Loss: 0.50344\n","Epoch: 00 [13321/20299 ( 66%)], Train Loss: 0.50334\n","Epoch: 00 [13331/20299 ( 66%)], Train Loss: 0.50319\n","Epoch: 00 [13341/20299 ( 66%)], Train Loss: 0.50318\n","Epoch: 00 [13351/20299 ( 66%)], Train Loss: 0.50292\n","Epoch: 00 [13361/20299 ( 66%)], Train Loss: 0.50271\n","Epoch: 00 [13371/20299 ( 66%)], Train Loss: 0.50240\n","Epoch: 00 [13381/20299 ( 66%)], Train Loss: 0.50250\n","Epoch: 00 [13391/20299 ( 66%)], Train Loss: 0.50270\n","Epoch: 00 [13401/20299 ( 66%)], Train Loss: 0.50260\n","Epoch: 00 [13411/20299 ( 66%)], Train Loss: 0.50274\n","Epoch: 00 [13421/20299 ( 66%)], Train Loss: 0.50256\n","Epoch: 00 [13431/20299 ( 66%)], Train Loss: 0.50254\n","Epoch: 00 [13441/20299 ( 66%)], Train Loss: 0.50264\n","Epoch: 00 [13451/20299 ( 66%)], Train Loss: 0.50256\n","Epoch: 00 [13461/20299 ( 66%)], Train Loss: 0.50227\n","Epoch: 00 [13471/20299 ( 66%)], Train Loss: 0.50207\n","Epoch: 00 [13481/20299 ( 66%)], Train Loss: 0.50200\n","Epoch: 00 [13491/20299 ( 66%)], Train Loss: 0.50189\n","Epoch: 00 [13501/20299 ( 67%)], Train Loss: 0.50161\n","Epoch: 00 [13511/20299 ( 67%)], Train Loss: 0.50175\n","Epoch: 00 [13521/20299 ( 67%)], Train Loss: 0.50147\n","Epoch: 00 [13531/20299 ( 67%)], Train Loss: 0.50142\n","Epoch: 00 [13541/20299 ( 67%)], Train Loss: 0.50116\n","Epoch: 00 [13551/20299 ( 67%)], Train Loss: 0.50095\n","Epoch: 00 [13561/20299 ( 67%)], Train Loss: 0.50088\n","Epoch: 00 [13571/20299 ( 67%)], Train Loss: 0.50073\n","Epoch: 00 [13581/20299 ( 67%)], Train Loss: 0.50052\n","Epoch: 00 [13591/20299 ( 67%)], Train Loss: 0.50073\n","Epoch: 00 [13601/20299 ( 67%)], Train Loss: 0.50052\n","Epoch: 00 [13611/20299 ( 67%)], Train Loss: 0.50030\n","Epoch: 00 [13621/20299 ( 67%)], Train Loss: 0.50009\n","Epoch: 00 [13631/20299 ( 67%)], Train Loss: 0.50009\n","Epoch: 00 [13641/20299 ( 67%)], Train Loss: 0.50000\n","Epoch: 00 [13651/20299 ( 67%)], Train Loss: 0.50000\n","Epoch: 00 [13661/20299 ( 67%)], Train Loss: 0.50013\n","Epoch: 00 [13671/20299 ( 67%)], Train Loss: 0.49982\n","Epoch: 00 [13681/20299 ( 67%)], Train Loss: 0.49984\n","Epoch: 00 [13691/20299 ( 67%)], Train Loss: 0.49993\n","Epoch: 00 [13701/20299 ( 67%)], Train Loss: 0.49989\n","Epoch: 00 [13711/20299 ( 68%)], Train Loss: 0.49973\n","Epoch: 00 [13721/20299 ( 68%)], Train Loss: 0.49959\n","Epoch: 00 [13731/20299 ( 68%)], Train Loss: 0.49929\n","Epoch: 00 [13741/20299 ( 68%)], Train Loss: 0.49916\n","Epoch: 00 [13751/20299 ( 68%)], Train Loss: 0.49915\n","Epoch: 00 [13761/20299 ( 68%)], Train Loss: 0.49895\n","Epoch: 00 [13771/20299 ( 68%)], Train Loss: 0.49883\n","Epoch: 00 [13781/20299 ( 68%)], Train Loss: 0.49890\n","Epoch: 00 [13791/20299 ( 68%)], Train Loss: 0.49865\n","Epoch: 00 [13801/20299 ( 68%)], Train Loss: 0.49859\n","Epoch: 00 [13811/20299 ( 68%)], Train Loss: 0.49908\n","Epoch: 00 [13821/20299 ( 68%)], Train Loss: 0.49884\n","Epoch: 00 [13831/20299 ( 68%)], Train Loss: 0.49919\n","Epoch: 00 [13841/20299 ( 68%)], Train Loss: 0.49901\n","Epoch: 00 [13851/20299 ( 68%)], Train Loss: 0.49917\n","Epoch: 00 [13861/20299 ( 68%)], Train Loss: 0.49901\n","Epoch: 00 [13871/20299 ( 68%)], Train Loss: 0.49901\n","Epoch: 00 [13881/20299 ( 68%)], Train Loss: 0.49878\n","Epoch: 00 [13891/20299 ( 68%)], Train Loss: 0.49876\n","Epoch: 00 [13901/20299 ( 68%)], Train Loss: 0.49871\n","Epoch: 00 [13911/20299 ( 69%)], Train Loss: 0.49857\n","Epoch: 00 [13921/20299 ( 69%)], Train Loss: 0.49840\n","Epoch: 00 [13931/20299 ( 69%)], Train Loss: 0.49827\n","Epoch: 00 [13941/20299 ( 69%)], Train Loss: 0.49803\n","Epoch: 00 [13951/20299 ( 69%)], Train Loss: 0.49776\n","Epoch: 00 [13961/20299 ( 69%)], Train Loss: 0.49753\n","Epoch: 00 [13971/20299 ( 69%)], Train Loss: 0.49730\n","Epoch: 00 [13981/20299 ( 69%)], Train Loss: 0.49706\n","Epoch: 00 [13991/20299 ( 69%)], Train Loss: 0.49674\n","Epoch: 00 [14001/20299 ( 69%)], Train Loss: 0.49650\n","Epoch: 00 [14011/20299 ( 69%)], Train Loss: 0.49644\n","Epoch: 00 [14021/20299 ( 69%)], Train Loss: 0.49621\n","Epoch: 00 [14031/20299 ( 69%)], Train Loss: 0.49603\n","Epoch: 00 [14041/20299 ( 69%)], Train Loss: 0.49596\n","Epoch: 00 [14051/20299 ( 69%)], Train Loss: 0.49568\n","Epoch: 00 [14061/20299 ( 69%)], Train Loss: 0.49568\n","Epoch: 00 [14071/20299 ( 69%)], Train Loss: 0.49547\n","Epoch: 00 [14081/20299 ( 69%)], Train Loss: 0.49531\n","Epoch: 00 [14091/20299 ( 69%)], Train Loss: 0.49519\n","Epoch: 00 [14101/20299 ( 69%)], Train Loss: 0.49489\n","Epoch: 00 [14111/20299 ( 70%)], Train Loss: 0.49473\n","Epoch: 00 [14121/20299 ( 70%)], Train Loss: 0.49485\n","Epoch: 00 [14131/20299 ( 70%)], Train Loss: 0.49503\n","Epoch: 00 [14141/20299 ( 70%)], Train Loss: 0.49475\n","Epoch: 00 [14151/20299 ( 70%)], Train Loss: 0.49467\n","Epoch: 00 [14161/20299 ( 70%)], Train Loss: 0.49493\n","Epoch: 00 [14171/20299 ( 70%)], Train Loss: 0.49481\n","Epoch: 00 [14181/20299 ( 70%)], Train Loss: 0.49458\n","Epoch: 00 [14191/20299 ( 70%)], Train Loss: 0.49450\n","Epoch: 00 [14201/20299 ( 70%)], Train Loss: 0.49425\n","Epoch: 00 [14211/20299 ( 70%)], Train Loss: 0.49408\n","Epoch: 00 [14221/20299 ( 70%)], Train Loss: 0.49408\n","Epoch: 00 [14231/20299 ( 70%)], Train Loss: 0.49400\n","Epoch: 00 [14241/20299 ( 70%)], Train Loss: 0.49375\n","Epoch: 00 [14251/20299 ( 70%)], Train Loss: 0.49351\n","Epoch: 00 [14261/20299 ( 70%)], Train Loss: 0.49339\n","Epoch: 00 [14271/20299 ( 70%)], Train Loss: 0.49320\n","Epoch: 00 [14281/20299 ( 70%)], Train Loss: 0.49302\n","Epoch: 00 [14291/20299 ( 70%)], Train Loss: 0.49268\n","Epoch: 00 [14301/20299 ( 70%)], Train Loss: 0.49242\n","Epoch: 00 [14311/20299 ( 71%)], Train Loss: 0.49221\n","Epoch: 00 [14321/20299 ( 71%)], Train Loss: 0.49192\n","Epoch: 00 [14331/20299 ( 71%)], Train Loss: 0.49171\n","Epoch: 00 [14341/20299 ( 71%)], Train Loss: 0.49148\n","Epoch: 00 [14351/20299 ( 71%)], Train Loss: 0.49130\n","Epoch: 00 [14361/20299 ( 71%)], Train Loss: 0.49121\n","Epoch: 00 [14371/20299 ( 71%)], Train Loss: 0.49115\n","Epoch: 00 [14381/20299 ( 71%)], Train Loss: 0.49115\n","Epoch: 00 [14391/20299 ( 71%)], Train Loss: 0.49087\n","Epoch: 00 [14401/20299 ( 71%)], Train Loss: 0.49065\n","Epoch: 00 [14411/20299 ( 71%)], Train Loss: 0.49045\n","Epoch: 00 [14421/20299 ( 71%)], Train Loss: 0.49067\n","Epoch: 00 [14431/20299 ( 71%)], Train Loss: 0.49081\n","Epoch: 00 [14441/20299 ( 71%)], Train Loss: 0.49071\n","Epoch: 00 [14451/20299 ( 71%)], Train Loss: 0.49042\n","Epoch: 00 [14461/20299 ( 71%)], Train Loss: 0.49029\n","Epoch: 00 [14471/20299 ( 71%)], Train Loss: 0.49042\n","Epoch: 00 [14481/20299 ( 71%)], Train Loss: 0.49031\n","Epoch: 00 [14491/20299 ( 71%)], Train Loss: 0.49025\n","Epoch: 00 [14501/20299 ( 71%)], Train Loss: 0.49005\n","Epoch: 00 [14511/20299 ( 71%)], Train Loss: 0.48995\n","Epoch: 00 [14521/20299 ( 72%)], Train Loss: 0.48980\n","Epoch: 00 [14531/20299 ( 72%)], Train Loss: 0.48983\n","Epoch: 00 [14541/20299 ( 72%)], Train Loss: 0.48982\n","Epoch: 00 [14551/20299 ( 72%)], Train Loss: 0.48968\n","Epoch: 00 [14561/20299 ( 72%)], Train Loss: 0.48979\n","Epoch: 00 [14571/20299 ( 72%)], Train Loss: 0.48956\n","Epoch: 00 [14581/20299 ( 72%)], Train Loss: 0.48998\n","Epoch: 00 [14591/20299 ( 72%)], Train Loss: 0.48970\n","Epoch: 00 [14601/20299 ( 72%)], Train Loss: 0.48958\n","Epoch: 00 [14611/20299 ( 72%)], Train Loss: 0.48935\n","Epoch: 00 [14621/20299 ( 72%)], Train Loss: 0.48910\n","Epoch: 00 [14631/20299 ( 72%)], Train Loss: 0.48886\n","Epoch: 00 [14641/20299 ( 72%)], Train Loss: 0.48865\n","Epoch: 00 [14651/20299 ( 72%)], Train Loss: 0.48848\n","Epoch: 00 [14661/20299 ( 72%)], Train Loss: 0.48826\n","Epoch: 00 [14671/20299 ( 72%)], Train Loss: 0.48857\n","Epoch: 00 [14681/20299 ( 72%)], Train Loss: 0.48861\n","Epoch: 00 [14691/20299 ( 72%)], Train Loss: 0.48846\n","Epoch: 00 [14701/20299 ( 72%)], Train Loss: 0.48836\n","Epoch: 00 [14711/20299 ( 72%)], Train Loss: 0.48834\n","Epoch: 00 [14721/20299 ( 73%)], Train Loss: 0.48825\n","Epoch: 00 [14731/20299 ( 73%)], Train Loss: 0.48800\n","Epoch: 00 [14741/20299 ( 73%)], Train Loss: 0.48782\n","Epoch: 00 [14751/20299 ( 73%)], Train Loss: 0.48785\n","Epoch: 00 [14761/20299 ( 73%)], Train Loss: 0.48770\n","Epoch: 00 [14771/20299 ( 73%)], Train Loss: 0.48769\n","Epoch: 00 [14781/20299 ( 73%)], Train Loss: 0.48774\n","Epoch: 00 [14791/20299 ( 73%)], Train Loss: 0.48800\n","Epoch: 00 [14801/20299 ( 73%)], Train Loss: 0.48785\n","Epoch: 00 [14811/20299 ( 73%)], Train Loss: 0.48792\n","Epoch: 00 [14821/20299 ( 73%)], Train Loss: 0.48807\n","Epoch: 00 [14831/20299 ( 73%)], Train Loss: 0.48791\n","Epoch: 00 [14841/20299 ( 73%)], Train Loss: 0.48772\n","Epoch: 00 [14851/20299 ( 73%)], Train Loss: 0.48764\n","Epoch: 00 [14861/20299 ( 73%)], Train Loss: 0.48760\n","Epoch: 00 [14871/20299 ( 73%)], Train Loss: 0.48737\n","Epoch: 00 [14881/20299 ( 73%)], Train Loss: 0.48737\n","Epoch: 00 [14891/20299 ( 73%)], Train Loss: 0.48705\n","Epoch: 00 [14901/20299 ( 73%)], Train Loss: 0.48693\n","Epoch: 00 [14911/20299 ( 73%)], Train Loss: 0.48681\n","Epoch: 00 [14921/20299 ( 74%)], Train Loss: 0.48671\n","Epoch: 00 [14931/20299 ( 74%)], Train Loss: 0.48664\n","Epoch: 00 [14941/20299 ( 74%)], Train Loss: 0.48682\n","Epoch: 00 [14951/20299 ( 74%)], Train Loss: 0.48666\n","Epoch: 00 [14961/20299 ( 74%)], Train Loss: 0.48661\n","Epoch: 00 [14971/20299 ( 74%)], Train Loss: 0.48637\n","Epoch: 00 [14981/20299 ( 74%)], Train Loss: 0.48618\n","Epoch: 00 [14991/20299 ( 74%)], Train Loss: 0.48607\n","Epoch: 00 [15001/20299 ( 74%)], Train Loss: 0.48578\n","Epoch: 00 [15011/20299 ( 74%)], Train Loss: 0.48579\n","Epoch: 00 [15021/20299 ( 74%)], Train Loss: 0.48547\n","Epoch: 00 [15031/20299 ( 74%)], Train Loss: 0.48527\n","Epoch: 00 [15041/20299 ( 74%)], Train Loss: 0.48523\n","Epoch: 00 [15051/20299 ( 74%)], Train Loss: 0.48523\n","Epoch: 00 [15061/20299 ( 74%)], Train Loss: 0.48520\n","Epoch: 00 [15071/20299 ( 74%)], Train Loss: 0.48538\n","Epoch: 00 [15081/20299 ( 74%)], Train Loss: 0.48533\n","Epoch: 00 [15091/20299 ( 74%)], Train Loss: 0.48545\n","Epoch: 00 [15101/20299 ( 74%)], Train Loss: 0.48524\n","Epoch: 00 [15111/20299 ( 74%)], Train Loss: 0.48512\n","Epoch: 00 [15121/20299 ( 74%)], Train Loss: 0.48508\n","Epoch: 00 [15131/20299 ( 75%)], Train Loss: 0.48512\n","Epoch: 00 [15141/20299 ( 75%)], Train Loss: 0.48525\n","Epoch: 00 [15151/20299 ( 75%)], Train Loss: 0.48529\n","Epoch: 00 [15161/20299 ( 75%)], Train Loss: 0.48517\n","Epoch: 00 [15171/20299 ( 75%)], Train Loss: 0.48522\n","Epoch: 00 [15181/20299 ( 75%)], Train Loss: 0.48523\n","Epoch: 00 [15191/20299 ( 75%)], Train Loss: 0.48512\n","Epoch: 00 [15201/20299 ( 75%)], Train Loss: 0.48501\n","Epoch: 00 [15211/20299 ( 75%)], Train Loss: 0.48485\n","Epoch: 00 [15221/20299 ( 75%)], Train Loss: 0.48458\n","Epoch: 00 [15231/20299 ( 75%)], Train Loss: 0.48429\n","Epoch: 00 [15241/20299 ( 75%)], Train Loss: 0.48415\n","Epoch: 00 [15251/20299 ( 75%)], Train Loss: 0.48403\n","Epoch: 00 [15261/20299 ( 75%)], Train Loss: 0.48395\n","Epoch: 00 [15271/20299 ( 75%)], Train Loss: 0.48402\n","Epoch: 00 [15281/20299 ( 75%)], Train Loss: 0.48398\n","Epoch: 00 [15291/20299 ( 75%)], Train Loss: 0.48398\n","Epoch: 00 [15301/20299 ( 75%)], Train Loss: 0.48376\n","Epoch: 00 [15311/20299 ( 75%)], Train Loss: 0.48373\n","Epoch: 00 [15321/20299 ( 75%)], Train Loss: 0.48358\n","Epoch: 00 [15331/20299 ( 76%)], Train Loss: 0.48368\n","Epoch: 00 [15341/20299 ( 76%)], Train Loss: 0.48357\n","Epoch: 00 [15351/20299 ( 76%)], Train Loss: 0.48339\n","Epoch: 00 [15361/20299 ( 76%)], Train Loss: 0.48371\n","Epoch: 00 [15371/20299 ( 76%)], Train Loss: 0.48367\n","Epoch: 00 [15381/20299 ( 76%)], Train Loss: 0.48342\n","Epoch: 00 [15391/20299 ( 76%)], Train Loss: 0.48322\n","Epoch: 00 [15401/20299 ( 76%)], Train Loss: 0.48330\n","Epoch: 00 [15411/20299 ( 76%)], Train Loss: 0.48339\n","Epoch: 00 [15421/20299 ( 76%)], Train Loss: 0.48325\n","Epoch: 00 [15431/20299 ( 76%)], Train Loss: 0.48307\n","Epoch: 00 [15441/20299 ( 76%)], Train Loss: 0.48309\n","Epoch: 00 [15451/20299 ( 76%)], Train Loss: 0.48303\n","Epoch: 00 [15461/20299 ( 76%)], Train Loss: 0.48315\n","Epoch: 00 [15471/20299 ( 76%)], Train Loss: 0.48298\n","Epoch: 00 [15481/20299 ( 76%)], Train Loss: 0.48287\n","Epoch: 00 [15491/20299 ( 76%)], Train Loss: 0.48283\n","Epoch: 00 [15501/20299 ( 76%)], Train Loss: 0.48280\n","Epoch: 00 [15511/20299 ( 76%)], Train Loss: 0.48264\n","Epoch: 00 [15521/20299 ( 76%)], Train Loss: 0.48238\n","Epoch: 00 [15531/20299 ( 77%)], Train Loss: 0.48216\n","Epoch: 00 [15541/20299 ( 77%)], Train Loss: 0.48222\n","Epoch: 00 [15551/20299 ( 77%)], Train Loss: 0.48236\n","Epoch: 00 [15561/20299 ( 77%)], Train Loss: 0.48212\n","Epoch: 00 [15571/20299 ( 77%)], Train Loss: 0.48193\n","Epoch: 00 [15581/20299 ( 77%)], Train Loss: 0.48178\n","Epoch: 00 [15591/20299 ( 77%)], Train Loss: 0.48160\n","Epoch: 00 [15601/20299 ( 77%)], Train Loss: 0.48165\n","Epoch: 00 [15611/20299 ( 77%)], Train Loss: 0.48153\n","Epoch: 00 [15621/20299 ( 77%)], Train Loss: 0.48136\n","Epoch: 00 [15631/20299 ( 77%)], Train Loss: 0.48117\n","Epoch: 00 [15641/20299 ( 77%)], Train Loss: 0.48132\n","Epoch: 00 [15651/20299 ( 77%)], Train Loss: 0.48138\n","Epoch: 00 [15661/20299 ( 77%)], Train Loss: 0.48134\n","Epoch: 00 [15671/20299 ( 77%)], Train Loss: 0.48144\n","Epoch: 00 [15681/20299 ( 77%)], Train Loss: 0.48137\n","Epoch: 00 [15691/20299 ( 77%)], Train Loss: 0.48121\n","Epoch: 00 [15701/20299 ( 77%)], Train Loss: 0.48101\n","Epoch: 00 [15711/20299 ( 77%)], Train Loss: 0.48089\n","Epoch: 00 [15721/20299 ( 77%)], Train Loss: 0.48064\n","Epoch: 00 [15731/20299 ( 77%)], Train Loss: 0.48049\n","Epoch: 00 [15741/20299 ( 78%)], Train Loss: 0.48036\n","Epoch: 00 [15751/20299 ( 78%)], Train Loss: 0.48035\n","Epoch: 00 [15761/20299 ( 78%)], Train Loss: 0.48035\n","Epoch: 00 [15771/20299 ( 78%)], Train Loss: 0.48031\n","Epoch: 00 [15781/20299 ( 78%)], Train Loss: 0.48023\n","Epoch: 00 [15791/20299 ( 78%)], Train Loss: 0.48007\n","Epoch: 00 [15801/20299 ( 78%)], Train Loss: 0.47996\n","Epoch: 00 [15811/20299 ( 78%)], Train Loss: 0.47993\n","Epoch: 00 [15821/20299 ( 78%)], Train Loss: 0.48001\n","Epoch: 00 [15831/20299 ( 78%)], Train Loss: 0.48001\n","Epoch: 00 [15841/20299 ( 78%)], Train Loss: 0.47978\n","Epoch: 00 [15851/20299 ( 78%)], Train Loss: 0.47969\n","Epoch: 00 [15861/20299 ( 78%)], Train Loss: 0.47954\n","Epoch: 00 [15871/20299 ( 78%)], Train Loss: 0.47948\n","Epoch: 00 [15881/20299 ( 78%)], Train Loss: 0.47965\n","Epoch: 00 [15891/20299 ( 78%)], Train Loss: 0.47952\n","Epoch: 00 [15901/20299 ( 78%)], Train Loss: 0.47959\n","Epoch: 00 [15911/20299 ( 78%)], Train Loss: 0.47965\n","Epoch: 00 [15921/20299 ( 78%)], Train Loss: 0.47958\n","Epoch: 00 [15931/20299 ( 78%)], Train Loss: 0.47973\n","Epoch: 00 [15941/20299 ( 79%)], Train Loss: 0.47959\n","Epoch: 00 [15951/20299 ( 79%)], Train Loss: 0.47953\n","Epoch: 00 [15961/20299 ( 79%)], Train Loss: 0.47930\n","Epoch: 00 [15971/20299 ( 79%)], Train Loss: 0.47936\n","Epoch: 00 [15981/20299 ( 79%)], Train Loss: 0.47924\n","Epoch: 00 [15991/20299 ( 79%)], Train Loss: 0.47941\n","Epoch: 00 [16001/20299 ( 79%)], Train Loss: 0.47951\n","Epoch: 00 [16011/20299 ( 79%)], Train Loss: 0.47965\n","Epoch: 00 [16021/20299 ( 79%)], Train Loss: 0.47949\n","Epoch: 00 [16031/20299 ( 79%)], Train Loss: 0.47940\n","Epoch: 00 [16041/20299 ( 79%)], Train Loss: 0.47937\n","Epoch: 00 [16051/20299 ( 79%)], Train Loss: 0.47924\n","Epoch: 00 [16061/20299 ( 79%)], Train Loss: 0.47913\n","Epoch: 00 [16071/20299 ( 79%)], Train Loss: 0.47913\n","Epoch: 00 [16081/20299 ( 79%)], Train Loss: 0.47911\n","Epoch: 00 [16091/20299 ( 79%)], Train Loss: 0.47898\n","Epoch: 00 [16101/20299 ( 79%)], Train Loss: 0.47883\n","Epoch: 00 [16111/20299 ( 79%)], Train Loss: 0.47866\n","Epoch: 00 [16121/20299 ( 79%)], Train Loss: 0.47885\n","Epoch: 00 [16131/20299 ( 79%)], Train Loss: 0.47871\n","Epoch: 00 [16141/20299 ( 80%)], Train Loss: 0.47859\n","Epoch: 00 [16151/20299 ( 80%)], Train Loss: 0.47853\n","Epoch: 00 [16161/20299 ( 80%)], Train Loss: 0.47862\n","Epoch: 00 [16171/20299 ( 80%)], Train Loss: 0.47872\n","Epoch: 00 [16181/20299 ( 80%)], Train Loss: 0.47868\n","Epoch: 00 [16191/20299 ( 80%)], Train Loss: 0.47854\n","Epoch: 00 [16201/20299 ( 80%)], Train Loss: 0.47835\n","Epoch: 00 [16211/20299 ( 80%)], Train Loss: 0.47829\n","Epoch: 00 [16221/20299 ( 80%)], Train Loss: 0.47813\n","Epoch: 00 [16231/20299 ( 80%)], Train Loss: 0.47797\n","Epoch: 00 [16241/20299 ( 80%)], Train Loss: 0.47796\n","Epoch: 00 [16251/20299 ( 80%)], Train Loss: 0.47772\n","Epoch: 00 [16261/20299 ( 80%)], Train Loss: 0.47771\n","Epoch: 00 [16271/20299 ( 80%)], Train Loss: 0.47762\n","Epoch: 00 [16281/20299 ( 80%)], Train Loss: 0.47735\n","Epoch: 00 [16291/20299 ( 80%)], Train Loss: 0.47736\n","Epoch: 00 [16301/20299 ( 80%)], Train Loss: 0.47743\n","Epoch: 00 [16311/20299 ( 80%)], Train Loss: 0.47765\n","Epoch: 00 [16321/20299 ( 80%)], Train Loss: 0.47737\n","Epoch: 00 [16331/20299 ( 80%)], Train Loss: 0.47726\n","Epoch: 00 [16341/20299 ( 81%)], Train Loss: 0.47730\n","Epoch: 00 [16351/20299 ( 81%)], Train Loss: 0.47731\n","Epoch: 00 [16361/20299 ( 81%)], Train Loss: 0.47716\n","Epoch: 00 [16371/20299 ( 81%)], Train Loss: 0.47697\n","Epoch: 00 [16381/20299 ( 81%)], Train Loss: 0.47679\n","Epoch: 00 [16391/20299 ( 81%)], Train Loss: 0.47687\n","Epoch: 00 [16401/20299 ( 81%)], Train Loss: 0.47666\n","Epoch: 00 [16411/20299 ( 81%)], Train Loss: 0.47662\n","Epoch: 00 [16421/20299 ( 81%)], Train Loss: 0.47695\n","Epoch: 00 [16431/20299 ( 81%)], Train Loss: 0.47674\n"]}]},{"cell_type":"code","metadata":{"id":"o6-F5rzbHSK5"},"source":[""],"execution_count":null,"outputs":[]}]}